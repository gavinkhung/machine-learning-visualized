\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    \usepackage{svg}
    % Keep aspect ratio if custom image width or height is specified
    \setkeys{Gin}{keepaspectratio}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{soul}      % strikethrough (\st) support for pandoc >= 3.0.0
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \makeatletter
    \newsavebox\pandoc@box
    \newcommand*\pandocbounded[1]{%
      \sbox\pandoc@box{#1}%
      % scaling factors for width and height
      \Gscale@div\@tempa\textheight{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
      \Gscale@div\@tempb\linewidth{\wd\pandoc@box}%
      % select the smaller of both
      \ifdim\@tempb\p@<\@tempa\p@
        \let\@tempa\@tempb
      \fi
      % scaling accordingly (\@tempa < 1)
      \ifdim\@tempa\p@<\p@
        \scalebox{\@tempa}{\usebox\pandoc@box}%
      % scaling not needed, use as it is
      \else
        \usebox{\pandoc@box}%
      \fi
    }
    \makeatother

    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{combined}
    
    
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@ges}{\let\PY@bf=\textbf\let\PY@it=\textit}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \section{Gradient Descent}\label{gradient-descent}

Gradient Descent is an algorithm that finds the local extrema of a
function. This is applicable to machine learning, because we want to
find the optimal parameters that minimize our loss function. In machine
learning, loss functions quantify the amount of error between the
predicted values from a machine learning model and the actual expected
values. In this notebook, we will perform linear regression by using
gradient descent to find the optimal slope and y-intercept.

    \subsection{Training Dataset}\label{training-dataset}

    Importing the libraries

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{import} \PY{n+nn}{scienceplots}
\PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k+kn}{import} \PY{n}{display}\PY{p}{,} \PY{n}{Latex}\PY{p}{,} \PY{n}{Image}

\PY{k+kn}{from} \PY{n+nn}{celluloid} \PY{k+kn}{import} \PY{n}{Camera}

\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{style}\PY{o}{.}\PY{n}{use}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{science}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{no\PYZhy{}latex}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Let's look at the training dataset. We will use columns 2 and 4 of the
txt file. The linear regression model will find the optimal slope and
y-intercept to fit the data.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{fname} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{REGRESSION\PYZhy{}gradientDescent\PYZhy{}data.txt}\PY{l+s+s2}{\PYZdq{}}
\PY{n}{x}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{loadtxt}\PY{p}{(}\PY{n}{fname}\PY{p}{,} \PY{n}{delimiter}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{,}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{unpack}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{skiprows}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{usecols}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}

\PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
\PY{n}{ax} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}1f77b4}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{o}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.25}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<matplotlib.collections.PathCollection at 0xffffa59fad50>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{combined_files/combined_5_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Loss Function: Mean Squared
Error}\label{loss-function-mean-squared-error}

    For the linear regression model, the predicted value \(\hat{y}\) is the
dot product of the weight and x input vectors plus a bias term

\(\hat{y} = w x + b\)

We will use the mean squared error function as our loss function.

\begin{align*}
MSE &= \frac{1}{n} \sum_{i=1}^{n}(y_{i}-\hat{y})^2 \\
&= \frac{1}{n} \sum_{i=1}^{n}(y_{i}-(w x_{i} + b))^2
\end{align*}

    \subsection{Loss Function Gradient}\label{loss-function-gradient}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{mse\PYZus{}loss}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{square}\PY{p}{(}\PY{n}{y} \PY{o}{\PYZhy{}} \PY{p}{(}\PY{n}{w} \PY{o}{*} \PY{n}{x} \PY{o}{+} \PY{n}{b}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    In each epoch of gradient descent, a parameter is updated by subtracting
the product of the gradient of the function and the learning rate
(\(lr\)). The learning rate controls how much the parameters should
change. Small learning rates are precise, but are slow. Large learning
rates are fast, but may prevent the model from finding the local
extrema.

\begin{align*}
X_{n+1} = X_n - lr * \frac{\partial}{\partial X} f(X_n)
\end{align*}

Since we are finding the optimal slope (\(w\)) and y-intercept (\(b\))
for our linear regression model, we must find the partial derivatives of
the loss function with respect to \(w\) and \(b\).

    \subsection{Loss Function in Terms of
W}\label{loss-function-in-terms-of-w}

    Loss function with respect to \(w\):

\begin{align*}
\frac{\partial }{\partial w} \left( MSE \right) &= \frac{\partial }{\partial w}[\frac{1}{n} \sum_{i=1}^{n}(y_{i}-(w x_{i} + b))^2] \\
&= \frac{1}{n} \sum_{i=1}^{n} \frac{\partial }{\partial w}[(y_{i}-(w x_{i} + b))^2] \\
&= \frac{2}{n} \sum_{i=1}^{n} (y_{i}-(w x_{i} + b))\frac{\partial }{\partial w}[y_{i}-(w x_{i} + b)] \\
&= \frac{2}{n} \sum_{i=1}^{n} (y_{i}-(w x_{i} + b))(-x_{i}) \\ 
&= -\frac{2}{n} \sum_{i=1}^{n}x_{i}(y_{i}-(w x_{i} + b))
\end{align*}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{mse\PYZus{}loss\PYZus{}dw}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{o}{\PYZhy{}}\PY{l+m+mi}{2} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{x} \PY{o}{*} \PY{p}{(}\PY{n}{y} \PY{o}{\PYZhy{}} \PY{p}{(}\PY{n}{w} \PY{o}{*} \PY{n}{x} \PY{o}{+} \PY{n}{b}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Loss Function in Terms of
b}\label{loss-function-in-terms-of-b}

    Loss function with respect to \(b\):

\begin{align*}
\frac{\partial}{\partial b} \left( MSE \right) &=  \frac{\partial }{\partial b}[\frac{1}{n} \sum_{i=1}^{n}(y_{i}-(w x_{i} + b))^2] \\
&= \frac{1}{n} \sum_{i=1}^{n} \frac{\partial }{\partial b}[(y_{i}-(w x_{i} + b))^2] \\
&= \frac{2}{n} \sum_{i=1}^{n} (y_{i}-(w x_{i} + b))\frac{\partial }{\partial b}[y_{i}-(w x_{i} + b)] \\
&= \frac{2}{n} \sum_{i=1}^{n} (y_{i}-(w x_{i} + b))(-1) \\ 
&= -\frac{2}{n} \sum_{i=1}^{n} (y_{i}-(w x_{i} + b))
\end{align*}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{mse\PYZus{}loss\PYZus{}db}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{o}{\PYZhy{}}\PY{l+m+mi}{2} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{y} \PY{o}{\PYZhy{}} \PY{p}{(}\PY{n}{w} \PY{o}{*} \PY{n}{x} \PY{o}{+} \PY{n}{b}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Training the Linear Regression
Model}\label{training-the-linear-regression-model}

    Let's define a function that uses the gradient algorithm to update the
parameters of the loss function. The function uses the gradient
functions we derived earlier.

General Gradient Descent Equation:
\[
X_{n+1} = X_n - \text{lr} \cdot \frac{\partial}{\partial X} f(X_n)
\]

Bias Gradient Descent:

\begin{align*}
b &= b - \eta \frac{\partial}{\partial b} [L(\vec{w}, b)] \\
&= b - \eta [-\frac{2}{n} \sum_{i=1}^{n} (y_{i}-(w x_{i} + b))]
\end{align*}

    Weights Gradient Descent:

\begin{align*}
b &= b - \eta \frac{\partial}{\partial b} [L(\vec{w}, b)] \\
&= b - \eta [-\frac{2}{n} \sum_{i=1}^{n}x_{i}(y_{i}-(w x_{i} + b))]
\end{align*}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{update\PYZus{}w\PYZus{}and\PYZus{}b}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} update w and b}
    \PY{n}{w} \PY{o}{=} \PY{n}{w} \PY{o}{\PYZhy{}} \PY{n}{mse\PYZus{}loss\PYZus{}dw}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{)} \PY{o}{*} \PY{n}{learning\PYZus{}rate}
    \PY{n}{b} \PY{o}{=} \PY{n}{b} \PY{o}{\PYZhy{}} \PY{n}{mse\PYZus{}loss\PYZus{}db}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{)} \PY{o}{*} \PY{n}{learning\PYZus{}rate}

    \PY{k}{return} \PY{n}{w}\PY{p}{,} \PY{n}{b}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Graphing functions}\label{graphing-functions}

Let's define helper functions to plot the graphs.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{create\PYZus{}plots}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{plt}\PY{o}{.}\PY{n}{ioff}\PY{p}{(}\PY{p}{)}
    \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{16} \PY{o}{/} \PY{l+m+mf}{9.0} \PY{o}{*} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{4} \PY{o}{*} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{layout}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{constrained}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{fig}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Gradient Descent}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax0} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{ax0}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Spending}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontweight}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{normal}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax0}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sales}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontweight}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{normal}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax0}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Linear Regression}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{ax1} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{projection}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{3d}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Slope, w}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Intercept, b}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{set\PYZus{}zlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{view\PYZus{}init}\PY{p}{(}\PY{l+m+mi}{15}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{35}\PY{p}{)}

    \PY{n}{camera} \PY{o}{=} \PY{n}{Camera}\PY{p}{(}\PY{n}{fig}\PY{p}{)}
    \PY{k}{return} \PY{n}{ax0}\PY{p}{,} \PY{n}{ax1}\PY{p}{,} \PY{n}{camera}


\PY{k}{def} \PY{n+nf}{generate\PYZus{}error\PYZus{}range}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{N}\PY{p}{,} \PY{n}{w\PYZus{}max}\PY{p}{,} \PY{n}{b\PYZus{}max}\PY{p}{)}\PY{p}{:}
    \PY{n}{w\PYZus{}range} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{w\PYZus{}max}\PY{p}{,} \PY{n}{w\PYZus{}max} \PY{o}{/} \PY{n}{N}\PY{p}{)}
    \PY{n}{b\PYZus{}range} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{b\PYZus{}max}\PY{p}{,} \PY{n}{b\PYZus{}max} \PY{o}{/} \PY{n}{N}\PY{p}{)}
    \PY{n}{w\PYZus{}range}\PY{p}{,} \PY{n}{b\PYZus{}range} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{meshgrid}\PY{p}{(}\PY{n}{w\PYZus{}range}\PY{p}{,} \PY{n}{b\PYZus{}range}\PY{p}{)}
    \PY{n}{w\PYZus{}range} \PY{o}{=} \PY{n}{w\PYZus{}range}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
    \PY{n}{b\PYZus{}range} \PY{o}{=} \PY{n}{b\PYZus{}range}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}

    \PY{n}{error\PYZus{}range} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{]}\PY{p}{)}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{min}\PY{p}{(}\PY{n}{w\PYZus{}range}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{b\PYZus{}range}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{:}
        \PY{n}{error\PYZus{}range} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{error\PYZus{}range}\PY{p}{,} \PY{n}{mse\PYZus{}loss}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w\PYZus{}range}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{b\PYZus{}range}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{p}{)}

    \PY{k}{return} \PY{n}{w\PYZus{}range}\PY{p}{,} \PY{n}{b\PYZus{}range}\PY{p}{,} \PY{n}{error\PYZus{}range}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Training the model}\label{training-the-model}

The train function will update the parameters in each epoch and update
the visualization.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{train}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w0}\PY{p}{,} \PY{n}{b0}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{p}{,} \PY{n}{epochs}\PY{p}{,} \PY{n}{output\PYZus{}filename}\PY{p}{)}\PY{p}{:}
    \PY{n}{w} \PY{o}{=} \PY{n}{w0}
    \PY{n}{b} \PY{o}{=} \PY{n}{b0}

    \PY{n}{ax0}\PY{p}{,} \PY{n}{ax1}\PY{p}{,} \PY{n}{camera} \PY{o}{=} \PY{n}{create\PYZus{}plots}\PY{p}{(}\PY{p}{)}
    \PY{n}{loss\PYZus{}dims} \PY{o}{=} \PY{l+m+mi}{20}
    \PY{n}{w\PYZus{}max} \PY{o}{=} \PY{l+m+mf}{0.5}
    \PY{n}{b\PYZus{}max} \PY{o}{=} \PY{l+m+mi}{15}
    \PY{n}{w\PYZus{}range}\PY{p}{,} \PY{n}{b\PYZus{}range}\PY{p}{,} \PY{n}{error\PYZus{}range} \PY{o}{=} \PY{n}{generate\PYZus{}error\PYZus{}range}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{loss\PYZus{}dims}\PY{p}{,} \PY{n}{w\PYZus{}max}\PY{p}{,} \PY{n}{b\PYZus{}max}\PY{p}{)}

    \PY{k}{for} \PY{n}{e} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{epochs}\PY{p}{)}\PY{p}{:}
        \PY{n}{w}\PY{p}{,} \PY{n}{b} \PY{o}{=} \PY{n}{update\PYZus{}w\PYZus{}and\PYZus{}b}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{p}{)}
        \PY{k}{if} \PY{p}{(}
            \PY{p}{(}\PY{n}{e} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}
            \PY{o+ow}{or} \PY{p}{(}\PY{n}{e} \PY{o}{\PYZlt{}} \PY{l+m+mi}{60} \PY{o+ow}{and} \PY{n}{e} \PY{o}{\PYZpc{}} \PY{l+m+mi}{5} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}
            \PY{o+ow}{or} \PY{p}{(}\PY{n}{e} \PY{o}{\PYZlt{}} \PY{l+m+mi}{3000} \PY{o+ow}{and} \PY{n}{e} \PY{o}{\PYZpc{}} \PY{l+m+mi}{1000} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}
            \PY{o+ow}{or} \PY{p}{(}\PY{n}{e} \PY{o}{\PYZpc{}} \PY{l+m+mi}{3000} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}
        \PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} Plot the error given the current slope and y\PYZhy{}intercept}
            \PY{n}{ax1}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{w\PYZus{}range}\PY{p}{,} \PY{n}{b\PYZus{}range}\PY{p}{,} \PY{n}{error\PYZus{}range}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{blue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.05}\PY{p}{)}
            \PY{n}{ax1}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{p}{[}\PY{n}{w}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{b}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{mse\PYZus{}loss}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{)}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{red}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{s}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}

            \PY{c+c1}{\PYZsh{} Plot the linear regression lines}
            \PY{n}{ax0}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}1f77b4}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{o}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.25}\PY{p}{)}
            \PY{n}{X\PYZus{}plot} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{)}
            \PY{n}{ax0}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{X\PYZus{}plot}\PY{p}{,} \PY{n}{X\PYZus{}plot} \PY{o}{*} \PY{n}{w} \PY{o}{+} \PY{n}{b}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{black}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

            \PY{c+c1}{\PYZsh{} print the loss}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{epoch: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb}{str}\PY{p}{(}\PY{n}{e}\PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{loss: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{mse\PYZus{}loss}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{)}\PY{p}{)}\PY{p}{)}
            \PY{n}{camera}\PY{o}{.}\PY{n}{snap}\PY{p}{(}\PY{p}{)}

    \PY{n}{animation} \PY{o}{=} \PY{n}{camera}\PY{o}{.}\PY{n}{animate}\PY{p}{(}\PY{p}{)}
    \PY{n}{animation}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{n}{output\PYZus{}filename}\PY{p}{,} \PY{n}{writer}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pillow}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}

    \PY{k}{return} \PY{n}{w}\PY{p}{,} \PY{n}{b}
\end{Verbatim}
\end{tcolorbox}

    Let's train the linear regression model on a sample dataset.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{fname} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{REGRESSION\PYZhy{}gradientDescent\PYZhy{}data.txt}\PY{l+s+s2}{\PYZdq{}}
\PY{n}{x}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{loadtxt}\PY{p}{(}\PY{n}{fname}\PY{p}{,} \PY{n}{delimiter}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{,}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{unpack}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{skiprows}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{usecols}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
\PY{n}{output\PYZus{}filename} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{gradient\PYZus{}descent.gif}\PY{l+s+s2}{\PYZdq{}}
\PY{n}{train}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mf}{0.00005}\PY{p}{,} \PY{l+m+mi}{4000}\PY{p}{,} \PY{n}{output\PYZus{}filename}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
epoch:  0 loss: 197.25274270926414
epoch:  5 loss: 112.88939636458916
epoch:  10 loss: 74.66409989764938
epoch:  15 loss: 57.34204113744597
epoch:  20 loss: 49.490344392988355
epoch:  25 loss: 45.929244266843334
epoch:  30 loss: 44.31200864892866
epoch:  35 loss: 43.57543700948768
epoch:  40 loss: 43.237843105871576
epoch:  45 loss: 43.08099946508624
epoch:  50 loss: 43.00603965880256
epoch:  55 loss: 42.968173826457125
epoch:  1000 loss: 41.62567353799647
epoch:  2000 loss: 40.307849717949104
epoch:  3000 loss: 39.06382181292731
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{combined_files/combined_27_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
(np.float64(0.4560414772297029), np.float64(1.0259430403235812))
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{output\PYZus{}filename}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<IPython.core.display.Image object>
\end{Verbatim}
\end{tcolorbox}
        
    \section{K-Means Clustering}\label{k-means-clustering}

This is an unsupervised clustering algorithm that assigns points to a
centroid. This is a quick way of grouping data points together and to
identify outliers. The downside of this algorithm is that it takes up a
lot of memory as everything has to be loaded in. However, it is very
easy to implement and has many use cases

    Import the libraries

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{import} \PY{n+nn}{scienceplots}

\PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k+kn}{import} \PY{n}{Image}
\PY{k+kn}{from} \PY{n+nn}{celluloid} \PY{k+kn}{import} \PY{n}{Camera}

\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{style}\PY{o}{.}\PY{n}{use}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{science}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{no\PYZhy{}latex}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Example Dataset}\label{example-dataset}

Let's generate a dataset of random points

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{K} \PY{o}{=} \PY{l+m+mi}{12}
\PY{n}{w} \PY{o}{=} \PY{l+m+mi}{1200}
\PY{n}{h} \PY{o}{=} \PY{l+m+mi}{675}
\PY{n}{nums} \PY{o}{=} \PY{l+m+mi}{100}

\PY{n}{colors} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{n}{K}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}

\PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{n}{nums}\PY{p}{)}
\PY{n}{y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{h}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{n}{nums}\PY{p}{)}
\PY{n}{pts} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{column\PYZus{}stack}\PY{p}{(}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} plot the points}
\PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
\PY{n}{ax} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{111}\PY{p}{)}

\PY{n}{ax}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<matplotlib.collections.PathCollection at 0xffff90423150>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{combined_files/combined_33_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Distance Functions}\label{distance-functions}

When assigning points to centroids, we assign them to the closest
centroid. In order to quantify this, we need to state how we measure
distance. The following are 2 examples of distance functions.

Given point \(p1\) at \((x_1, y_1)\) and point \(p2\) at \((x_2, y_2)\),
we can develop the following distance functions

Euclidean Distance: \(\sqrt{(x_2-x_1)^2 + (y_2-y_1)^2}\)

Manhattan Distance: \(|x_2-x_1| + |y_2-y_1|\)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{euclidean\PYZus{}distance}\PY{p}{(}\PY{n}{p1}\PY{p}{,} \PY{n}{p2}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{p}{(}\PY{n}{p1}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{p2}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{o}{*}\PY{o}{*} \PY{l+m+mi}{2} \PY{o}{+} \PY{p}{(}\PY{n}{p1}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{p2}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{o}{*}\PY{o}{*} \PY{l+m+mi}{2}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{manhattan\PYZus{}distance}\PY{p}{(}\PY{n}{p1}\PY{p}{,} \PY{n}{p2}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{p1}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{p2}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{o}{+} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{p1}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{p2}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsection{K Means Setup}\label{k-means-setup}

At the beginning, the centroids are initialized with random x and y
values.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{centroids\PYZus{}x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{n}{K}\PY{p}{)}
\PY{n}{centroids\PYZus{}y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{h}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{n}{K}\PY{p}{)}
\PY{n}{centroids} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{column\PYZus{}stack}\PY{p}{(}\PY{p}{(}\PY{n}{centroids\PYZus{}x}\PY{p}{,} \PY{n}{centroids\PYZus{}y}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Graphing Functions}\label{graphing-functions}

Create a helper function to create a plot with the sum of the distances
squared on the left and the centroids on the right. Also initialize
variables for the visualization, like the sum of the distances so far.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{create\PYZus{}plots}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{16} \PY{o}{/} \PY{l+m+mf}{9.0} \PY{o}{*} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{4} \PY{o}{*} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{layout}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{constrained}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{fig}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{K\PYZhy{}Means Clustering Unsupervised}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{K Clusters}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontweight}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{normal}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sum of Euclidean Distance Squared}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontweight}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{normal}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Elbow Method}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{off}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{off}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{X}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Y}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Centroids}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{camera} \PY{o}{=} \PY{n}{Camera}\PY{p}{(}\PY{n}{fig}\PY{p}{)}
    \PY{k}{return} \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{camera}

\PY{n}{boundary\PYZus{}div} \PY{o}{=} \PY{l+m+mi}{25}
\PY{n}{x\PYZus{}boundary\PYZus{}inc} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n}{w} \PY{o}{/} \PY{n}{boundary\PYZus{}div}\PY{p}{)}
\PY{n}{y\PYZus{}boundary\PYZus{}inc} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n}{h} \PY{o}{/} \PY{n}{boundary\PYZus{}div}\PY{p}{)}

\PY{n}{x\PYZus{}boundary} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{x\PYZus{}boundary\PYZus{}inc} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{y\PYZus{}boundary} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{h}\PY{p}{,} \PY{n}{y\PYZus{}boundary\PYZus{}inc} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{x\PYZus{}boundary}\PY{p}{,} \PY{n}{y\PYZus{}boundary} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{meshgrid}\PY{p}{(}\PY{n}{x\PYZus{}boundary}\PY{p}{,} \PY{n}{y\PYZus{}boundary}\PY{p}{)}
\PY{n}{colors\PYZus{}idx\PYZus{}boundary} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{K}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{n}{x\PYZus{}boundary}\PY{o}{.}\PY{n}{shape}\PY{p}{)}

\PY{n}{x\PYZus{}boundary\PYZus{}flat} \PY{o}{=} \PY{n}{x\PYZus{}boundary}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
\PY{n}{y\PYZus{}boundary\PYZus{}flat} \PY{o}{=} \PY{n}{y\PYZus{}boundary}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}

\PY{n}{dists} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{K}\PY{p}{)}
\PY{n}{dists\PYZus{}idx} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{K} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Training the Model}\label{training-the-model}

Let's bring everything together. In this visualization, I show the
centroids with varying values of K, which is the total number of
centroids. For every value of K, I run the algorithm for a certain
number of epochs. At the start, centroids start at a random location on
the grid. In each epoch, points are assigned to the closest centroid.
Then, the next location of the centroid is the average x and y value of
all the points assigned to it in the previoius iteration.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{ax0}\PY{p}{,} \PY{n}{ax1}\PY{p}{,} \PY{n}{camera} \PY{o}{=} \PY{n}{create\PYZus{}plots}\PY{p}{(}\PY{p}{)}
\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{8}

\PY{n}{output\PYZus{}filename} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{k\PYZus{}means.gif}\PY{l+s+s2}{\PYZdq{}}

\PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{K} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
    \PY{n}{acc\PYZus{}dist\PYZus{}squared} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{k}{for} \PY{n}{e} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{epochs}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} Draw the boundaries}
        \PY{k}{for} \PY{n}{index} \PY{o+ow}{in} \PY{n}{np}\PY{o}{.}\PY{n}{ndindex}\PY{p}{(}\PY{n}{x\PYZus{}boundary}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{:}
            \PY{n}{x} \PY{o}{=} \PY{n}{x\PYZus{}boundary}\PY{p}{[}\PY{n}{index}\PY{p}{]}
            \PY{n}{y} \PY{o}{=} \PY{n}{y\PYZus{}boundary}\PY{p}{[}\PY{n}{index}\PY{p}{]}

            \PY{n}{colors\PYZus{}idx\PYZus{}boundary}\PY{p}{[}\PY{n}{index}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{n}{min\PYZus{}group} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{c+c1}{\PYZsh{} set min distance to largest possible distance initially}
            \PY{n}{min\PYZus{}dist} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{w}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{o}{+} \PY{n}{h}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}

            \PY{n}{curr\PYZus{}pt} \PY{o}{=} \PY{p}{[}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{]}
            \PY{n}{curr\PYZus{}c} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{k}{for} \PY{n}{c} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{k}\PY{p}{)}\PY{p}{:}
                \PY{n}{curr\PYZus{}c} \PY{o}{=} \PY{n}{centroids}\PY{p}{[}\PY{n}{c}\PY{p}{]}

                \PY{n}{dist} \PY{o}{=} \PY{n}{euclidean\PYZus{}distance}\PY{p}{(}\PY{n}{curr\PYZus{}pt}\PY{p}{,} \PY{n}{curr\PYZus{}c}\PY{p}{)}
                \PY{k}{if} \PY{n}{dist} \PY{o}{\PYZlt{}} \PY{n}{min\PYZus{}dist}\PY{p}{:}
                    \PY{n}{min\PYZus{}dist} \PY{o}{=} \PY{n}{dist}
                    \PY{n}{min\PYZus{}group} \PY{o}{=} \PY{n}{c}
            \PY{n}{colors\PYZus{}idx\PYZus{}boundary}\PY{p}{[}\PY{n}{index}\PY{p}{]} \PY{o}{=} \PY{n}{min\PYZus{}group}

        \PY{n}{colors\PYZus{}boundary} \PY{o}{=} \PY{n}{colors}\PY{p}{[}\PY{n}{colors\PYZus{}idx\PYZus{}boundary}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{]}
        \PY{n}{ax1}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}
            \PY{n}{x\PYZus{}boundary\PYZus{}flat}\PY{p}{,} \PY{n}{y\PYZus{}boundary\PYZus{}flat}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{colors\PYZus{}boundary}\PY{p}{,} \PY{n}{s}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.45}
        \PY{p}{)}

        \PY{c+c1}{\PYZsh{} Assign each point to a centroid}
        \PY{n}{groups} \PY{o}{=} \PY{p}{[}\PY{p}{[}\PY{p}{]} \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{k}\PY{p}{)}\PY{p}{]}
        \PY{n}{acc\PYZus{}dist\PYZus{}squared} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{nums}\PY{p}{)}\PY{p}{:}
            \PY{n}{min\PYZus{}group} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{c+c1}{\PYZsh{} set min distance to largest possible distance initially}
            \PY{n}{min\PYZus{}dist} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{w}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{o}{+} \PY{n}{h}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}

            \PY{n}{curr\PYZus{}pt} \PY{o}{=} \PY{n}{pts}\PY{p}{[}\PY{n}{i}\PY{p}{]}
            \PY{n}{curr\PYZus{}c} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{k}{for} \PY{n}{c} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{k}\PY{p}{)}\PY{p}{:}
                \PY{n}{curr\PYZus{}c} \PY{o}{=} \PY{n}{centroids}\PY{p}{[}\PY{n}{c}\PY{p}{]}

                \PY{n}{dist} \PY{o}{=} \PY{n}{euclidean\PYZus{}distance}\PY{p}{(}\PY{n}{curr\PYZus{}pt}\PY{p}{,} \PY{n}{curr\PYZus{}c}\PY{p}{)}
                \PY{k}{if} \PY{n}{dist} \PY{o}{\PYZlt{}} \PY{n}{min\PYZus{}dist}\PY{p}{:}
                    \PY{n}{min\PYZus{}dist} \PY{o}{=} \PY{n}{dist}
                    \PY{n}{min\PYZus{}group} \PY{o}{=} \PY{n}{c}

            \PY{n}{groups}\PY{p}{[}\PY{n}{min\PYZus{}group}\PY{p}{]}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{curr\PYZus{}pt}\PY{p}{)}
            \PY{n}{acc\PYZus{}dist\PYZus{}squared} \PY{o}{+}\PY{o}{=} \PY{n}{min\PYZus{}dist}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}

        \PY{c+c1}{\PYZsh{} Centroids}
        \PY{k}{for} \PY{n}{g} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{k}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} Draw the centroids}
            \PY{n}{curr\PYZus{}centroid} \PY{o}{=} \PY{n}{centroids}\PY{p}{[}\PY{n}{g}\PY{p}{]}
            \PY{n}{curr\PYZus{}centroid} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{curr\PYZus{}centroid}\PY{p}{]}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{int32}\PY{p}{)}
            \PY{n}{ax1}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{curr\PYZus{}centroid}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{curr\PYZus{}centroid}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{n}{colors}\PY{p}{[}\PY{n}{g}\PY{p}{]}\PY{p}{,} \PY{n}{s}\PY{o}{=}\PY{l+m+mi}{8}\PY{p}{)}

            \PY{n}{group\PYZus{}pts} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{groups}\PY{p}{[}\PY{n}{g}\PY{p}{]}\PY{p}{)}
            \PY{k}{if} \PY{n}{group\PYZus{}pts}\PY{o}{.}\PY{n}{size} \PY{o}{!=} \PY{l+m+mi}{0}\PY{p}{:}
                \PY{c+c1}{\PYZsh{} Draw lines between points and the centroids}
                \PY{n}{pts\PYZus{}in\PYZus{}group} \PY{o}{=} \PY{n}{group\PYZus{}pts}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
                \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{pts\PYZus{}in\PYZus{}group}\PY{p}{)}\PY{p}{:}
                    \PY{n}{group\PYZus{}pt} \PY{o}{=} \PY{n}{group\PYZus{}pts}\PY{p}{[}\PY{n}{i}\PY{p}{]}
                    \PY{n}{ax1}\PY{o}{.}\PY{n}{plot}\PY{p}{(}
                        \PY{p}{[}\PY{n}{group\PYZus{}pt}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{centroids}\PY{p}{[}\PY{n}{g}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{,}
                        \PY{p}{[}\PY{n}{group\PYZus{}pt}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{centroids}\PY{p}{[}\PY{n}{g}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{,}
                        \PY{n}{color}\PY{o}{=}\PY{n}{colors}\PY{p}{[}\PY{n}{g}\PY{p}{]}\PY{p}{,}
                        \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,}
                        \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.55}\PY{p}{,}
                    \PY{p}{)}

                \PY{c+c1}{\PYZsh{} Update the location of the centroids}
                \PY{n}{new\PYZus{}centroid} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{group\PYZus{}pts}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
                \PY{n}{centroids}\PY{p}{[}\PY{n}{g}\PY{p}{]} \PY{o}{=} \PY{n}{new\PYZus{}centroid}
                \PY{n}{new\PYZus{}centroid} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{new\PYZus{}centroid}\PY{p}{]}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{int32}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Draw the points}
        \PY{n}{ax1}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{pts}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{pts}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{black}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{s}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.3}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Draw the Elbow Method graph}
        \PY{k}{if} \PY{n}{k} \PY{o}{\PYZhy{}} \PY{l+m+mi}{2} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{:}
            \PY{n}{ax0}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{dists\PYZus{}idx}\PY{p}{[}\PY{p}{:} \PY{n}{k} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{dists}\PY{p}{[}\PY{p}{:} \PY{n}{k} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{red}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{camera}\PY{o}{.}\PY{n}{snap}\PY{p}{(}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} if e \PYZpc{} 2 == 0:}
        \PY{c+c1}{\PYZsh{}     camera.snap()}
        \PY{c+c1}{\PYZsh{} else:}
        \PY{c+c1}{\PYZsh{}     ax0.clear()}
        \PY{c+c1}{\PYZsh{}     ax1.clear()}

    \PY{n}{acc\PYZus{}dist\PYZus{}squared} \PY{o}{/}\PY{o}{=} \PY{n}{nums}
    \PY{n}{dists}\PY{p}{[}\PY{n}{k} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{n}{acc\PYZus{}dist\PYZus{}squared}
    \PY{n+nb}{print}\PY{p}{(}\PY{n}{k}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{acc\PYZus{}dist\PYZus{}squared}\PY{p}{)}

\PY{n}{animation} \PY{o}{=} \PY{n}{camera}\PY{o}{.}\PY{n}{animate}\PY{p}{(}\PY{p}{)}
\PY{n}{animation}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{k\PYZus{}means.gif}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{writer}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pillow}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{close}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
0 158627.63
1 62379.95
2 47984.72
3 40085.6
4 28638.72
5 24290.07
6 20755.56
7 15453.13
8 12900.14
9 12373.94
10 12023.81
11 11002.26
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{output\PYZus{}filename}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<IPython.core.display.Image object>
\end{Verbatim}
\end{tcolorbox}
        
    \section{Principal Component
Analysis}\label{principal-component-analysis}

An important decision to make when training machine learning models is
the the features to use for the training dataset. Principal Component
Analysis allows you to see which features account for most of the
variance, simplifying the dataset to a smaller number of correlated
variables.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{o}{\PYZpc{}}\PY{k}{matplotlib} widget
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{mpl\PYZus{}toolkits}\PY{n+nn}{.}\PY{n+nn}{mplot3d} \PY{k+kn}{import} \PY{n}{Axes3D}
\PY{k+kn}{import} \PY{n+nn}{scienceplots}
\PY{k+kn}{from} \PY{n+nn}{celluloid} \PY{k+kn}{import} \PY{n}{Camera}

\PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k+kn}{import} \PY{n}{Image}

\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{style}\PY{o}{.}\PY{n}{use}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{science}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{no\PYZhy{}latex}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Target Dataset}\label{target-dataset}

Let's generate a noisy list of points by generating points between a
start and end point and by adding random noise to each point

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{generate\PYZus{}noisy\PYZus{}hyperplane}\PY{p}{(}\PY{n}{num\PYZus{}points}\PY{p}{,} \PY{n}{start\PYZus{}pt}\PY{p}{,} \PY{n}{end\PYZus{}pt}\PY{p}{,} \PY{n}{noise}\PY{o}{=}\PY{l+m+mf}{0.25}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} create a plane from the start to the end point}
    \PY{n}{t} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mf}{0.0} \PY{o}{+} \PY{n}{noise}\PY{p}{,} \PY{l+m+mf}{1.0} \PY{o}{\PYZhy{}} \PY{n}{noise}\PY{p}{,} \PY{n}{num\PYZus{}points}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{points} \PY{o}{=} \PY{n}{start\PYZus{}pt} \PY{o}{+} \PY{n}{t} \PY{o}{*} \PY{p}{(}\PY{n}{end\PYZus{}pt} \PY{o}{\PYZhy{}} \PY{n}{start\PYZus{}pt}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} add noise to plane}
    \PY{n}{noise} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{noise}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{p}{(}\PY{n}{num\PYZus{}points}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}
    \PY{n}{points} \PY{o}{=} \PY{n}{points} \PY{o}{+} \PY{n}{noise}

    \PY{k}{return} \PY{n}{points}


\PY{n}{start\PYZus{}pt} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n}{end\PYZus{}pt} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n}{X} \PY{o}{=} \PY{n}{generate\PYZus{}noisy\PYZus{}hyperplane}\PY{p}{(}\PY{l+m+mi}{200}\PY{p}{,} \PY{n}{start\PYZus{}pt}\PY{p}{,} \PY{n}{end\PYZus{}pt}\PY{p}{)}

\PY{c+c1}{\PYZsh{} plot the points}
\PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
\PY{n}{ax} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{111}\PY{p}{,} \PY{n}{projection}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{3d}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{blue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Original Data}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{combined_files/combined_46_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Eigenvectors and
Eigenvalues}\label{eigenvectors-and-eigenvalues}

When you multiply a matrix with its eigenvector, you get a multiple of
the eigenvector. The scalar multiple is the eigenvector's eigenvalue.
The process of finding the eigenvectors and eigenvalues of a matrix is
called Eigendecomposition.

\[A \vec{v} = \lambda \vec{v}\]

The scalar \(\lambda\) is the eigenvalue and the vector \(\vec{v}\) is
the corresponding eigenvector. The eigendecomposition typically involves
solving the determinant \(det(A - \lambda I) = 0\), where \(I\) is the
identity matrix.

Use numpy to quickly get the eigenvalues and eigenvectors of a matrix

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{mat }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{4}\NormalTok{, }\OperatorTok{{-}}\DecValTok{2}\NormalTok{],}
\NormalTok{                [}\DecValTok{1}\NormalTok{,  }\DecValTok{1}\NormalTok{]])}
\NormalTok{eig\_vals, eig\_vecs }\OperatorTok{=}\NormalTok{ np.linalg.eig(mat)}
\end{Highlighting}
\end{Shaded}

    \subsection{Lagrange Multipliers (Optimization with
constraints)}\label{lagrange-multipliers-optimization-with-constraints}

Recall from Multivariable Calculus that Lagrange Multipliers allow you
to find the extrema of a function \(f(x, y, z, ...)\) that is subject to
a constraint function \(g(x, y, z, ...)=0\).

The Lagrange Multipliers technique states that the solution of this
constrainted optimization problem is the solution to the following
system of equations:

\[\nabla L = 0\]

where

\[L(x, y, z, ... \lambda) = f(x, y, z, ... \lambda) - \lambda g(x, y, z, ... \lambda)\]

    \subsection{PCA Derivation (Eigendecomposition of Covariance
Matrix)}\label{pca-derivation-eigendecomposition-of-covariance-matrix}

Recall that our goal is to find the vectors \(v\) that account for most
of the variance.

Given input vector \(x_i\) and vector \(v\), we want to project every
input point to \(v\) in each dimension.

\[z_i = x_i^Tv\]

The variance is

\[(x_i^Tv)^2 = z_i^2\]

To find the maximum variance across all of the projections for the \(n\)
dimensions.

\begin{align*}
\max \sum_{i=1}^{n} (x_i^Tv)^2 &= \max \sum_{i=1}^{n} z_i^2 \\ 
&= \max z^Tz \\ 
&= \arg\max (xv)^Txv \\ 
\end{align*}

Since the ratios of the Principal Components is all that matters, let's
introduce the constaint that \[v^Tv = 1\]

    Solving the constrainted optimization with Lagrange Multipliers, we
define the Lagrangian function:

\[ L = \arg\max v^Tx^Txv - \lambda (v^Tv - 1)\]

Let's solve the Lagrangian function by solving \(\nabla L = 0\)

\begin{align*}
0 &= \frac{\partial L}{\partial v} \\ 
&= \frac{\partial}{\partial v}[v^Tx^Txv - \lambda (v^Tv - 1)]  \\ 
&= 2x^Txv - 2\lambda v  \\
&= x^Txv - \lambda v  \\
&= (x^Tx)v - \lambda v  \\
(x^Tx)v &= \lambda v  \\
\end{align*}

Given that \(x^Tx\) is the covariance of a matrix, we see that the
solution to PCA is simply the eigendecomposition of the covariance
matrix.

    \subsection{PCA Implementation}\label{pca-implementation}

To recap the two sections above, PCA consists of the following parts: 1.
Standard the input data by dividing the difference of the data and the
mean by the standard deviation. 2. Compute the covariance matrix of the
standardized input 3. Compute eigenvalues and eigenvectors of the
covariance matrix 4. To get the projected data, matrix multiply the
standardized input and the eigenvectors.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{pca}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{dims}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} subtract the mean to center the data and divide by standard deviation}
    \PY{n}{X\PYZus{}centered} \PY{o}{=} \PY{p}{(}\PY{n}{X} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} compute covariance matrix}
    \PY{n}{cov} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{cov}\PY{p}{(}\PY{n}{X\PYZus{}centered}\PY{o}{.}\PY{n}{T}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} eigendecomposition of the covariance matrix}
    \PY{c+c1}{\PYZsh{} the eigenvectors are the principal components}
    \PY{c+c1}{\PYZsh{} the principal components are the columns of the eig\PYZus{}vecs matrix}
    \PY{n}{eig\PYZus{}vals}\PY{p}{,} \PY{n}{eig\PYZus{}vecs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{eig}\PY{p}{(}\PY{n}{cov}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} sort the eigenvalues and eigenvectors}
    \PY{n}{sorted\PYZus{}idx} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argsort}\PY{p}{(}\PY{n}{eig\PYZus{}vals}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
    \PY{n}{eig\PYZus{}vals} \PY{o}{=} \PY{n}{eig\PYZus{}vals}\PY{p}{[}\PY{n}{sorted\PYZus{}idx}\PY{p}{]}
    \PY{n}{eig\PYZus{}vecs} \PY{o}{=} \PY{n}{eig\PYZus{}vecs}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{sorted\PYZus{}idx}\PY{p}{]}

    \PY{c+c1}{\PYZsh{} perform dimensionality reduction using the computed principal components}
    \PY{c+c1}{\PYZsh{} if you want to reduce to K dimensions, simplify take the first K columns}
    \PY{n}{projected} \PY{o}{=} \PY{n}{X\PYZus{}centered} \PY{o}{@} \PY{n}{eig\PYZus{}vecs}

    \PY{c+c1}{\PYZsh{} compute the variance of each dimension (column)}
    \PY{n}{pc\PYZus{}variances} \PY{o}{=} \PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{var}\PY{p}{(}\PY{n}{projected}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{dims}\PY{p}{)}\PY{p}{]}

    \PY{k}{return} \PY{n}{eig\PYZus{}vals}\PY{p}{,} \PY{n}{eig\PYZus{}vecs}\PY{p}{,} \PY{n}{projected}\PY{p}{,} \PY{n}{pc\PYZus{}variances}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Graphing Functions}\label{graphing-functions}

Utility functions to create the visualizations

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{create\PYZus{}plots}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{16} \PY{o}{/} \PY{l+m+mf}{9.0} \PY{o}{*} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{4} \PY{o}{*} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
    \PY{n}{fig}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Principal Component Analysis}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{ax0} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{121}\PY{p}{,} \PY{n}{projection}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{3d}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax0}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{X}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax0}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Y}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax0}\PY{o}{.}\PY{n}{set\PYZus{}zlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Z}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax0}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{PC Hyperplanes}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax0}\PY{o}{.}\PY{n}{view\PYZus{}init}\PY{p}{(}\PY{l+m+mi}{17}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{125}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
    \PY{n}{ax0}\PY{o}{.}\PY{n}{set\PYZus{}xlim}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{ax0}\PY{o}{.}\PY{n}{set\PYZus{}ylim}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{ax0}\PY{o}{.}\PY{n}{set\PYZus{}zlim}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{ax0}\PY{o}{.}\PY{n}{tick\PYZus{}params}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{both}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{which}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{both}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{length}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}

    \PY{n}{ax1} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{122}\PY{p}{,} \PY{n}{projection}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{3d}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{X}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Y}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{set\PYZus{}zlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Z}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Projected Data}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{view\PYZus{}init}\PY{p}{(}\PY{l+m+mi}{17}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{125}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} ax1.set\PYZus{}xlim(\PYZhy{}1, 1)}
    \PY{c+c1}{\PYZsh{} ax1.set\PYZus{}ylim(\PYZhy{}1, 1)}
    \PY{c+c1}{\PYZsh{} ax1.set\PYZus{}zlim(\PYZhy{}1, 1)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{tick\PYZus{}params}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{both}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{which}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{both}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{length}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} plt.axis(\PYZsq{}equal\PYZsq{})}

    \PY{n}{camera} \PY{o}{=} \PY{n}{Camera}\PY{p}{(}\PY{n}{fig}\PY{p}{)}
    \PY{k}{return} \PY{n}{ax0}\PY{p}{,} \PY{n}{ax1}\PY{p}{,} \PY{n}{camera}


\PY{k}{def} \PY{n+nf}{plot\PYZus{}hyperplane}\PY{p}{(}\PY{n}{ax}\PY{p}{,} \PY{n}{pc\PYZus{}vector}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{red}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{scaling}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.3}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Create a grid of points}
    \PY{n}{points} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{scaling}\PY{p}{)}
    \PY{n}{xx}\PY{p}{,} \PY{n}{yy} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{meshgrid}\PY{p}{(}\PY{n}{points}\PY{p}{,} \PY{n}{points}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} the z value is the defined by the hyperplane from the principal component vector}
    \PY{n}{pc\PYZus{}vector} \PY{o}{/}\PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{pc\PYZus{}vector}\PY{p}{)}
    \PY{n}{z} \PY{o}{=} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{pc\PYZus{}vector}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{*} \PY{n}{xx} \PY{o}{\PYZhy{}} \PY{n}{pc\PYZus{}vector}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{*} \PY{n}{yy}\PY{p}{)} \PY{o}{/} \PY{n}{pc\PYZus{}vector}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}

    \PY{n}{ax}\PY{o}{.}\PY{n}{plot\PYZus{}surface}\PY{p}{(}\PY{n}{xx}\PY{p}{,} \PY{n}{yy}\PY{p}{,} \PY{n}{z}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{n}{color}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{n}{alpha}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Visualize PCA}\label{visualize-pca}

Given the derivation of PCA, let's visualize the projected data with
different values for the target dimension.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{visualize\PYZus{}pca}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{dims}\PY{p}{,} \PY{n}{output\PYZus{}filename}\PY{p}{)}\PY{p}{:}
    \PY{n}{ax0}\PY{p}{,} \PY{n}{ax1}\PY{p}{,} \PY{n}{camera} \PY{o}{=} \PY{n}{create\PYZus{}plots}\PY{p}{(}\PY{p}{)}
    \PY{n}{colors} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{red}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{green}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{blue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}

    \PY{k}{for} \PY{n}{dim} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{dims} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
        \PY{n}{eig\PYZus{}vals}\PY{p}{,} \PY{n}{eig\PYZus{}vecs}\PY{p}{,} \PY{n}{projected}\PY{p}{,} \PY{n}{pc\PYZus{}variances} \PY{o}{=} \PY{n}{pca}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{dims}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} plot the original data}
        \PY{n}{ax0}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{blue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Original Data}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} plot the pca hyperplanes}
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{dim}\PY{p}{)}\PY{p}{:}
            \PY{n}{plot\PYZus{}hyperplane}\PY{p}{(}\PY{n}{ax0}\PY{p}{,} \PY{n}{eig\PYZus{}vecs}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{n}{colors}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} plot the projected data from the principal components}
        \PY{n}{curr\PYZus{}projected} \PY{o}{=} \PY{n}{projected}
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{dim}\PY{p}{,} \PY{n}{dims}\PY{p}{)}\PY{p}{:}
            \PY{k}{if} \PY{n}{i} \PY{o}{\PYZlt{}} \PY{n}{dims}\PY{p}{:}
                \PY{n}{curr\PYZus{}projected}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{k}{if} \PY{n}{dim} \PY{o}{!=} \PY{l+m+mi}{0}\PY{p}{:}
            \PY{n}{ax1}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}
                \PY{n}{curr\PYZus{}projected}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
                \PY{n}{curr\PYZus{}projected}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
                \PY{n}{curr\PYZus{}projected}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}
                \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{blue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
                \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Projected Data}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
            \PY{p}{)}

        \PY{n}{camera}\PY{o}{.}\PY{n}{snap}\PY{p}{(}\PY{p}{)}

    \PY{n}{animation} \PY{o}{=} \PY{n}{camera}\PY{o}{.}\PY{n}{animate}\PY{p}{(}\PY{n}{interval}\PY{o}{=}\PY{l+m+mi}{2000}\PY{p}{)}
    \PY{n}{animation}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{n}{output\PYZus{}filename}\PY{p}{,} \PY{n}{writer}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pillow}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}

    \PY{n}{eig\PYZus{}vals}\PY{p}{,} \PY{n}{eig\PYZus{}vecs}\PY{p}{,} \PY{n}{projected}\PY{p}{,} \PY{n}{pc\PYZus{}variances} \PY{o}{=} \PY{n}{pca}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{dims}\PY{p}{)}

    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{variance percentage per principal component}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{variance\PYZus{}percentage} \PY{o}{=} \PY{n}{eig\PYZus{}vals} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{eig\PYZus{}vals}\PY{p}{)}
    \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{percentage} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{variance\PYZus{}percentage}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{th PC: }\PY{l+s+si}{\PYZob{}}\PY{n+nb}{round}\PY{p}{(}\PY{n}{percentage}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{,}\PY{+w}{ }\PY{l+m+mi}{2}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{variance per principal component}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{variance} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{pc\PYZus{}variances}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{th PC: }\PY{l+s+si}{\PYZob{}}\PY{n+nb}{round}\PY{p}{(}\PY{n}{variance}\PY{p}{,}\PY{+w}{ }\PY{l+m+mi}{2}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{hyperplanes}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{dim}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{hyperplane }\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{: }\PY{l+s+si}{\PYZob{}}\PY{n}{eig\PYZus{}vecs}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{+w}{ }\PY{n}{i}\PY{p}{]}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{dims} \PY{o}{=} \PY{l+m+mi}{3}
\PY{n}{output\PYZus{}filename} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pca.gif}\PY{l+s+s2}{\PYZdq{}}

\PY{n}{visualize\PYZus{}pca}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{dims}\PY{p}{,} \PY{n}{output\PYZus{}filename}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{combined_files/combined_57_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
variance percentage per principal component
1th PC: 69.12\%
2th PC: 17.52\%
3th PC: 13.36\%
variance per principal component
1th PC: 2.07
2th PC: 0.53
3th PC: 0.4

hyperplanes
hyperplane 0: [-0.58180084 -0.55533668 -0.59422972]
hyperplane 1: [ 0.51390531 -0.81729222  0.26064299]
hyperplane 2: [-0.63040394 -0.1537355   0.76089176]
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{output\PYZus{}filename}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<IPython.core.display.Image object>
\end{Verbatim}
\end{tcolorbox}
        
    \subsection{Scikit-Learn
Implementation}\label{scikit-learn-implementation}

As a check for correctness, let's compare our results with the PCA
module from scikit-learn.

Note: The sign of the values might not match exactly. They just need to
have the same ratios, which they do. Our implementation matches the one
from scikit-learn.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k+kn}{import} \PY{n}{PCA}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k+kn}{import} \PY{n}{StandardScaler}

\PY{n}{scaler} \PY{o}{=} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}
\PY{n}{X\PYZus{}centered} \PY{o}{=} \PY{n}{scaler}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X}\PY{p}{)}

\PY{n}{pca} \PY{o}{=} \PY{n}{PCA}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{dims}\PY{p}{)}
\PY{n}{projected} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}centered}\PY{p}{)}
\PY{n}{eig\PYZus{}vecs} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{components\PYZus{}}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{hyperplanes}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{dims}\PY{p}{)}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{hyperplane }\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{: }\PY{l+s+si}{\PYZob{}}\PY{n}{eig\PYZus{}vecs}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]

hyperplanes
hyperplane 0: [0.58180084 0.55533668 0.59422972]
hyperplane 1: [-0.51390531  0.81729222 -0.26064299]
hyperplane 2: [-0.63040394 -0.1537355   0.76089176]
    \end{Verbatim}

    Let's also plot scikit's learn projected data. Our implementation seems
to match for the projected as well.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
\PY{n}{ax} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{111}\PY{p}{,} \PY{n}{projection}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{3d}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{projected}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{projected}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{projected}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{blue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Projected Data}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{combined_files/combined_62_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \section{Logistic Regression}\label{logistic-regression}

Binary Classification model that finds the optimal the weights and bias
and returns probabilites of the two classes

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} import numpy as np}
\PY{k+kn}{import} \PY{n+nn}{autograd}\PY{n+nn}{.}\PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{from} \PY{n+nn}{autograd} \PY{k+kn}{import} \PY{n}{grad}
\PY{k+kn}{from} \PY{n+nn}{autograd} \PY{k+kn}{import} \PY{n}{elementwise\PYZus{}grad} \PY{k}{as} \PY{n}{egrad}

\PY{o}{\PYZpc{}}\PY{k}{matplotlib} widget
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{mpl\PYZus{}toolkits}\PY{n+nn}{.}\PY{n+nn}{mplot3d} \PY{k+kn}{import} \PY{n}{Axes3D}

\PY{k+kn}{import} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{datasets} \PY{k}{as} \PY{n+nn}{skdatasets}

\PY{k+kn}{from} \PY{n+nn}{celluloid} \PY{k+kn}{import} \PY{n}{Camera}
\PY{k+kn}{import} \PY{n+nn}{scienceplots}
\PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k+kn}{import} \PY{n}{Image}

\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{style}\PY{o}{.}\PY{n}{use}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{science}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{no\PYZhy{}latex}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Training Dataset}\label{training-dataset}

Let's import the breast cancer dataset. The logistic regression will
perform binary classification using the mean perimeter and mean radius
of the tumor.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{dataset} \PY{o}{=} \PY{n}{skdatasets}\PY{o}{.}\PY{n}{load\PYZus{}breast\PYZus{}cancer}\PY{p}{(}\PY{p}{)}

\PY{n}{features\PYZus{}used} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{8}\PY{p}{]}
\PY{n}{X} \PY{o}{=} \PY{n}{dataset}\PY{o}{.}\PY{n}{data}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features\PYZus{}used}\PY{p}{]}
\PY{n}{feature\PYZus{}names} \PY{o}{=} \PY{n}{dataset}\PY{o}{.}\PY{n}{feature\PYZus{}names}\PY{p}{[}\PY{n}{features\PYZus{}used}\PY{p}{]}

\PY{c+c1}{\PYZsh{} min\PYZhy{}max normalize the features along the columns}
\PY{n}{X\PYZus{}min\PYZus{}vals} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{X\PYZus{}max\PYZus{}vals} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{X} \PY{o}{=} \PY{p}{(}\PY{n}{X} \PY{o}{\PYZhy{}} \PY{n}{X\PYZus{}min\PYZus{}vals}\PY{p}{)} \PY{o}{/} \PY{p}{(}\PY{n}{X\PYZus{}max\PYZus{}vals} \PY{o}{\PYZhy{}} \PY{n}{X\PYZus{}min\PYZus{}vals}\PY{p}{)}

\PY{n}{Y} \PY{o}{=} \PY{n}{dataset}\PY{o}{.}\PY{n}{target}
\PY{n}{target\PYZus{}names} \PY{o}{=} \PY{n}{dataset}\PY{o}{.}\PY{n}{target\PYZus{}names}

\PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
\PY{n}{ax} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{p}{)}

\PY{n}{ax}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{Y}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{n}{feature\PYZus{}names}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{n}{feature\PYZus{}names}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Breast Cancer Dataset}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, 'Breast Cancer Dataset')
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{combined_files/combined_66_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Activation Function}\label{activation-function}

Recall that the output of the perceptron was the dot product between the
weight vector \(\vec{w}\) and the input vector \(\vec{x}\) plus a
constant bias term \(b\)

Perceptron: \(y=w^Tx + b\)

Activation functions are applied after the computation.

    \subsubsection{Sigmoid Function}\label{sigmoid-function}

In order to do binary classification, we would like to limit the value
of the output to be in the range (0, 1) and get a value to represent the
probability of the output being assigned to either class. The sigmoid
function is perfect for this

\[ \sigma(z) = \frac{1}{1+e^-z} \]

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{sigmoid} \PY{o}{=} \PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{l+m+mi}{1} \PY{o}{/} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{x}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{plot}\PY{p}{(}\PY{n}{fx}\PY{p}{,} \PY{n}{x\PYZus{}min}\PY{p}{,} \PY{n}{x\PYZus{}max}\PY{p}{,} \PY{n}{points}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{title}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{:}

    \PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{x\PYZus{}min}\PY{p}{,} \PY{n}{x\PYZus{}max}\PY{p}{,} \PY{n}{points}\PY{p}{)}
    \PY{n}{y} \PY{o}{=} \PY{n}{fx}\PY{p}{(}\PY{n}{x}\PY{p}{)}

    \PY{n}{fig}\PY{p}{,} \PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{p}{)}
    \PY{n}{axes}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
    \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{X}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Y}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Activation Function}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}


\PY{n}{x\PYZus{}min} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{10}
\PY{n}{x\PYZus{}max} \PY{o}{=} \PY{l+m+mi}{10}
\PY{n}{points} \PY{o}{=} \PY{l+m+mi}{100}
\PY{n}{plot}\PY{p}{(}\PY{n}{sigmoid}\PY{p}{,} \PY{n}{x\PYZus{}min}\PY{p}{,} \PY{n}{x\PYZus{}max}\PY{p}{,} \PY{n}{points}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{combined_files/combined_70_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Gradient of Sigmoid
Function}\label{gradient-of-sigmoid-function}

Gradient Descent will be used later to find the optimal weight values.
As a result, let's calculate the gradient of the sigmoid function.

    \begin{align*}
\sigma^\prime
&= \frac{\partial}{\partial z} \sigma(z) \\
&= \frac{\partial}{\partial z} (\frac{1}{1+e^{-z}}) \\
&= \frac{\partial}{\partial z} (1+e^{-z})^{-1}) \\
&= (-1)(1+e^{-z})^{-2}\frac{\partial}{\partial z}(1+e^{-z}) \\
&= (-1)(1+e^{-z})^{-2}(e^{-z})\frac{\partial}{\partial z}(-z) \\
&= (-1)(1+e^{-z})^{-2}(e^{-z})(-1) \\
&= \frac{e^{-z}}{(1+e^{-z})^{2}}
\end{align*}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{sigmoid\PYZus{}prime} \PY{o}{=} \PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{x}\PY{p}{)} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{x}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
\PY{n}{plot}\PY{p}{(}\PY{n}{sigmoid\PYZus{}prime}\PY{p}{,} \PY{n}{x\PYZus{}min}\PY{p}{,} \PY{n}{x\PYZus{}max}\PY{p}{,} \PY{n}{points}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{combined_files/combined_73_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Autograd}\label{autograd}

Alternatively, you can use autograd to differentiate a numpy function.
Pytorch and JAX also implement autograd.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} grad() differentiates scalar inputs}
\PY{n}{sigmoid\PYZus{}prime\PYZus{}grad} \PY{o}{=} \PY{n}{grad}\PY{p}{(}\PY{n}{sigmoid}\PY{p}{)}
\PY{c+c1}{\PYZsh{} egrad() differentiates vectorized inputs}
\PY{n}{sigmoid\PYZus{}prime\PYZus{}egrad} \PY{o}{=} \PY{n}{egrad}\PY{p}{(}\PY{n}{sigmoid}\PY{p}{)}

\PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{x\PYZus{}min}\PY{p}{,} \PY{n}{x\PYZus{}max}\PY{p}{,} \PY{n}{points}\PY{p}{)}
\PY{k}{assert} \PY{n}{sigmoid\PYZus{}prime\PYZus{}grad}\PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{o}{==} \PY{n}{sigmoid\PYZus{}prime}\PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{allclose}\PY{p}{(}\PY{n}{sigmoid\PYZus{}prime\PYZus{}egrad}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{,} \PY{n}{sigmoid\PYZus{}prime}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}

\PY{n}{plot}\PY{p}{(}\PY{n}{sigmoid\PYZus{}prime\PYZus{}egrad}\PY{p}{,} \PY{n}{x\PYZus{}min}\PY{p}{,} \PY{n}{x\PYZus{}max}\PY{p}{,} \PY{n}{points}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{combined_files/combined_75_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Loss Function}\label{loss-function}

The binary cross entropy loss function will be used for logistic
regression. This loss function is derived from the definition of maximum
likelihood estimation.

For the binary classification model, the probability of seeing the first
class is the sigmoid activation function is applied over the sum of the
bias \(b\) and the dot product of the weight vector \(\vec{w}\) and the
input vector \(\vec{x}\). The probability of seeing the other class is
the difference between 1 and the probability of seeing the other class.

\[ P(Y=1 \mid \vec{x}; \vec{w}, b) = \sigma{(\vec{w} \cdot \vec{x} + b)}
\]

\[ P(Y=0 \mid \vec{x}; \vec{w}, b) = 1 - P(Y=1 \mid \vec{x}; \vec{w}, b)
\]

    \subsubsection{Maximum Likelihood
Estimation}\label{maximum-likelihood-estimation}

Maximum likelihhod estimation finds the optimal weights and bias to
maximize the probability of seeing the training data.

The probability of getting the correct binary prediction function in
terms of \(\vec{w}\) and \(b\) is the following. This can also be
thought of as a Bernoulli distribution.

\[
P(Y \mid \vec{x}; \vec{w}, b) =
\left[ \sigma(\vec{w} \cdot \vec{x} + b) \right]^{y} \,
\left[ 1 - \sigma(\vec{w} \cdot \vec{x} + b) \right]^{1 - y}
\]

With a training dataset of \(i\) examples of \(\vec{x_i}\) features and
\(y_i\) labels, so the total probability is written as the product of
the probabilites of all the training examples. Consider this as the
likelihood of the training dataset with the current weights and bias.

\[
P(Y \mid \vec{x_i}; \vec{w}, b) 
= \prod_{i=1}^{n}
\left[ \sigma(\vec{w} \cdot \vec{x_i} + b) \right]^{y_i}
\left[ 1 - \sigma(\vec{w} \cdot \vec{x_i} + b) \right]^{1 - y_i}
\]

Keep in mind that we want to find the set of optimal parameters
\(\vec{w}\) and \(b\) that maximize the total likelihood.

\[
P(Y \mid \vec{x_i}; \vec{w}, b) 
= \max_{\vec{w}, b} \;
\prod_{i=1}^{n}
\left[ \sigma(\vec{w} \cdot \vec{x_i} + b) \right]^{y_i}
\left[ 1 - \sigma(\vec{w} \cdot \vec{x_i} + b) \right]^{1 - y_i}
\]

    In order to find the optimal weights and bias for the logistic
regression model, we use gradient descent, which is a solution to
optimization problems. We have to take the partial derivative of the
likelihood with respect to \(\vec{w}\) and \(b\).

In it's current form, the total probability is a lot of multiplications.
Per the product rule for derivatives, the partial derivatives will also
be a lot of multiplication. In order to avoid this, we can take the
logarithm of the likelihood, which converts the multiplications into
additions.

\begin{align*}
\ln(P(Y \mid \vec{x_i}; \vec{w}, b)) &= \max_{\vec{w}, b} \ln(\prod_{i=1}^{n} [\sigma{(\vec{w} \cdot \vec{x_i} + b)}]^{y_i} [1 - \sigma{(\vec{w} \cdot \vec{x_i} + b)}]^{1-y_i}) \\
&= \max_{\vec{w}, b} \sum_{i=1}^{n}[ \ln([\sigma{(\vec{w} \cdot \vec{x_i} + b)}]^{y_i} [1 - \sigma{(\vec{w} \cdot \vec{x_i} + b)}]^{1-y_i})] \\
&= \max_{\vec{w}, b} \sum_{i=1}^{n}[ \ln([\sigma{(\vec{w} \cdot \vec{x_i} + b)}]^{y_i}) + \ln([1 - \sigma{(\vec{w} \cdot \vec{x_i} + b)}]^{1-y_i})] \\
&= \max_{\vec{w}, b} \sum_{i=1}^{n}[ y_i \ln(\sigma{(\vec{w} \cdot \vec{x_i} + b)}) + (1-y_i) \ln(1 - \sigma{(\vec{w} \cdot \vec{x_i} + b)})] \\
\end{align*}

We define the negative (multiply equation above by -1) log of the
likelihood as the binary cross entropy loss function. Let's also divide
by the number of training examples to make this the average loss across
the \(n\) examples.

\begin{align*}
L(\vec{w}, b) = -\frac{1}{n}  \sum_{i=1}^{n}[ y_i \ln(\sigma{(\vec{w} \cdot \vec{x_i} + b)}) + (1-y_i) \ln(1 - \sigma{(\vec{w} \cdot \vec{x_i} + b)})] 
\end{align*}

    \subsubsection{Binary Cross Entropy Loss
Function}\label{binary-cross-entropy-loss-function}

Recall that
\(\hat y = \sigma{(\vec{w} \cdot \vec{x} + b)} = \frac{1}{1+e^-(\vec{w} \cdot \vec{x} + b)}\)

To simplify the calculation of the loss, let's rewrite it in terms of
\(\hat y\)

\begin{align*}
L(\hat y) &= -\frac{1}{n} \sum_{i=1}^{n} [y_i \ln(\hat y_i) + (1-y_i) \ln(1 - \hat y_i)]
\end{align*}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{bce}\PY{p}{(}\PY{n}{y\PYZus{}true}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{y\PYZus{}true} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{y\PYZus{}pred}\PY{p}{)} \PY{o}{+} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}true}\PY{p}{)} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Loss Function in Terms of W and
B}\label{loss-function-in-terms-of-w-and-b}

Before taking the partial derivative of the loss function with respect
to \(\vec{w}\) and \(b\). Let's simplify it to make the partial
derivative calculaton easier.

\(\hat y = \sigma{(\vec{w} \cdot \vec{x} + b)} = \frac{1}{1+e^-(\vec{w} \cdot \vec{x} + b)}\)

\begin{align*}
L(\vec{w}, b) &= -\frac{1}{n}  \sum_{i=1}^{n}[ y_i \ln(\sigma{(\vec{w} \cdot \vec{x_i} + b)}) + (1-y_i) \ln(1 - \sigma{(\vec{w} \cdot \vec{x_i} + b)})] \\ 
&= -\frac{1}{n}  \sum_{i=1}^{n}[ y_i \ln(\sigma{(\vec{w} \cdot \vec{x_i} + b)}) - y_i \ln(1 - \sigma{(\vec{w} \cdot \vec{x_i} + b)}) + \ln(1 - \sigma{(\vec{w} \cdot \vec{x_i} + b)})] \\ 
&= -\frac{1}{n}  \sum_{i=1}^{n}[ y_i \ln(\frac{\sigma{(\vec{w} \cdot \vec{x_i} + b)}}{1 - \sigma{(\vec{w} \cdot \vec{x_i} + b)}}) + \ln(1 - \sigma{(\vec{w} \cdot \vec{x_i} + b)})] \\
&= -\frac{1}{n}  \sum_{i=1}^{n}[ y_i \ln(\frac{\frac{1}{1+e^{-(\vec{w} \cdot \vec{x} + b)}}}{1 - \frac{1}{1+e^{-(\vec{w} \cdot \vec{x} + b)}}}) + \ln(1 - \frac{1}{1+e^{-(\vec{w} \cdot \vec{x} + b)}})] \\
&= -\frac{1}{n} \sum_{i=1}^{n} [y_i \ln(e^{\vec{w} \cdot \vec{x_i} + b}) + \ln(\frac{1}{1+e^{\vec{w} \cdot \vec{x_i} + b}})] \\ 
&= -\frac{1}{n} \sum_{i=1}^{n} [y_i (\vec{w} \cdot \vec{x_i} + b) - \ln(1+e^{\vec{w} \cdot \vec{x_i} + b})] \\ 
\end{align*}

    \subsection{Loss Function Gradient}\label{loss-function-gradient}

    Loss function with respect to \(W\)

\begin{align*}
\nabla_{W} [L(\vec{w}, b)] &= \nabla_{W} [-\frac{1}{n} \sum_{i=1}^{n} [y_i (\vec{w} \cdot \vec{x_i} + b) - \ln(1+e^{\vec{w} \cdot \vec{x_i} + b})]] \\
&= -\frac{1}{n} \sum_{i=1}^{n} [y_i \nabla_{W} [\vec{w} \cdot \vec{x_i} + b] - \nabla_{W}[\ln(1+e^{\vec{w} \cdot \vec{x_i} + b})]] \\
&= -\frac{1}{n} \sum_{i=1}^{n} [y_i x_i - (\frac{1}{1+e^{\vec{w} \cdot \vec{x_i} + b}}) \nabla_{W}[1+e^{\vec{w} \cdot \vec{x_i} + b}]] \\
&= -\frac{1}{n} \sum_{i=1}^{n} [y_i x_i - (\frac{1}{1+e^{\vec{w} \cdot \vec{x_i} + b}}) (e^{\vec{w} \cdot \vec{x_i} + b})\nabla_{W}[\vec{w} \cdot \vec{x_i} + b]] \\
&= -\frac{1}{n} \sum_{i=1}^{n} [y_i x_i - (\frac{1}{1+e^{\vec{w} \cdot \vec{x_i} + b}}) (e^{\vec{w} \cdot \vec{x_i} + b})(x_i)] \\
&= -\frac{1}{n} \sum_{i=1}^{n} [y_i x_i - x_i(\frac{e^{\vec{w} \cdot \vec{x_i} + b}}{1+e^{\vec{w} \cdot \vec{x_i} + b}})] \\
&= -\frac{1}{n} \sum_{i=1}^{n} [y_i x_i - x_i(\frac{1}{1+e^{-(\vec{w} \cdot \vec{x_i} + b)}})] \\ 
&= -\frac{1}{n} \sum_{i=1}^{n} [y_i x_i - x_i\hat y_i] \\ 
&= \frac{1}{n} \sum_{i=1}^{n} [\hat y_i x_i - y_i x_i  ] \\ 
&= \frac{1}{n} \sum_{i=1}^{n} [x_i (\hat y_i - y_i)] \\
\end{align*}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{bce\PYZus{}dw}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y\PYZus{}true}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{x} \PY{o}{*} \PY{p}{(}\PY{n}{y\PYZus{}pred} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}true}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Loss function with respect to \(b\)

\begin{align*}
\frac{\partial}{\partial b}  [L(\vec{w}, b)] &= \frac{\partial}{\partial b} [-\frac{1}{n} \sum_{i=1}^{n} [y_i (\vec{w} \cdot \vec{x_i} + b) - \ln(1+e^{\vec{w} \cdot \vec{x_i} + b})]] \\
&= -\frac{1}{n} \sum_{i=1}^{n} [y_i \frac{\partial}{\partial b} [\vec{w} \cdot \vec{x_i} + b] - \frac{\partial}{\partial b}[\ln(1+e^{\vec{w} \cdot \vec{x_i} + b})]] \\
&= -\frac{1}{n} \sum_{i=1}^{n} [y_i (1) - (\frac{1}{1+e^{\vec{w} \cdot \vec{x_i} + b}}) \frac{\partial}{\partial b}[1+e^{\vec{w} \cdot \vec{x_i} + b}] ] \\
&= -\frac{1}{n} \sum_{i=1}^{n} [y_i - (\frac{1}{1+e^{\vec{w} \cdot \vec{x_i} + b}}) (e^{\vec{w} \cdot \vec{x_i} + b}) \frac{\partial}{\partial b}[\vec{w} \cdot \vec{x_i} + b]] \\
&= -\frac{1}{n} \sum_{i=1}^{n} [y_i - (\frac{1}{1+e^{\vec{w} \cdot \vec{x_i} + b}}) (e^{\vec{w} \cdot \vec{x_i} + b}) (1)] \\
&= -\frac{1}{n} \sum_{i=1}^{n} [y_i - (\frac{e^{\vec{w} \cdot \vec{x_i} + b}}{1+e^{\vec{w} \cdot \vec{x_i} + b}})] \\
&= -\frac{1}{n} \sum_{i=1}^{n} [y_i - (\frac{1}{1+e^-({\vec{w} \cdot \vec{x_i} + b})})] \\
&= -\frac{1}{n} \sum_{i=1}^{n} [y_i - \hat y_i] \\
&= \frac{1}{n} \sum_{i=1}^{n} (\hat y_i - y_i) \\
\end{align*}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{bce\PYZus{}db}\PY{p}{(}\PY{n}{y\PYZus{}true}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{y\PYZus{}pred} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}true}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Gradient Descent}\label{gradient-descent}

With the binary cross entropy functions with respect to \(\vec{w}\) and
\(b\), the gradient descent equations are:

    Gradient Descent for weights \(\vec{w}\)

\begin{align*}
\vec{w} &= \vec{w} - \eta \nabla_{W} [L(\vec{w}, b)] \\
&= \vec{w} - \eta [\frac{1}{n} \sum_{i=1}^{n} [x_i (\hat y_i - y_i)]]
\end{align*}

    Gradient Descent for bias \(b\)

\begin{align*}
b &= b - \eta \frac{\partial}{\partial b} [L(\vec{w}, b)] \\
&= b - \eta [\frac{1}{n} \sum_{i=1}^{n} (\hat y_i - y_i)]
\end{align*}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{gradient\PYZus{}descent}\PY{p}{(}\PY{n}{weights}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{bias}\PY{p}{,} \PY{n}{y\PYZus{}true}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{p}{)}\PY{p}{:}
    \PY{n}{weights} \PY{o}{=} \PY{n}{weights} \PY{o}{\PYZhy{}} \PY{n}{learning\PYZus{}rate} \PY{o}{*} \PY{n}{bce\PYZus{}dw}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y\PYZus{}true}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}
    \PY{n}{bias} \PY{o}{=} \PY{n}{bias} \PY{o}{\PYZhy{}} \PY{n}{learning\PYZus{}rate} \PY{o}{*} \PY{n}{bce\PYZus{}db}\PY{p}{(}\PY{n}{y\PYZus{}true}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}

    \PY{k}{return} \PY{n}{weights}\PY{p}{,} \PY{n}{bias}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Graphing functions}\label{graphing-functions}

Utility functions to create the visualizations

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{create\PYZus{}plots}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{16} \PY{o}{/} \PY{l+m+mf}{9.0} \PY{o}{*} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{4} \PY{o}{*} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
    \PY{n}{fig}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Logistic Regression}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Epoch}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontweight}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{normal}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontweight}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{normal}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Binary Cross Entropy Error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{off}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{off}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{projection}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{3d}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{X}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Y}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}zlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Z}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Prediction Probabilities}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{view\PYZus{}init}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{35}\PY{p}{)}

    \PY{n}{camera} \PY{o}{=} \PY{n}{Camera}\PY{p}{(}\PY{n}{fig}\PY{p}{)}
    \PY{k}{return} \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{camera}


\PY{k}{def} \PY{n+nf}{plot\PYZus{}graphs}\PY{p}{(}
    \PY{n}{ax0}\PY{p}{,}
    \PY{n}{ax1}\PY{p}{,}
    \PY{n}{idx}\PY{p}{,}
    \PY{n}{visible\PYZus{}mse}\PY{p}{,}
    \PY{n}{mse\PYZus{}idx}\PY{p}{,}
    \PY{n}{errors}\PY{p}{,}
    \PY{n}{features}\PY{p}{,}
    \PY{n}{labels}\PY{p}{,}
    \PY{n}{predictions}\PY{p}{,}
    \PY{n}{points\PYZus{}x}\PY{p}{,}
    \PY{n}{points\PYZus{}y}\PY{p}{,}
    \PY{n}{surface\PYZus{}predictions}\PY{p}{,}
\PY{p}{)}\PY{p}{:}
    \PY{n}{ax0}\PY{o}{.}\PY{n}{plot}\PY{p}{(}
        \PY{n}{mse\PYZus{}idx}\PY{p}{[}\PY{n}{visible\PYZus{}mse}\PY{p}{]}\PY{p}{[}\PY{p}{:} \PY{n}{idx} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
        \PY{n}{errors}\PY{p}{[}\PY{n}{visible\PYZus{}mse}\PY{p}{]}\PY{p}{[}\PY{p}{:} \PY{n}{idx} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
        \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{red}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
        \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{,}
    \PY{p}{)}

    \PY{c+c1}{\PYZsh{} Plot Logistic Regression Predictions}
    \PY{c+c1}{\PYZsh{} Ground truth and training data}
    \PY{n}{ground\PYZus{}truth\PYZus{}legend} \PY{o}{=} \PY{n}{ax1}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}
        \PY{n}{features}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
        \PY{n}{features}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
        \PY{n}{labels}\PY{p}{,}
        \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{red}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
        \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{,}
        \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ground Truth}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{p}{)}
    \PY{c+c1}{\PYZsh{} Logistic Regression Predictions}
    \PY{n}{predictions\PYZus{}legend} \PY{o}{=} \PY{n}{ax1}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}
        \PY{n}{features}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
        \PY{n}{features}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
        \PY{n}{predictions}\PY{p}{,}
        \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{blue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
        \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,}
        \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Prediction}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{plot\PYZus{}surface}\PY{p}{(}
        \PY{n}{points\PYZus{}x}\PY{p}{,}
        \PY{n}{points\PYZus{}y}\PY{p}{,}
        \PY{n}{surface\PYZus{}predictions}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{dims}\PY{p}{,} \PY{n}{dims}\PY{p}{)}\PY{p}{,}
        \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{blue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
        \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,}
    \PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{legend}\PY{p}{(}
        \PY{p}{(}\PY{n}{ground\PYZus{}truth\PYZus{}legend}\PY{p}{,} \PY{n}{predictions\PYZus{}legend}\PY{p}{)}\PY{p}{,}
        \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ground Truth}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Predictions}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,}
        \PY{n}{loc}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{upper left}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Training the model}\label{training-the-model}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{fit}\PY{p}{(}
    \PY{n}{w0}\PY{p}{,} \PY{n}{b0}\PY{p}{,} \PY{n}{features}\PY{p}{,} \PY{n}{labels}\PY{p}{,} \PY{n}{dims}\PY{p}{,} \PY{n}{epochs}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{p}{,} \PY{n}{optimizer}\PY{p}{,} \PY{n}{output\PYZus{}filename}
\PY{p}{)}\PY{p}{:}
    \PY{n}{mse\PYZus{}idx} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{epochs} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{errors} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{full}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{ax0}\PY{p}{,} \PY{n}{ax1}\PY{p}{,} \PY{n}{camera} \PY{o}{=} \PY{n}{create\PYZus{}plots}\PY{p}{(}\PY{p}{)}

    \PY{n}{points} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{dims}\PY{p}{)}
    \PY{n}{points\PYZus{}x}\PY{p}{,} \PY{n}{points\PYZus{}y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{meshgrid}\PY{p}{(}\PY{n}{points}\PY{p}{,} \PY{n}{points}\PY{p}{)}
    \PY{n}{surface\PYZus{}points} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{column\PYZus{}stack}\PY{p}{(}\PY{p}{(}\PY{n}{points\PYZus{}x}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{points\PYZus{}y}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}

    \PY{n}{weights} \PY{o}{=} \PY{n}{w0}
    \PY{n}{bias} \PY{o}{=} \PY{n}{b0}

    \PY{k}{for} \PY{n}{idx} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{epochs}\PY{p}{)}\PY{p}{:}
        \PY{n}{error} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{n}{predictions} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{]}\PY{p}{)}
        \PY{n}{surface\PYZus{}predictions} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{]}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} fit the model on the training data}
        \PY{k}{for} \PY{n}{x}\PY{p}{,} \PY{n}{y} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{features}\PY{p}{,} \PY{n}{labels}\PY{p}{)}\PY{p}{:}
            \PY{n}{output} \PY{o}{=} \PY{n}{sigmoid}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{weights}\PY{p}{,} \PY{n}{x}\PY{p}{)} \PY{o}{+} \PY{n}{bias}\PY{p}{)}

            \PY{n}{predictions} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{predictions}\PY{p}{,} \PY{n}{output}\PY{p}{)}

            \PY{c+c1}{\PYZsh{} Store Error}
            \PY{n}{error} \PY{o}{+}\PY{o}{=} \PY{n}{bce}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{output}\PY{p}{)}

            \PY{c+c1}{\PYZsh{} Gradient Descent}
            \PY{n}{weights}\PY{p}{,} \PY{n}{bias} \PY{o}{=} \PY{n}{optimizer}\PY{p}{(}\PY{n}{weights}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{bias}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{output}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} error /= len(features)}

        \PY{c+c1}{\PYZsh{} Visualization}
        \PY{k}{if} \PY{p}{(}
            \PY{n}{idx} \PY{o}{\PYZlt{}} \PY{l+m+mi}{5}
            \PY{o+ow}{or} \PY{p}{(}\PY{n}{idx} \PY{o}{\PYZlt{}} \PY{l+m+mi}{15} \PY{o+ow}{and} \PY{n}{idx} \PY{o}{\PYZpc{}} \PY{l+m+mi}{5} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}
            \PY{o+ow}{or} \PY{p}{(}\PY{n}{idx} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{50} \PY{o+ow}{and} \PY{n}{idx} \PY{o}{\PYZpc{}} \PY{l+m+mi}{25} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}
            \PY{o+ow}{or} \PY{p}{(}\PY{n}{idx} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{1000} \PY{o+ow}{and} \PY{n}{idx} \PY{o}{\PYZpc{}} \PY{l+m+mi}{200} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}
            \PY{o+ow}{or} \PY{n}{idx} \PY{o}{\PYZpc{}} \PY{l+m+mi}{500} \PY{o}{==} \PY{l+m+mi}{0}
        \PY{p}{)}\PY{p}{:}
            \PY{k}{for} \PY{n}{surface\PYZus{}point} \PY{o+ow}{in} \PY{n}{surface\PYZus{}points}\PY{p}{:}
                \PY{n}{output} \PY{o}{=} \PY{n}{sigmoid}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{weights}\PY{p}{,} \PY{n}{surface\PYZus{}point}\PY{p}{)} \PY{o}{+} \PY{n}{bias}\PY{p}{)}
                \PY{n}{surface\PYZus{}predictions} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{surface\PYZus{}predictions}\PY{p}{,} \PY{n}{output}\PY{p}{)}

            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{epoch: }\PY{l+s+si}{\PYZob{}}\PY{n}{idx}\PY{l+s+si}{:}\PY{l+s+s2}{\PYZgt{}4}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, BCA: }\PY{l+s+si}{\PYZob{}}\PY{n+nb}{round}\PY{p}{(}\PY{n}{error}\PY{p}{,}\PY{+w}{ }\PY{l+m+mi}{2}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

            \PY{c+c1}{\PYZsh{} Plot BCE}
            \PY{n}{errors}\PY{p}{[}\PY{n}{idx}\PY{p}{]} \PY{o}{=} \PY{n}{error}
            \PY{n}{visible\PYZus{}mse} \PY{o}{=} \PY{n}{errors} \PY{o}{!=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}

            \PY{n}{plot\PYZus{}graphs}\PY{p}{(}
                \PY{n}{ax0}\PY{p}{,}
                \PY{n}{ax1}\PY{p}{,}
                \PY{n}{idx}\PY{p}{,}
                \PY{n}{visible\PYZus{}mse}\PY{p}{,}
                \PY{n}{mse\PYZus{}idx}\PY{p}{,}
                \PY{n}{errors}\PY{p}{,}
                \PY{n}{features}\PY{p}{,}
                \PY{n}{labels}\PY{p}{,}
                \PY{n}{predictions}\PY{p}{,}
                \PY{n}{points\PYZus{}x}\PY{p}{,}
                \PY{n}{points\PYZus{}y}\PY{p}{,}
                \PY{n}{surface\PYZus{}predictions}\PY{p}{,}
            \PY{p}{)}

            \PY{n}{camera}\PY{o}{.}\PY{n}{snap}\PY{p}{(}\PY{p}{)}

    \PY{n}{animation} \PY{o}{=} \PY{n}{camera}\PY{o}{.}\PY{n}{animate}\PY{p}{(}\PY{p}{)}
    \PY{n}{animation}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{n}{output\PYZus{}filename}\PY{p}{,} \PY{n}{writer}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pillow}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{5001}
\PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{l+m+mf}{0.0005}
\PY{n}{dims} \PY{o}{=} \PY{l+m+mi}{10}

\PY{n}{w0} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\PY{n}{b0} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{p}{)}

\PY{n}{output\PYZus{}filename} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{logistic\PYZus{}regression.gif}\PY{l+s+s2}{\PYZdq{}}
\PY{n}{fit}\PY{p}{(}\PY{n}{w0}\PY{p}{,} \PY{n}{b0}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{Y}\PY{p}{,} \PY{n}{dims}\PY{p}{,} \PY{n}{epochs}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{p}{,} \PY{n}{gradient\PYZus{}descent}\PY{p}{,} \PY{n}{output\PYZus{}filename}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
epoch:    0, BCA: 444.23
epoch:    1, BCA: 438.52
epoch:    2, BCA: 433.32
epoch:    3, BCA: 428.58
epoch:    4, BCA: 424.25
epoch:    5, BCA: 420.28
epoch:   10, BCA: 404.5
epoch:   25, BCA: 375.53
epoch:   50, BCA: 341.83
epoch:  200, BCA: 231.95
epoch:  400, BCA: 176.56
epoch:  500, BCA: 161.66
epoch:  600, BCA: 150.72
epoch:  800, BCA: 135.71
epoch: 1000, BCA: 125.86
epoch: 1500, BCA: 111.57
epoch: 2000, BCA: 103.85
epoch: 2500, BCA: 99.03
epoch: 3000, BCA: 95.75
epoch: 3500, BCA: 93.39
epoch: 4000, BCA: 91.61
epoch: 4500, BCA: 90.24
epoch: 5000, BCA: 89.16
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{combined_files/combined_95_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{output\PYZus{}filename}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<IPython.core.display.Image object>
\end{Verbatim}
\end{tcolorbox}
        
    \section{Perceptron}\label{perceptron}

The perceptron algorithm finds the optimal weights for a hyperplane to
separate two classes, which is also known as binary classification.

    Import the libraries

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{c+c1}{\PYZsh{} \PYZpc{}matplotlib ipympl}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{mpl\PYZus{}toolkits}\PY{n+nn}{.}\PY{n+nn}{mplot3d} \PY{k+kn}{import} \PY{n}{Axes3D}

\PY{k+kn}{from} \PY{n+nn}{celluloid} \PY{k+kn}{import} \PY{n}{Camera}
\PY{k+kn}{import} \PY{n+nn}{scienceplots}
\PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k+kn}{import} \PY{n}{Image}

\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{style}\PY{o}{.}\PY{n}{use}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{science}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{no\PYZhy{}latex}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Training Dataset}\label{training-dataset}

Let's generate a dataset where the label is determined by a linear
decision boundary. Our perceptron will layer find the weights of a
normal vector to separate the dataset into two classes.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{generate\PYZus{}dataset}\PY{p}{(}\PY{n}{dims}\PY{p}{,} \PY{n}{normal\PYZus{}vector}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} create 3D grid of points}
    \PY{n}{points} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{dims}\PY{p}{)}
    \PY{n}{X}\PY{p}{,} \PY{n}{Y}\PY{p}{,} \PY{n}{Z} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{meshgrid}\PY{p}{(}\PY{n}{points}\PY{p}{,} \PY{n}{points}\PY{p}{,} \PY{n}{points}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} features are the x, y, z coordinates}
    \PY{n}{features} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{column\PYZus{}stack}\PY{p}{(}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{Y}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{Z}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} labels are the side each point is on the hyperplane}
    \PY{n}{distances} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{features}\PY{p}{,} \PY{n}{normal\PYZus{}vector}\PY{p}{)}
    \PY{n}{labels} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{distances} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{k}{return} \PY{n}{X}\PY{p}{,} \PY{n}{Y}\PY{p}{,} \PY{n}{Z}\PY{p}{,} \PY{n}{features}\PY{p}{,} \PY{n}{labels}

\PY{c+c1}{\PYZsh{} normalized normal vector}
\PY{n}{target\PYZus{}normal\PYZus{}vector} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{]}\PY{p}{)}
\PY{n}{target\PYZus{}normal\PYZus{}vector} \PY{o}{=} \PY{n}{target\PYZus{}normal\PYZus{}vector} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{target\PYZus{}normal\PYZus{}vector}\PY{p}{)}

\PY{n}{scaling} \PY{o}{=} \PY{l+m+mi}{5}
\PY{n}{X}\PY{p}{,} \PY{n}{Y}\PY{p}{,} \PY{n}{Z}\PY{p}{,} \PY{n}{features}\PY{p}{,} \PY{n}{labels} \PY{o}{=} \PY{n}{generate\PYZus{}dataset}\PY{p}{(}\PY{n}{scaling}\PY{p}{,} \PY{n}{target\PYZus{}normal\PYZus{}vector}\PY{p}{)}

\PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
\PY{n}{ax} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{111}\PY{p}{,} \PY{n}{projection}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{3d}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} plot the points}
\PY{n}{ax}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{features}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{features}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{features}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.3}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{labels}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<mpl\_toolkits.mplot3d.art3d.Path3DCollection at 0xffff4f3cae90>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{combined_files/combined_101_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Hyperplane}\label{hyperplane}

A hyperplane is a flat subspace that is one less dimension than the
current space. It can be used to linearly separate a dataset. The
equation of a hyperplane is defined by the vector normal to the
hyperplane \(\vec{w}\)

\begin{align*}
\vec{w} \cdot \vec{x} = w_1 x_1 + ... + w_n x_n = 0
\end{align*}

In our case, the \(\vec{x}\) is the x, y, z coordinate.

\begin{align*}
\vec{w} \cdot \vec{x} &= 0 \\
&= w_1 x + w_2 y + w_3 z
\end{align*}

Since we want to perform binary classification using the side a point is
on relative from the hyperplane, the z value can be our predicted label

\begin{align*}
z = -(w_1 x + w_2 y) / w_3
\end{align*}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{generate\PYZus{}hyperplane}\PY{p}{(}\PY{n}{scaling}\PY{p}{,} \PY{n}{normal\PYZus{}vector}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} create 2D points}
    \PY{n}{points} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{scaling}\PY{p}{)}
    \PY{n}{xx}\PY{p}{,} \PY{n}{yy} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{meshgrid}\PY{p}{(}\PY{n}{points}\PY{p}{,} \PY{n}{points}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} the z value is the defined by the hyperplane}
    \PY{n}{zz} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{p}{(}\PY{n}{normal\PYZus{}vector}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{*} \PY{n}{xx} \PY{o}{+} \PY{n}{normal\PYZus{}vector}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{*} \PY{n}{yy}\PY{p}{)} \PY{o}{/} \PY{n}{normal\PYZus{}vector}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}
    \PY{k}{return} \PY{n}{xx}\PY{p}{,} \PY{n}{yy}\PY{p}{,} \PY{n}{zz}

\PY{n}{xx}\PY{p}{,} \PY{n}{yy}\PY{p}{,} \PY{n}{zz} \PY{o}{=} \PY{n}{generate\PYZus{}hyperplane}\PY{p}{(}\PY{n}{scaling}\PY{p}{,} \PY{n}{target\PYZus{}normal\PYZus{}vector}\PY{p}{)}

\PY{c+c1}{\PYZsh{} visualize the hyperplane}
\PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
\PY{n}{ax} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{111}\PY{p}{,} \PY{n}{projection}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{3d}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} plot the hyperplane defined by the normal vector}
\PY{n}{ax}\PY{o}{.}\PY{n}{plot\PYZus{}surface}\PY{p}{(}\PY{n}{xx}\PY{p}{,} \PY{n}{yy}\PY{p}{,} \PY{n}{zz}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{gray}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{quiver}\PY{p}{(}
    \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+m+mi}{0}\PY{p}{,}
    \PY{n}{target\PYZus{}normal\PYZus{}vector}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
    \PY{n}{target\PYZus{}normal\PYZus{}vector}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
    \PY{n}{target\PYZus{}normal\PYZus{}vector}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}
    \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{green}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{n}{length}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,}
    \PY{n}{arrow\PYZus{}length\PYZus{}ratio}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,}
\PY{p}{)}

\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{X}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Y}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}zlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Z}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Hyperplane}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
Text(0.5, 0.92, 'Hyperplane')
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{combined_files/combined_103_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Loss Function: Hinge Loss}\label{loss-function-hinge-loss}

Loss functions are used to quantify the error of a prediction.

The perceptron uses the hinge loss function, which returns 0 for correct
predictions and 1 for incorrect predictions.

\begin{align*}
L(\vec{w}, b) = max(0, -y(\vec{w} \cdot \vec{x} + b)
\end{align*}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{hinge\PYZus{}loss}\PY{p}{(}\PY{n}{w}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n+nb}{max}\PY{p}{(}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{n}{y} \PY{o}{*} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{w}\PY{p}{,} \PY{n}{x}\PY{p}{)} \PY{o}{+} \PY{n}{b}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Hinge Loss Gradient}\label{hinge-loss-gradient}

In order to run gradient descent to update our parameters, the gradients
with respect to W and b must be calculated

    \subsection{Hinge Loss Gradient in Terms of
B}\label{hinge-loss-gradient-in-terms-of-b}

Loss function with respect to \(b\)

\[
\frac{\partial L}{\partial b} =
\begin{cases}
0, & -y (\vec{w} \cdot \vec{x} + b) > 1 \\
-y, & \text{otherwise}
\end{cases}
\]

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{hinge\PYZus{}loss\PYZus{}db}\PY{p}{(}\PY{n}{w}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{:}
    \PY{k}{if} \PY{n}{y} \PY{o}{*} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{w}\PY{p}{,} \PY{n}{x}\PY{p}{)} \PY{o}{+} \PY{n}{b}\PY{p}{)} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mf}{0.0}\PY{p}{:}
        \PY{k}{return} \PY{o}{\PYZhy{}}\PY{n}{y}
    \PY{k}{return} \PY{l+m+mi}{0}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Hinge Loss Gradient in Terms of
W}\label{hinge-loss-gradient-in-terms-of-w}

Loss function with respect to \(\vec{w}\)

\[
\nabla\{W\} {[}L(\vec{w}, b){]} =
\begin{cases}
0, & -y (\vec{w} \cdot \vec{x} + b) > 1 \\
-y \vec{x}, & \text{otherwise}
\end{cases}
\]

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{hinge\PYZus{}loss\PYZus{}dw}\PY{p}{(}\PY{n}{w}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{:}
    \PY{k}{if} \PY{n}{y} \PY{o}{*} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{w}\PY{p}{,} \PY{n}{x}\PY{p}{)} \PY{o}{+} \PY{n}{b}\PY{p}{)} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mf}{0.0}\PY{p}{:}
        \PY{k}{return} \PY{o}{\PYZhy{}}\PY{n}{y} \PY{o}{*} \PY{n}{x}
    \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{zeros\PYZus{}like}\PY{p}{(}\PY{n}{x}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Graphing functions}\label{graphing-functions}

Utility functions to create the visualizations

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{create\PYZus{}plots}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{16} \PY{o}{/} \PY{l+m+mf}{9.0} \PY{o}{*} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{4} \PY{o}{*} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{layout}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{constrained}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{fig}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Perceptron}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Epoch}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontweight}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{normal}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontweight}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{normal}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Hinge Loss}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Z, Distance to Hyperplane}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontweight}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{normal}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontweight}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{normal}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Linear Transformation}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{off}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{off}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{off}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{off}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{projection}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{3d}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{X}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Y}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}zlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Z}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Hyperplane Decision Boundary}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{view\PYZus{}init}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{35}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}xlim}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylim}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}zlim}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}

    \PY{n}{camera} \PY{o}{=} \PY{n}{Camera}\PY{p}{(}\PY{n}{fig}\PY{p}{)}
    \PY{k}{return} \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{camera}


\PY{k}{def} \PY{n+nf}{plot\PYZus{}graphs}\PY{p}{(}
    \PY{n}{ax0}\PY{p}{,}
    \PY{n}{ax1}\PY{p}{,}
    \PY{n}{ax2}\PY{p}{,}
    \PY{n}{idx}\PY{p}{,}
    \PY{n}{visible\PYZus{}err}\PY{p}{,}
    \PY{n}{err\PYZus{}idx}\PY{p}{,}
    \PY{n}{errors}\PY{p}{,}
    \PY{n}{scaling}\PY{p}{,}
    \PY{n}{target\PYZus{}normal\PYZus{}vector}\PY{p}{,}
    \PY{n}{predictions}\PY{p}{,}
    \PY{n}{features}\PY{p}{,}
    \PY{n}{labels}\PY{p}{,}
    \PY{n}{weights}\PY{p}{,}
\PY{p}{)}\PY{p}{:}
    \PY{n}{ax0}\PY{o}{.}\PY{n}{plot}\PY{p}{(}
        \PY{n}{err\PYZus{}idx}\PY{p}{[}\PY{n}{visible\PYZus{}err}\PY{p}{]}\PY{p}{[}\PY{p}{:} \PY{n}{idx} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
        \PY{n}{errors}\PY{p}{[}\PY{n}{visible\PYZus{}err}\PY{p}{]}\PY{p}{[}\PY{p}{:} \PY{n}{idx} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
        \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{red}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{p}{)}

    \PY{c+c1}{\PYZsh{} Ground truth}
    \PY{n}{xx\PYZus{}target}\PY{p}{,} \PY{n}{yy\PYZus{}target}\PY{p}{,} \PY{n}{zz\PYZus{}target} \PY{o}{=} \PY{n}{generate\PYZus{}hyperplane}\PY{p}{(}\PY{n}{scaling}\PY{p}{,} \PY{n}{target\PYZus{}normal\PYZus{}vector}\PY{p}{)}
    \PY{n}{ground\PYZus{}truth\PYZus{}legend} \PY{o}{=} \PY{n}{ax2}\PY{o}{.}\PY{n}{plot\PYZus{}surface}\PY{p}{(}
        \PY{n}{xx\PYZus{}target}\PY{p}{,}
        \PY{n}{yy\PYZus{}target}\PY{p}{,}
        \PY{n}{zz\PYZus{}target}\PY{p}{,}
        \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{red}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
        \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,}
        \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ground Truth}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{p}{)}
    \PY{n}{ax2}\PY{o}{.}\PY{n}{quiver}\PY{p}{(}
        \PY{l+m+mi}{0}\PY{p}{,}
        \PY{l+m+mi}{0}\PY{p}{,}
        \PY{l+m+mi}{0}\PY{p}{,}
        \PY{n}{target\PYZus{}normal\PYZus{}vector}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
        \PY{n}{target\PYZus{}normal\PYZus{}vector}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
        \PY{n}{target\PYZus{}normal\PYZus{}vector}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}
        \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{red}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
        \PY{n}{length}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,}
        \PY{n}{arrow\PYZus{}length\PYZus{}ratio}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,}
    \PY{p}{)}

    \PY{c+c1}{\PYZsh{} Perceptron predictions using 2D graph to show linear transformation}
    \PY{k}{def} \PY{n+nf}{generate\PYZus{}colors}\PY{p}{(}\PY{n}{arr}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{green}\PY{l+s+s2}{\PYZdq{}} \PY{k}{if} \PY{n}{d} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{l+m+mi}{0} \PY{k}{else} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{orange}\PY{l+s+s2}{\PYZdq{}} \PY{k}{for} \PY{n}{d} \PY{o+ow}{in} \PY{n}{arr}\PY{p}{]}

    \PY{n}{ground\PYZus{}truth\PYZus{}colors} \PY{o}{=} \PY{n}{generate\PYZus{}colors}\PY{p}{(}\PY{n}{labels}\PY{p}{)}
    \PY{n}{ax1}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}
        \PY{n}{predictions}\PY{p}{,}
        \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{predictions}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{,}
        \PY{n}{c}\PY{o}{=}\PY{n}{ground\PYZus{}truth\PYZus{}colors}\PY{p}{,}
        \PY{n}{marker}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{o}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
        \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.3}\PY{p}{,}
    \PY{p}{)}

    \PY{c+c1}{\PYZsh{} Perceptron predictions using 3D graph to show hyperplane}
    \PY{n}{predictions\PYZus{}colors} \PY{o}{=} \PY{n}{generate\PYZus{}colors}\PY{p}{(}\PY{n}{predictions}\PY{p}{)}
    \PY{n}{predictions\PYZus{}norm} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{maximum}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{p}{(}\PY{n}{predictions}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{l+m+mf}{0.2}\PY{p}{)}

    \PY{n}{ax2}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}
        \PY{n}{features}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
        \PY{n}{features}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
        \PY{n}{features}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}
        \PY{n}{c}\PY{o}{=}\PY{n}{predictions\PYZus{}colors}\PY{p}{,}
        \PY{n}{marker}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{o}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
        \PY{n}{alpha}\PY{o}{=}\PY{n}{predictions\PYZus{}norm}\PY{p}{,}
    \PY{p}{)}

    \PY{n}{xx}\PY{p}{,} \PY{n}{yy}\PY{p}{,} \PY{n}{zz} \PY{o}{=} \PY{n}{generate\PYZus{}hyperplane}\PY{p}{(}\PY{n}{scaling}\PY{p}{,} \PY{n}{weights}\PY{p}{)}
    \PY{n}{predictions\PYZus{}legend} \PY{o}{=} \PY{n}{ax2}\PY{o}{.}\PY{n}{plot\PYZus{}surface}\PY{p}{(}
        \PY{n}{xx}\PY{p}{,}
        \PY{n}{yy}\PY{p}{,}
        \PY{n}{zz}\PY{p}{,}
        \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{blue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
        \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,}
        \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Prediction}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{p}{)}
    \PY{n}{ax2}\PY{o}{.}\PY{n}{quiver}\PY{p}{(}
        \PY{l+m+mi}{0}\PY{p}{,}
        \PY{l+m+mi}{0}\PY{p}{,}
        \PY{l+m+mi}{0}\PY{p}{,}
        \PY{n}{weights}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
        \PY{n}{weights}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
        \PY{n}{weights}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}
        \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{blue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
        \PY{n}{length}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,}
        \PY{n}{arrow\PYZus{}length\PYZus{}ratio}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,}
    \PY{p}{)}

    \PY{c+c1}{\PYZsh{} Legend}
    \PY{n}{ax2}\PY{o}{.}\PY{n}{legend}\PY{p}{(}
        \PY{p}{(}\PY{n}{ground\PYZus{}truth\PYZus{}legend}\PY{p}{,} \PY{n}{predictions\PYZus{}legend}\PY{p}{)}\PY{p}{,}
        \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ground Truth}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Predictions}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,}
        \PY{n}{loc}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{upper left}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Gradient Descent}\label{gradient-descent}

Gradient Descent will be used to update the weights and bias.

    Bias Gradient Descent:

\begin{align*}
b &= b - \eta \frac{\partial}{\partial b} [L(\vec{w}, b)] \\
&= b - \eta \begin{cases}
0 & -y(\vec{w} \cdot \vec{x} + b) > 1 \\
-y & \text{otherwise}
\end{cases}
\end{align*}

    Weights Gradient Descent:

\begin{align*}
\vec{w} &= \vec{w} - \eta \nabla_{W} [L(\vec{w}, b)] \\
&= \vec{w} - \eta \begin{cases}
0 & -y(\vec{w} \cdot \vec{x} + b) > 1 \\
-y x & \text{otherwise}
\end{cases}
\end{align*}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{gradient\PYZus{}descent}\PY{p}{(}\PY{n}{weights}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{bias}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{p}{)}\PY{p}{:}
    \PY{n}{weights} \PY{o}{=} \PY{n}{weights} \PY{o}{\PYZhy{}} \PY{n}{learning\PYZus{}rate} \PY{o}{*} \PY{n}{hinge\PYZus{}loss\PYZus{}dw}\PY{p}{(}\PY{n}{weights}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{bias}\PY{p}{,} \PY{n}{y}\PY{p}{)}
    \PY{n}{bias} \PY{o}{=} \PY{n}{bias} \PY{o}{\PYZhy{}} \PY{n}{learning\PYZus{}rate} \PY{o}{*} \PY{n}{hinge\PYZus{}loss\PYZus{}db}\PY{p}{(}\PY{n}{weights}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{bias}\PY{p}{,} \PY{n}{y}\PY{p}{)}

    \PY{k}{return} \PY{n}{weights}\PY{p}{,} \PY{n}{bias}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Training the Model}\label{training-the-model}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{fit}\PY{p}{(}
    \PY{n}{weights}\PY{p}{,}
    \PY{n}{bias}\PY{p}{,}
    \PY{n}{target\PYZus{}normal\PYZus{}vector}\PY{p}{,}
    \PY{n}{features}\PY{p}{,}
    \PY{n}{labels}\PY{p}{,}
    \PY{n}{X}\PY{p}{,}
    \PY{n}{Y}\PY{p}{,}
    \PY{n}{Z}\PY{p}{,}
    \PY{n}{scaling}\PY{p}{,}
    \PY{n}{epochs}\PY{p}{,}
    \PY{n}{learning\PYZus{}rate}\PY{p}{,}
    \PY{n}{optimizer}\PY{p}{,}
    \PY{n}{output\PYZus{}filename}\PY{p}{,}
\PY{p}{)}\PY{p}{:}
    \PY{n}{err\PYZus{}idx} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{epochs} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{errors} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{full}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{ax0}\PY{p}{,} \PY{n}{ax1}\PY{p}{,} \PY{n}{ax2}\PY{p}{,} \PY{n}{camera} \PY{o}{=} \PY{n}{create\PYZus{}plots}\PY{p}{(}\PY{p}{)}

    \PY{k}{for} \PY{n}{idx} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{epochs}\PY{p}{)}\PY{p}{:}
        \PY{n}{error} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{n}{predictions} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{]}\PY{p}{)}

        \PY{k}{for} \PY{n}{x}\PY{p}{,} \PY{n}{y} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{features}\PY{p}{,} \PY{n}{labels}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} Forward Propagation}
            \PY{n}{output} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{weights}\PY{p}{,} \PY{n}{x}\PY{p}{)} \PY{o}{+} \PY{n}{bias}

            \PY{n}{predictions} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{predictions}\PY{p}{,} \PY{n}{output}\PY{p}{)}

            \PY{c+c1}{\PYZsh{} Store Error}
            \PY{n}{error} \PY{o}{+}\PY{o}{=} \PY{n}{hinge\PYZus{}loss}\PY{p}{(}\PY{n}{weights}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{bias}\PY{p}{,} \PY{n}{y}\PY{p}{)}

            \PY{c+c1}{\PYZsh{} Gradient Descent}
            \PY{n}{weights}\PY{p}{,} \PY{n}{bias} \PY{o}{=} \PY{n}{optimizer}\PY{p}{(}\PY{n}{weights}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{bias}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{p}{)}

        \PY{n}{error} \PY{o}{/}\PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}
        \PY{n}{weights} \PY{o}{=} \PY{n}{weights} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{weights}\PY{p}{)}

        \PY{k}{if} \PY{p}{(}
            \PY{n}{idx} \PY{o}{\PYZlt{}} \PY{l+m+mi}{5}
            \PY{o+ow}{or} \PY{p}{(}\PY{n}{idx} \PY{o}{\PYZlt{}} \PY{l+m+mi}{15} \PY{o+ow}{and} \PY{n}{idx} \PY{o}{\PYZpc{}} \PY{l+m+mi}{2} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}
            \PY{o+ow}{or} \PY{p}{(}\PY{n}{idx} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{50} \PY{o+ow}{and} \PY{n}{idx} \PY{o}{\PYZpc{}} \PY{l+m+mi}{10} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}
            \PY{o+ow}{or} \PY{p}{(}\PY{n}{idx} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{1000} \PY{o+ow}{and} \PY{n}{idx} \PY{o}{\PYZpc{}} \PY{l+m+mi}{20} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}
            \PY{o+ow}{or} \PY{n}{idx} \PY{o}{\PYZpc{}} \PY{l+m+mi}{250} \PY{o}{==} \PY{l+m+mi}{0}
        \PY{p}{)}\PY{p}{:}

            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{epoch: }\PY{l+s+si}{\PYZob{}}\PY{n}{idx}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, MSE: }\PY{l+s+si}{\PYZob{}}\PY{n}{error}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

            \PY{c+c1}{\PYZsh{} Plot MSE}
            \PY{n}{errors}\PY{p}{[}\PY{n}{idx}\PY{p}{]} \PY{o}{=} \PY{n}{error}
            \PY{n}{visible\PYZus{}err} \PY{o}{=} \PY{n}{errors} \PY{o}{!=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}

            \PY{n}{plot\PYZus{}graphs}\PY{p}{(}
                \PY{n}{ax0}\PY{p}{,}
                \PY{n}{ax1}\PY{p}{,}
                \PY{n}{ax2}\PY{p}{,}
                \PY{n}{idx}\PY{p}{,}
                \PY{n}{visible\PYZus{}err}\PY{p}{,}
                \PY{n}{err\PYZus{}idx}\PY{p}{,}
                \PY{n}{errors}\PY{p}{,}
                \PY{n}{scaling}\PY{p}{,}
                \PY{n}{target\PYZus{}normal\PYZus{}vector}\PY{p}{,}
                \PY{n}{predictions}\PY{p}{,}
                \PY{n}{features}\PY{p}{,}
                \PY{n}{labels}\PY{p}{,}
                \PY{n}{weights}\PY{p}{,}
            \PY{p}{)}

            \PY{n}{camera}\PY{o}{.}\PY{n}{snap}\PY{p}{(}\PY{p}{)}

    \PY{n}{animation} \PY{o}{=} \PY{n}{camera}\PY{o}{.}\PY{n}{animate}\PY{p}{(}\PY{p}{)}
    \PY{n}{animation}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{n}{output\PYZus{}filename}\PY{p}{,} \PY{n}{writer}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pillow}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Let's put everything together and train our Perceptron

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{weights} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{1.0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{1.0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{1.0}\PY{p}{]}\PY{p}{)}
\PY{n}{weights} \PY{o}{=} \PY{n}{weights} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{weights}\PY{p}{)}

\PY{n}{bias} \PY{o}{=} \PY{l+m+mi}{0}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{301}
\PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{l+m+mf}{0.0005}

\PY{n}{output\PYZus{}filename} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{perceptron.gif}\PY{l+s+s2}{\PYZdq{}}
\PY{n}{fit}\PY{p}{(}
    \PY{n}{weights}\PY{p}{,}
    \PY{n}{bias}\PY{p}{,}
    \PY{n}{target\PYZus{}normal\PYZus{}vector}\PY{p}{,}
    \PY{n}{features}\PY{p}{,}
    \PY{n}{labels}\PY{p}{,}
    \PY{n}{X}\PY{p}{,}
    \PY{n}{Y}\PY{p}{,}
    \PY{n}{Z}\PY{p}{,}
    \PY{n}{scaling}\PY{p}{,}
    \PY{n}{epochs}\PY{p}{,}
    \PY{n}{learning\PYZus{}rate}\PY{p}{,}
    \PY{n}{gradient\PYZus{}descent}\PY{p}{,}
    \PY{n}{output\PYZus{}filename}\PY{p}{,}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
epoch: 0, MSE: 9.314269414709884
epoch: 1, MSE: 9.227024830601328
epoch: 2, MSE: 9.145087443313994
epoch: 3, MSE: 9.054781136556876
epoch: 4, MSE: 8.960407022428585
epoch: 6, MSE: 8.758853058777225
epoch: 8, MSE: 8.53921460232333
epoch: 10, MSE: 8.30137439690734
epoch: 12, MSE: 8.08268967439665
epoch: 14, MSE: 7.848010649913293
epoch: 20, MSE: 7.2399656649458395
epoch: 30, MSE: 6.264254344109915
epoch: 40, MSE: 5.307156953825979
epoch: 50, MSE: 4.404348110031334
epoch: 60, MSE: 3.5837443020613877
epoch: 80, MSE: 2.0340687540122775
epoch: 100, MSE: 0.9393759997635168
epoch: 120, MSE: 0.35445883826326263
epoch: 140, MSE: 0.12744345824321743
epoch: 160, MSE: 0.018715050957433404
epoch: 180, MSE: 0.0
epoch: 200, MSE: 0.0
epoch: 220, MSE: 0.0
epoch: 240, MSE: 0.0
epoch: 250, MSE: 0.0
epoch: 260, MSE: 0.0
epoch: 280, MSE: 0.0
epoch: 300, MSE: 0.0
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{combined_files/combined_121_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Output GIF}\label{output-gif}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{output\PYZus{}filename}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<IPython.core.display.Image object>
\end{Verbatim}
\end{tcolorbox}
        
    \section{Neural Network Weights
Visualization}\label{neural-network-weights-visualization}

Neural Networks at a high-level just consist of matrix multiplications
at each layer. Matrix multiplications are linear transformations. This
visualization shows the linear transformations at each layer and the
loss landscape of each layer. This Notebook builds on top of the
\texttt{Neural\ Network} Notebook. Look at the previous Notebook for the
derivation of Backpropagation and the math behind neural networks.

    Gavin's Note: The goal of this visualization is show that
Backpropagation updates the weights and biases in the most optimal way.
In order to visualize this, this program changes the weights to make
them non optimal to show that the loss increases. As a result, this
program will take a very long time.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{torch}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn} \PY{k}{as} \PY{n+nn}{nn}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn}\PY{n+nn}{.}\PY{n+nn}{functional} \PY{k}{as} \PY{n+nn}{F}

\PY{c+c1}{\PYZsh{} \PYZpc{}matplotlib ipympl}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{mpl\PYZus{}toolkits}\PY{n+nn}{.}\PY{n+nn}{mplot3d} \PY{k+kn}{import} \PY{n}{Axes3D}

\PY{k+kn}{from} \PY{n+nn}{celluloid} \PY{k+kn}{import} \PY{n}{Camera}
\PY{k+kn}{import} \PY{n+nn}{scienceplots}
\PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k+kn}{import} \PY{n}{Image}

\PY{n}{torch}\PY{o}{.}\PY{n}{manual\PYZus{}seed}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{style}\PY{o}{.}\PY{n}{use}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{science}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{no\PYZhy{}latex}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Training Dataset}\label{training-dataset}

Let's generate a non-linear dataset, since neural networks can fit this
function while linear models, such as a perceptron, can't converge on
this dataset

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} generate the non\PYZhy{}linear dataset, meaning that a hyperplane can\PYZsq{}t separate the data}
\PY{k}{def} \PY{n+nf}{generate\PYZus{}XOR}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{N} \PY{o}{=} \PY{l+m+mi}{500}
    \PY{n}{X} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{n}{N}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
    \PY{n}{y} \PY{o}{=} \PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mf}{0.5}\PY{p}{)} \PY{o}{!=} \PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mf}{0.5}\PY{p}{)}

    \PY{k}{return} \PY{n}{X}\PY{p}{,} \PY{n}{y}


\PY{n}{X}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{generate\PYZus{}XOR}\PY{p}{(}\PY{p}{)}

\PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
\PY{n}{ax} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{y}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<matplotlib.collections.PathCollection at 0xffff2f2330e0>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{combined_files/combined_128_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Graph Functions}\label{graph-functions}

In our training function, we use the gradient descent optimizer to
update the weights and move on. What if we didn't use the weights from
the optimizer? These graphing functions manually change the values in
the weight matrix of our neural network's layers and run the neural
network to see how the loss changes.

There are also graphing functions that show the linear transformation of
each layer.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{create\PYZus{}scatterplots}\PY{p}{(}\PY{n}{rows}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{cols}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{width\PYZus{}scale}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{height\PYZus{}scale}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
    \PY{n}{fig}\PY{p}{,} \PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}
        \PY{n}{rows}\PY{p}{,}
        \PY{n}{cols}\PY{p}{,}
        \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{16} \PY{o}{/} \PY{l+m+mf}{9.0} \PY{o}{*} \PY{l+m+mi}{4} \PY{o}{*} \PY{n}{width\PYZus{}scale}\PY{p}{,} \PY{l+m+mi}{4} \PY{o}{*} \PY{n}{height\PYZus{}scale}\PY{p}{)}\PY{p}{,}
        \PY{n}{layout}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{constrained}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{p}{)}
    \PY{n}{axes} \PY{o}{=} \PY{n}{axes}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}

    \PY{n}{layer\PYZus{}idx} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{axis} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{axes}\PY{p}{)}\PY{p}{:}
        \PY{k}{if} \PY{o+ow}{not} \PY{p}{(}\PY{p}{(}\PY{n}{i} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{\PYZpc{}} \PY{n}{cols} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
            \PY{n}{axis}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Layer }\PY{l+s+si}{\PYZob{}}\PY{n}{layer\PYZus{}idx}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{n}{layer\PYZus{}idx} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}

    \PY{n}{axes}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Predictions}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{axes}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{cols}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Mean Squared Error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{camera} \PY{o}{=} \PY{n}{Camera}\PY{p}{(}\PY{n}{fig}\PY{p}{)}
    \PY{k}{return} \PY{n}{axes}\PY{p}{,} \PY{n}{camera}


\PY{k}{def} \PY{n+nf}{create\PYZus{}3d\PYZus{}plots}\PY{p}{(}\PY{n}{rows}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{cols}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{width\PYZus{}scale}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{height\PYZus{}scale}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
    \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}
        \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{16} \PY{o}{/} \PY{l+m+mf}{9.0} \PY{o}{*} \PY{l+m+mi}{4} \PY{o}{*} \PY{n}{width\PYZus{}scale}\PY{p}{,} \PY{l+m+mi}{4} \PY{o}{*} \PY{n}{height\PYZus{}scale}\PY{p}{)}\PY{p}{,} \PY{n}{layout}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{constrained}\PY{l+s+s2}{\PYZdq{}}
    \PY{p}{)}
    \PY{n}{axes} \PY{o}{=} \PY{p}{[}\PY{p}{]}

    \PY{n}{layer\PYZus{}idx} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{rows} \PY{o}{*} \PY{n}{cols}\PY{p}{)}\PY{p}{:}
        \PY{k}{if} \PY{o+ow}{not} \PY{p}{(}\PY{p}{(}\PY{n}{i} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{\PYZpc{}} \PY{n}{cols} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
            \PY{n}{axis} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{n}{rows}\PY{p}{,} \PY{n}{cols}\PY{p}{,} \PY{n}{i} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{projection}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{3d}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{n}{axis}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Layer }\PY{l+s+si}{\PYZob{}}\PY{n}{layer\PYZus{}idx}\PY{+w}{ }\PY{o}{+}\PY{+w}{ }\PY{l+m+mi}{1}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{n}{axes}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{axis}\PY{p}{)}
            \PY{n}{layer\PYZus{}idx} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
        \PY{k}{else}\PY{p}{:}
            \PY{n}{axes}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{n}{rows}\PY{p}{,} \PY{n}{cols}\PY{p}{,} \PY{n}{i} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}

    \PY{n}{axes}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Predictions}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{axes}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{cols}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Mean Squared Error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{camera} \PY{o}{=} \PY{n}{Camera}\PY{p}{(}\PY{n}{fig}\PY{p}{)}
    \PY{k}{return} \PY{n}{axes}\PY{p}{,} \PY{n}{camera}


\PY{k}{def} \PY{n+nf}{plot\PYZus{}layer\PYZus{}loss\PYZus{}landscape}\PY{p}{(}
    \PY{n}{axis}\PY{p}{,}
    \PY{n}{model}\PY{p}{,}
    \PY{n}{target\PYZus{}layer\PYZus{}idx}\PY{p}{,}
    \PY{n}{neuron\PYZus{}idx}\PY{p}{,}
    \PY{n}{features}\PY{p}{,}
    \PY{n}{labels}\PY{p}{,}
    \PY{n}{w1\PYZus{}min}\PY{p}{,}
    \PY{n}{w1\PYZus{}max}\PY{p}{,}
    \PY{n}{w2\PYZus{}min}\PY{p}{,}
    \PY{n}{w2\PYZus{}max}\PY{p}{,}
    \PY{n}{loss\PYZus{}dims}\PY{p}{,}
    \PY{n}{device}\PY{p}{,}
    \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{blue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Plot how the loss changes when the first two weights in the first neuron change\PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{loss\PYZus{}fn} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{MSELoss}\PY{p}{(}\PY{p}{)}

    \PY{n}{init} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{get\PYZus{}values}\PY{p}{(}\PY{n}{target\PYZus{}layer\PYZus{}idx}\PY{p}{,} \PY{n}{neuron\PYZus{}idx}\PY{p}{)}
    \PY{n}{w1} \PY{o}{=} \PY{n}{init}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}
    \PY{n}{w2} \PY{o}{=} \PY{n}{init}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}

    \PY{n}{target\PYZus{}layer\PYZus{}idx} \PY{o}{=} \PY{n}{target\PYZus{}layer\PYZus{}idx} \PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{layers}\PY{p}{)}

    \PY{n}{w1\PYZus{}range} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{w1\PYZus{}min} \PY{o}{+} \PY{n}{w1}\PY{p}{,} \PY{n}{w1\PYZus{}max} \PY{o}{+} \PY{n}{w1}\PY{p}{,} \PY{n}{loss\PYZus{}dims}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
    \PY{n}{w2\PYZus{}range} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{w2\PYZus{}min} \PY{o}{+} \PY{n}{w2}\PY{p}{,} \PY{n}{w2\PYZus{}max} \PY{o}{+} \PY{n}{w2}\PY{p}{,} \PY{n}{loss\PYZus{}dims}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
    \PY{n}{w1\PYZus{}range}\PY{p}{,} \PY{n}{w2\PYZus{}range} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{meshgrid}\PY{p}{(}\PY{n}{w1\PYZus{}range}\PY{p}{,} \PY{n}{w2\PYZus{}range}\PY{p}{,} \PY{n}{indexing}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ij}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{w\PYZus{}range} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{stack}\PY{p}{(}\PY{p}{(}\PY{n}{w1\PYZus{}range}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{w2\PYZus{}range}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}

    \PY{n}{error\PYZus{}range} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{]}\PY{p}{)}

    \PY{k}{for} \PY{n}{target\PYZus{}layer\PYZus{}weight} \PY{o+ow}{in} \PY{n}{w\PYZus{}range}\PY{p}{:}
        \PY{n}{model}\PY{o}{.}\PY{n}{override\PYZus{}layer\PYZus{}weight}\PY{p}{(}
            \PY{n}{target\PYZus{}layer\PYZus{}idx}\PY{p}{,} \PY{n}{neuron\PYZus{}idx}\PY{p}{,} \PY{n}{init} \PY{o}{+} \PY{n}{target\PYZus{}layer\PYZus{}weight}
        \PY{p}{)}
        \PY{n}{error} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{k}{for} \PY{n}{x}\PY{p}{,} \PY{n}{y} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{features}\PY{p}{,} \PY{n}{labels}\PY{p}{)}\PY{p}{:}
            \PY{n}{output} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{n}{x}\PY{p}{)}
            \PY{n}{y} \PY{o}{=} \PY{n}{y}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
            \PY{n}{loss} \PY{o}{=} \PY{n}{loss\PYZus{}fn}\PY{p}{(}\PY{n}{output}\PY{p}{,} \PY{n}{y}\PY{p}{)}
            \PY{n}{error} \PY{o}{+}\PY{o}{=} \PY{n}{loss}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}
        \PY{n}{error} \PY{o}{/}\PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{labels}\PY{p}{)}
        \PY{n}{error\PYZus{}range} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{error\PYZus{}range}\PY{p}{,} \PY{n}{error}\PY{p}{)}

        \PY{k}{if} \PY{n}{np}\PY{o}{.}\PY{n}{isclose}\PY{p}{(}\PY{n}{target\PYZus{}layer\PYZus{}weight}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{w1}\PY{p}{,} \PY{n}{atol}\PY{o}{=}\PY{l+m+mf}{0.25}\PY{p}{)} \PY{o+ow}{and} \PY{n}{np}\PY{o}{.}\PY{n}{isclose}\PY{p}{(}
            \PY{n}{target\PYZus{}layer\PYZus{}weight}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{w2}\PY{p}{,} \PY{n}{atol}\PY{o}{=}\PY{l+m+mf}{0.25}
        \PY{p}{)}\PY{p}{:}
            \PY{n}{axis}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{p}{[}\PY{n}{w1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{w2}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{error}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{n}{color}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.4}\PY{p}{)}

    \PY{n}{axis}\PY{o}{.}\PY{n}{plot\PYZus{}surface}\PY{p}{(}
        \PY{n}{w1\PYZus{}range}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{,}
        \PY{n}{w2\PYZus{}range}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{,}
        \PY{n}{error\PYZus{}range}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{loss\PYZus{}dims}\PY{p}{,} \PY{n}{loss\PYZus{}dims}\PY{p}{)}\PY{p}{,}
        \PY{n}{color}\PY{o}{=}\PY{n}{color}\PY{p}{,}
        \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,}
    \PY{p}{)}
    \PY{n}{model}\PY{o}{.}\PY{n}{override\PYZus{}layer\PYZus{}weight}\PY{p}{(}\PY{n}{target\PYZus{}layer\PYZus{}idx}\PY{p}{,} \PY{n}{neuron\PYZus{}idx}\PY{p}{,} \PY{n}{init}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{plot\PYZus{}mse\PYZus{}and\PYZus{}predictions}\PY{p}{(}
    \PY{n}{axes}\PY{p}{,} \PY{n}{features}\PY{p}{,} \PY{n}{idx}\PY{p}{,} \PY{n}{visible\PYZus{}mse}\PY{p}{,} \PY{n}{mse\PYZus{}idx}\PY{p}{,} \PY{n}{errors}\PY{p}{,} \PY{n}{predictions}\PY{p}{,} \PY{n}{cmap}\PY{p}{,} \PY{n}{cols}\PY{p}{,} \PY{n}{device}
\PY{p}{)}\PY{p}{:}
    \PY{n}{features\PYZus{}cpu} \PY{o}{=} \PY{n}{features}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Plot MSE}
    \PY{n}{mse\PYZus{}ax} \PY{o}{=} \PY{n}{axes}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{cols}\PY{p}{]}
    \PY{n}{mse\PYZus{}ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}
        \PY{n}{mse\PYZus{}idx}\PY{p}{[}\PY{n}{visible\PYZus{}mse}\PY{p}{]}\PY{p}{[}\PY{p}{:} \PY{n}{idx} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
        \PY{n}{errors}\PY{p}{[}\PY{n}{visible\PYZus{}mse}\PY{p}{]}\PY{p}{[}\PY{p}{:} \PY{n}{idx} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
        \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{red}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
        \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{,}
    \PY{p}{)}
    \PY{n}{mse\PYZus{}ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}
        \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
        \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
        \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{white}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
        \PY{n}{alpha}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,}
    \PY{p}{)}

    \PY{c+c1}{\PYZsh{} Plot Predictions}
    \PY{n}{predictions\PYZus{}classes} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{predictions} \PY{o}{\PYZgt{}} \PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}

    \PY{n}{predictions\PYZus{}ax} \PY{o}{=} \PY{n}{axes}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
    \PY{n}{predictions\PYZus{}ax}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}
        \PY{n}{features\PYZus{}cpu}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
        \PY{n}{features\PYZus{}cpu}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
        \PY{n}{c}\PY{o}{=}\PY{n}{predictions\PYZus{}classes}\PY{p}{,}
        \PY{n}{cmap}\PY{o}{=}\PY{n}{cmap}\PY{p}{,}
        \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{,}
    \PY{p}{)}


\PY{k}{def} \PY{n+nf}{plot\PYZus{}transformations\PYZus{}and\PYZus{}predictions}\PY{p}{(}
    \PY{n}{axes}\PY{p}{,}
    \PY{n}{model}\PY{p}{,}
    \PY{n}{idx}\PY{p}{,}
    \PY{n}{visible\PYZus{}mse}\PY{p}{,}
    \PY{n}{mse\PYZus{}idx}\PY{p}{,}
    \PY{n}{errors}\PY{p}{,}
    \PY{n}{predictions}\PY{p}{,}
    \PY{n}{features}\PY{p}{,}
    \PY{n}{labels}\PY{p}{,}
    \PY{n}{cmap}\PY{p}{,}
    \PY{n}{rows}\PY{p}{,}
    \PY{n}{cols}\PY{p}{,}
    \PY{n}{device}\PY{p}{,}
\PY{p}{)}\PY{p}{:}
    \PY{n}{plot\PYZus{}mse\PYZus{}and\PYZus{}predictions}\PY{p}{(}
        \PY{n}{axes}\PY{p}{,}
        \PY{n}{features}\PY{p}{,}
        \PY{n}{idx}\PY{p}{,}
        \PY{n}{visible\PYZus{}mse}\PY{p}{,}
        \PY{n}{mse\PYZus{}idx}\PY{p}{,}
        \PY{n}{errors}\PY{p}{,}
        \PY{n}{predictions}\PY{p}{,}
        \PY{n}{cmap}\PY{p}{,}
        \PY{n}{cols}\PY{p}{,}
        \PY{n}{device}\PY{p}{,}
    \PY{p}{)}
    \PY{n}{model}\PY{o}{.}\PY{n}{visualize}\PY{p}{(}\PY{n}{features}\PY{p}{,} \PY{n}{labels}\PY{p}{,} \PY{n}{axes}\PY{p}{,} \PY{n}{cmap}\PY{p}{,} \PY{n}{rows}\PY{p}{,} \PY{n}{cols}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{plot\PYZus{}loss\PYZus{}landscape\PYZus{}and\PYZus{}predictions}\PY{p}{(}
    \PY{n}{axes}\PY{p}{,}
    \PY{n}{model}\PY{p}{,}
    \PY{n}{idx}\PY{p}{,}
    \PY{n}{visible\PYZus{}mse}\PY{p}{,}
    \PY{n}{mse\PYZus{}idx}\PY{p}{,}
    \PY{n}{errors}\PY{p}{,}
    \PY{n}{predictions}\PY{p}{,}
    \PY{n}{features}\PY{p}{,}
    \PY{n}{labels}\PY{p}{,}
    \PY{n}{cmap}\PY{p}{,}
    \PY{n}{cols}\PY{p}{,}
    \PY{n}{device}\PY{p}{,}
    \PY{n}{w1\PYZus{}min}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{,}
    \PY{n}{w1\PYZus{}max}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,}
    \PY{n}{w2\PYZus{}min}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{,}
    \PY{n}{w2\PYZus{}max}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,}
    \PY{n}{loss\PYZus{}dims}\PY{o}{=}\PY{l+m+mi}{7}\PY{p}{,}
\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} this uses axes with index \PYZhy{}1 and \PYZhy{}1\PYZhy{}cols}
    \PY{n}{plot\PYZus{}mse\PYZus{}and\PYZus{}predictions}\PY{p}{(}
        \PY{n}{axes}\PY{p}{,}
        \PY{n}{features}\PY{p}{,}
        \PY{n}{idx}\PY{p}{,}
        \PY{n}{visible\PYZus{}mse}\PY{p}{,}
        \PY{n}{mse\PYZus{}idx}\PY{p}{,}
        \PY{n}{errors}\PY{p}{,}
        \PY{n}{predictions}\PY{p}{,}
        \PY{n}{cmap}\PY{p}{,}
        \PY{n}{cols}\PY{p}{,}
        \PY{n}{device}\PY{p}{,}
    \PY{p}{)}

    \PY{n}{num\PYZus{}layers} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{layers}\PY{p}{)}

    \PY{n}{target\PYZus{}layer\PYZus{}idx} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}

    \PY{k}{for} \PY{n}{index}\PY{p}{,} \PY{n}{axis} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n+nb}{reversed}\PY{p}{(}\PY{n}{axes}\PY{p}{)}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} in reverse order, predictions plot is index 0 and mse plot is index cols}
        \PY{k}{if} \PY{n}{index} \PY{o}{==} \PY{l+m+mi}{0} \PY{o+ow}{or} \PY{n}{index} \PY{o}{==} \PY{n}{cols} \PY{o+ow}{or} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{target\PYZus{}layer\PYZus{}idx}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{n}{num\PYZus{}layers}\PY{p}{:}
            \PY{k}{continue}
        \PY{n}{plot\PYZus{}layer\PYZus{}loss\PYZus{}landscape}\PY{p}{(}
            \PY{n}{axis}\PY{p}{,}
            \PY{n}{model}\PY{p}{,}
            \PY{n}{target\PYZus{}layer\PYZus{}idx}\PY{p}{,}
            \PY{l+m+mi}{0}\PY{p}{,}
            \PY{n}{features}\PY{p}{,}
            \PY{n}{labels}\PY{p}{,}
            \PY{n}{w1\PYZus{}min}\PY{p}{,}
            \PY{n}{w1\PYZus{}max}\PY{p}{,}
            \PY{n}{w2\PYZus{}min}\PY{p}{,}
            \PY{n}{w2\PYZus{}max}\PY{p}{,}
            \PY{n}{loss\PYZus{}dims}\PY{p}{,}
            \PY{n}{device}\PY{p}{,}
            \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{blue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
        \PY{p}{)}
        \PY{k}{if} \PY{n}{target\PYZus{}layer\PYZus{}idx} \PY{o}{!=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}
            \PY{n}{plot\PYZus{}layer\PYZus{}loss\PYZus{}landscape}\PY{p}{(}
                \PY{n}{axis}\PY{p}{,}
                \PY{n}{model}\PY{p}{,}
                \PY{n}{target\PYZus{}layer\PYZus{}idx}\PY{p}{,}
                \PY{l+m+mi}{1}\PY{p}{,}
                \PY{n}{features}\PY{p}{,}
                \PY{n}{labels}\PY{p}{,}
                \PY{n}{w1\PYZus{}min}\PY{p}{,}
                \PY{n}{w1\PYZus{}max}\PY{p}{,}
                \PY{n}{w2\PYZus{}min}\PY{p}{,}
                \PY{n}{w2\PYZus{}max}\PY{p}{,}
                \PY{n}{loss\PYZus{}dims}\PY{p}{,}
                \PY{n}{device}\PY{p}{,}
                \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{red}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
            \PY{p}{)}
        \PY{n}{target\PYZus{}layer\PYZus{}idx} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{l+m+mi}{1}
\end{Verbatim}
\end{tcolorbox}

    \subsection{PyTorch Implementation}\label{pytorch-implementation}

Let's define a feedforward neural network in PyTorch, but add custom
functions to each layer that manually change the weight values. We want
to use this function to see how the loss changes when the weights aren't
at the optimal value. This is used to show that Backpropagation tells us
how to update the weights optimally.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{VisualNet}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{VisualNet}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{ModuleList}\PY{p}{(}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{visualize}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{axes}\PY{p}{,} \PY{n}{cmap}\PY{p}{,} \PY{n}{rows}\PY{p}{,} \PY{n}{cols}\PY{p}{)}\PY{p}{:}
        \PY{n}{y\PYZus{}cpu} \PY{o}{=} \PY{n}{y}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}

        \PY{n}{layer\PYZus{}idx} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{axis} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{axes}\PY{p}{)}\PY{p}{:}
            \PY{k}{if} \PY{o+ow}{not} \PY{p}{(}\PY{p}{(}\PY{n}{i} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{\PYZpc{}} \PY{n}{cols} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
                \PY{n}{X\PYZus{}cpu} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}

                \PY{c+c1}{\PYZsh{} input and hidden layer outputs}
                \PY{k}{if} \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{!=} \PY{l+m+mi}{1}\PY{p}{:}
                    \PY{n}{axis}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}
                        \PY{n}{X\PYZus{}cpu}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X\PYZus{}cpu}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{y\PYZus{}cpu}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{n}{cmap}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}
                    \PY{p}{)}
                \PY{c+c1}{\PYZsh{} output layer is 1D, so set second dimenstional to zeros}
                \PY{k}{else}\PY{p}{:}
                    \PY{n}{axis}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}
                        \PY{n}{X\PYZus{}cpu}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
                        \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{X\PYZus{}cpu}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{,}
                        \PY{n}{c}\PY{o}{=}\PY{n}{y\PYZus{}cpu}\PY{p}{,}
                        \PY{n}{cmap}\PY{o}{=}\PY{n}{cmap}\PY{p}{,}
                        \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{,}
                    \PY{p}{)}

                \PY{k}{if} \PY{n}{layer\PYZus{}idx} \PY{o}{\PYZlt{}} \PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers}\PY{p}{)}\PY{p}{:}
                    \PY{n}{X} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{tanh}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers}\PY{p}{[}\PY{n}{layer\PYZus{}idx}\PY{p}{]}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{)}
                    \PY{n}{layer\PYZus{}idx} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}

    \PY{k}{def} \PY{n+nf}{override\PYZus{}layer\PYZus{}weight}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{layer\PYZus{}idx}\PY{p}{,} \PY{n}{neuron\PYZus{}idx}\PY{p}{,} \PY{n}{new\PYZus{}weights}\PY{p}{)}\PY{p}{:}
        \PY{k}{if} \PY{p}{(}\PY{n+nb}{abs}\PY{p}{(}\PY{n}{layer\PYZus{}idx}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers}\PY{p}{)}\PY{p}{)} \PY{o+ow}{or} \PY{p}{(}
            \PY{n+nb}{abs}\PY{p}{(}\PY{n}{neuron\PYZus{}idx}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers}\PY{p}{[}\PY{n}{layer\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{weight}\PY{p}{)}
        \PY{p}{)}\PY{p}{:}
            \PY{k}{return}

        \PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers}\PY{p}{[}\PY{n}{layer\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{weight}\PY{p}{[}\PY{n}{neuron\PYZus{}idx}\PY{p}{,} \PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{=} \PY{n}{new\PYZus{}weights}

    \PY{k}{def} \PY{n+nf}{get\PYZus{}values}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{layer\PYZus{}idx}\PY{p}{,} \PY{n}{neuron\PYZus{}idx}\PY{p}{)}\PY{p}{:}
        \PY{k}{if} \PY{p}{(}\PY{n+nb}{abs}\PY{p}{(}\PY{n}{layer\PYZus{}idx}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers}\PY{p}{)}\PY{p}{)} \PY{o+ow}{or} \PY{p}{(}
            \PY{n+nb}{abs}\PY{p}{(}\PY{n}{neuron\PYZus{}idx}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers}\PY{p}{[}\PY{n}{layer\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{weight}\PY{p}{)}
        \PY{p}{)}\PY{p}{:}
            \PY{k}{return} \PY{n}{torch}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)}

        \PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers}\PY{p}{[}\PY{n}{layer\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{weight}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{clone}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{n}{neuron\PYZus{}idx}\PY{p}{,} \PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}


\PY{k}{class} \PY{n+nc}{TorchNet}\PY{p}{(}\PY{n}{VisualNet}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{num\PYZus{}hidden\PYZus{}layers}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} define the layers}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{input\PYZus{}layer} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{input\PYZus{}layer}\PY{p}{)}

        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}hidden\PYZus{}layers}\PY{p}{)}\PY{p}{:}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output\PYZus{}layer} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output\PYZus{}layer}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} pass the result of the previous layer to the next layer}
        \PY{k}{for} \PY{n}{layer} \PY{o+ow}{in} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{:}
            \PY{n}{x} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{tanh}\PY{p}{(}\PY{n}{layer}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}

        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output\PYZus{}layer}\PY{p}{(}\PY{n}{x}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Training the Model}\label{training-the-model}

Similar to the previous Neural Network Notebook, let's pass in the
training data to our model, update the weights with our optimizer, and
update our visualization

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{torch\PYZus{}fit}\PY{p}{(}
    \PY{n}{model}\PY{p}{,}
    \PY{n}{features}\PY{p}{,}
    \PY{n}{labels}\PY{p}{,}
    \PY{n}{epochs}\PY{p}{,}
    \PY{n}{learning\PYZus{}rate}\PY{p}{,}
    \PY{n}{transformations\PYZus{}plot\PYZus{}filename}\PY{p}{,}
    \PY{n}{loss\PYZus{}landscape\PYZus{}plot\PYZus{}filename}\PY{p}{,}
    \PY{n}{device}\PY{p}{,}
    \PY{n}{rows}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,}
    \PY{n}{cols}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,}
    \PY{n}{width\PYZus{}scale}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,}
    \PY{n}{height\PYZus{}scale}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,}
\PY{p}{)}\PY{p}{:}
    \PY{n}{mse\PYZus{}idx} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{epochs} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{errors} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{full}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}

    \PY{n}{cmap} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{cm}\PY{o}{.}\PY{n}{colors}\PY{o}{.}\PY{n}{ListedColormap}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{red}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{blue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}

    \PY{n}{scatterplots}\PY{p}{,} \PY{n}{camera1} \PY{o}{=} \PY{n}{create\PYZus{}scatterplots}\PY{p}{(}\PY{n}{rows}\PY{p}{,} \PY{n}{cols}\PY{p}{,} \PY{n}{width\PYZus{}scale}\PY{p}{,} \PY{n}{height\PYZus{}scale}\PY{p}{)}
    \PY{n}{loss\PYZus{}plots}\PY{p}{,} \PY{n}{camera2} \PY{o}{=} \PY{n}{create\PYZus{}3d\PYZus{}plots}\PY{p}{(}\PY{n}{rows}\PY{p}{,} \PY{n}{cols}\PY{p}{,} \PY{n}{width\PYZus{}scale}\PY{p}{,} \PY{n}{height\PYZus{}scale}\PY{p}{)}

    \PY{n}{loss\PYZus{}fn} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{MSELoss}\PY{p}{(}\PY{p}{)}
    \PY{n}{optimizer} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{optim}\PY{o}{.}\PY{n}{SGD}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{n}{learning\PYZus{}rate}\PY{p}{,} \PY{n}{momentum}\PY{o}{=}\PY{l+m+mf}{0.3}\PY{p}{)}

    \PY{k}{for} \PY{n}{idx} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{epochs}\PY{p}{)}\PY{p}{:}
        \PY{n}{error} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{n}{predictions} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{]}\PY{p}{)}

        \PY{k}{for} \PY{n}{x}\PY{p}{,} \PY{n}{y} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{features}\PY{p}{,} \PY{n}{labels}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} Forward Propagation}
            \PY{n}{output} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{n}{x}\PY{p}{)}

            \PY{n}{output\PYZus{}np} \PY{o}{=} \PY{n}{output}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}
            \PY{n}{predictions} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{predictions}\PY{p}{,} \PY{n}{output\PYZus{}np}\PY{p}{)}

            \PY{c+c1}{\PYZsh{} Store Error}
            \PY{c+c1}{\PYZsh{} tensor(0.) \PYZhy{}\PYZgt{} tensor([0.]) to match shape of output variable}
            \PY{n}{y} \PY{o}{=} \PY{n}{y}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
            \PY{n}{loss} \PY{o}{=} \PY{n}{loss\PYZus{}fn}\PY{p}{(}\PY{n}{output}\PY{p}{,} \PY{n}{y}\PY{p}{)}

            \PY{n}{error} \PY{o}{+}\PY{o}{=} \PY{n}{loss}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}

            \PY{c+c1}{\PYZsh{} Backpropagation}
            \PY{n}{optimizer}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
            \PY{n}{loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
            \PY{n}{optimizer}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}

        \PY{k}{if} \PY{p}{(}
            \PY{n}{idx} \PY{o}{\PYZlt{}} \PY{l+m+mi}{5}
            \PY{o+ow}{or} \PY{p}{(}\PY{n}{idx} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{50} \PY{o+ow}{and} \PY{n}{idx} \PY{o}{\PYZpc{}} \PY{l+m+mi}{5} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}
            \PY{o+ow}{or} \PY{p}{(}\PY{n}{idx} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{1000} \PY{o+ow}{and} \PY{n}{idx} \PY{o}{\PYZpc{}} \PY{l+m+mi}{50} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}
            \PY{o+ow}{or} \PY{n}{idx} \PY{o}{\PYZpc{}} \PY{l+m+mi}{250} \PY{o}{==} \PY{l+m+mi}{0}
        \PY{p}{)}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{epoch: }\PY{l+s+si}{\PYZob{}}\PY{n}{idx}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, MSE: }\PY{l+s+si}{\PYZob{}}\PY{n}{error}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

            \PY{c+c1}{\PYZsh{} Plot MSE}
            \PY{n}{errors}\PY{p}{[}\PY{n}{idx}\PY{p}{]} \PY{o}{=} \PY{n}{error}
            \PY{n}{visible\PYZus{}mse} \PY{o}{=} \PY{n}{errors} \PY{o}{!=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}

            \PY{n}{plot\PYZus{}transformations\PYZus{}and\PYZus{}predictions}\PY{p}{(}
                \PY{n}{scatterplots}\PY{p}{,}
                \PY{n}{model}\PY{p}{,}
                \PY{n}{idx}\PY{p}{,}
                \PY{n}{visible\PYZus{}mse}\PY{p}{,}
                \PY{n}{mse\PYZus{}idx}\PY{p}{,}
                \PY{n}{errors}\PY{p}{,}
                \PY{n}{predictions}\PY{p}{,}
                \PY{n}{features}\PY{p}{,}
                \PY{n}{labels}\PY{p}{,}
                \PY{n}{cmap}\PY{p}{,}
                \PY{n}{rows}\PY{p}{,}
                \PY{n}{cols}\PY{p}{,}
                \PY{n}{device}\PY{p}{,}
            \PY{p}{)}

            \PY{n}{plot\PYZus{}loss\PYZus{}landscape\PYZus{}and\PYZus{}predictions}\PY{p}{(}
                \PY{n}{loss\PYZus{}plots}\PY{p}{,}
                \PY{n}{model}\PY{p}{,}
                \PY{n}{idx}\PY{p}{,}
                \PY{n}{visible\PYZus{}mse}\PY{p}{,}
                \PY{n}{mse\PYZus{}idx}\PY{p}{,}
                \PY{n}{errors}\PY{p}{,}
                \PY{n}{predictions}\PY{p}{,}
                \PY{n}{features}\PY{p}{,}
                \PY{n}{labels}\PY{p}{,}
                \PY{n}{cmap}\PY{p}{,}
                \PY{n}{cols}\PY{p}{,}
                \PY{n}{device}\PY{p}{,}
            \PY{p}{)}

            \PY{n}{camera1}\PY{o}{.}\PY{n}{snap}\PY{p}{(}\PY{p}{)}
            \PY{n}{camera2}\PY{o}{.}\PY{n}{snap}\PY{p}{(}\PY{p}{)}

    \PY{n}{animation1} \PY{o}{=} \PY{n}{camera1}\PY{o}{.}\PY{n}{animate}\PY{p}{(}\PY{p}{)}
    \PY{n}{animation1}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{n}{transformations\PYZus{}plot\PYZus{}filename}\PY{p}{,} \PY{n}{writer}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pillow}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{animation2} \PY{o}{=} \PY{n}{camera2}\PY{o}{.}\PY{n}{animate}\PY{p}{(}\PY{p}{)}
    \PY{n}{animation2}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{n}{loss\PYZus{}landscape\PYZus{}plot\PYZus{}filename}\PY{p}{,} \PY{n}{writer}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pillow}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Visualize the transformations and weight
updates}\label{visualize-the-transformations-and-weight-updates}

Let's call our training function with our model to create the
visualization.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{device} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{device}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cuda:0}\PY{l+s+s2}{\PYZdq{}} \PY{k}{if} \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{is\PYZus{}available}\PY{p}{(}\PY{p}{)} \PY{k}{else} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cpu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{torch\PYZus{}model} \PY{o}{=} \PY{n}{TorchNet}\PY{p}{(}\PY{n}{num\PYZus{}hidden\PYZus{}layers}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}

\PY{n}{rows} \PY{o}{=} \PY{l+m+mi}{2}
\PY{n}{cols} \PY{o}{=} \PY{l+m+mi}{3}

\PY{c+c1}{\PYZsh{} the inputs and outputs for PyTorch must be tensors}
\PY{n}{X\PYZus{}tensor} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{y\PYZus{}tensor} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}

\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{601}
\PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{l+m+mf}{0.005}

\PY{n}{transformations\PYZus{}plot\PYZus{}filename} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{neural\PYZus{}network\PYZus{}weights.gif}\PY{l+s+s2}{\PYZdq{}}
\PY{n}{loss\PYZus{}landscape\PYZus{}plot\PYZus{}filename} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{neural\PYZus{}network\PYZus{}weights\PYZus{}loss\PYZus{}landscape.gif}\PY{l+s+s2}{\PYZdq{}}
\PY{n}{torch\PYZus{}fit}\PY{p}{(}
    \PY{n}{torch\PYZus{}model}\PY{p}{,}
    \PY{n}{X\PYZus{}tensor}\PY{p}{,}
    \PY{n}{y\PYZus{}tensor}\PY{p}{,}
    \PY{n}{epochs}\PY{p}{,}
    \PY{n}{learning\PYZus{}rate}\PY{p}{,}
    \PY{n}{transformations\PYZus{}plot\PYZus{}filename}\PY{p}{,}
    \PY{n}{loss\PYZus{}landscape\PYZus{}plot\PYZus{}filename}\PY{p}{,}
    \PY{n}{device}\PY{p}{,}
    \PY{n}{rows}\PY{o}{=}\PY{n}{rows}\PY{p}{,}
    \PY{n}{cols}\PY{o}{=}\PY{n}{cols}\PY{p}{,}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
epoch: 0, MSE: 137.9026336669922
epoch: 1, MSE: 127.58297729492188
epoch: 2, MSE: 127.3271713256836
epoch: 3, MSE: 127.13655853271484
epoch: 4, MSE: 126.98948669433594
epoch: 5, MSE: 126.87287139892578
epoch: 10, MSE: 126.53106689453125
epoch: 15, MSE: 126.36543273925781
epoch: 20, MSE: 126.266357421875
epoch: 25, MSE: 126.19721984863281
epoch: 30, MSE: 126.14079284667969
epoch: 35, MSE: 126.08564758300781
epoch: 40, MSE: 126.01984405517578
epoch: 45, MSE: 125.92532348632812
epoch: 50, MSE: 125.76524353027344
epoch: 100, MSE: 95.95246124267578
epoch: 150, MSE: 56.10152053833008
epoch: 200, MSE: 54.98665237426758
epoch: 250, MSE: 38.98552703857422
epoch: 300, MSE: 20.435640335083008
epoch: 350, MSE: 13.933258056640625
epoch: 400, MSE: 11.308575630187988
epoch: 450, MSE: 9.152091979980469
epoch: 500, MSE: 7.561526298522949
epoch: 550, MSE: 6.4366888999938965
epoch: 600, MSE: 5.587584018707275
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{combined_files/combined_136_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{combined_files/combined_136_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    This visualization shows the internal linear transformation of each
layer in the neural network. We will show each layer transform an input
to an output. At the 3rd layer, the data becomes linearly separable. As
a result of these transformations, the network is able to classify
inputs into these two classes.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{transformations\PYZus{}plot\PYZus{}filename}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<IPython.core.display.Image object>
\end{Verbatim}
\end{tcolorbox}
        
    This visualization shows that the weights all update together such that
the loss function at the last layer reaches the minima. See how the 3D
plot in layer 4 is at a minima.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{loss\PYZus{}landscape\PYZus{}plot\PYZus{}filename}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<IPython.core.display.Image object>
\end{Verbatim}
\end{tcolorbox}
        
    \subsection{Vanishing Gradients}\label{vanishing-gradients}

It's a valid assumption that increasing the number of layers of a neural
network allows it to recognize more patterns. However, scaling is not
this straightforward. With the wrong architectures, the networks can
stop scaling. The next visualization increases the number of layers to
12 and shows that the weights can not update. This is known as vanishing
gradients since the updates being sent to the earlier layers in the
network are close to zero, preventing the weights from updating. The
ResNet paper introduced residual connections, which helped solved this
issue.

    \subsubsection{12 Layer Neural Network}\label{layer-neural-network}

Let's rerun our code but with a larger network this time

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{device} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{device}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cuda:0}\PY{l+s+s2}{\PYZdq{}} \PY{k}{if} \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{is\PYZus{}available}\PY{p}{(}\PY{p}{)} \PY{k}{else} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cpu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{torch\PYZus{}model} \PY{o}{=} \PY{n}{TorchNet}\PY{p}{(}\PY{n}{num\PYZus{}hidden\PYZus{}layers}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}

\PY{n}{rows} \PY{o}{=} \PY{l+m+mi}{2}
\PY{n}{cols} \PY{o}{=} \PY{l+m+mi}{7}

\PY{n}{X\PYZus{}tensor} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{y\PYZus{}tensor} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}

\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{1001}
\PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{l+m+mf}{0.005}

\PY{n}{width\PYZus{}scale} \PY{o}{=} \PY{l+m+mi}{4}
\PY{n}{height\PYZus{}scale} \PY{o}{=} \PY{l+m+mi}{2}

\PY{n}{transformations\PYZus{}plot\PYZus{}filename} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{vanishing\PYZus{}gradients/layers\PYZus{}12.gif}\PY{l+s+s2}{\PYZdq{}}
\PY{n}{loss\PYZus{}landscape\PYZus{}plot\PYZus{}filename} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{vanishing\PYZus{}gradients/layers\PYZus{}12\PYZus{}loss\PYZus{}landscape.gif}\PY{l+s+s2}{\PYZdq{}}
\PY{n}{torch\PYZus{}fit}\PY{p}{(}
    \PY{n}{torch\PYZus{}model}\PY{p}{,}
    \PY{n}{X\PYZus{}tensor}\PY{p}{,}
    \PY{n}{y\PYZus{}tensor}\PY{p}{,}
    \PY{n}{epochs}\PY{p}{,}
    \PY{n}{learning\PYZus{}rate}\PY{p}{,}
    \PY{n}{transformations\PYZus{}plot\PYZus{}filename}\PY{p}{,}
    \PY{n}{loss\PYZus{}landscape\PYZus{}plot\PYZus{}filename}\PY{p}{,}
    \PY{n}{device}\PY{p}{,}
    \PY{n}{rows}\PY{o}{=}\PY{n}{rows}\PY{p}{,}
    \PY{n}{cols}\PY{o}{=}\PY{n}{cols}\PY{p}{,}
    \PY{n}{width\PYZus{}scale}\PY{o}{=}\PY{n}{width\PYZus{}scale}\PY{p}{,}
    \PY{n}{height\PYZus{}scale}\PY{o}{=}\PY{n}{height\PYZus{}scale}\PY{p}{,}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
epoch: 0, MSE: 140.93698120117188
epoch: 1, MSE: 127.2381362915039
epoch: 2, MSE: 127.21744537353516
epoch: 3, MSE: 127.19805145263672
epoch: 4, MSE: 127.17977905273438
epoch: 5, MSE: 127.1625747680664
epoch: 10, MSE: 127.08917236328125
epoch: 15, MSE: 127.03060150146484
epoch: 20, MSE: 126.98188018798828
epoch: 25, MSE: 126.94023895263672
epoch: 30, MSE: 126.90355682373047
epoch: 35, MSE: 126.87098693847656
epoch: 40, MSE: 126.84139251708984
epoch: 45, MSE: 126.81401062011719
epoch: 50, MSE: 126.78815460205078
epoch: 100, MSE: 126.53387451171875
epoch: 150, MSE: 126.2475814819336
epoch: 200, MSE: 126.10691833496094
epoch: 250, MSE: 126.07827758789062
epoch: 300, MSE: 126.07350158691406
epoch: 350, MSE: 126.07276153564453
epoch: 400, MSE: 126.0726318359375
epoch: 450, MSE: 126.072509765625
epoch: 500, MSE: 126.07250213623047
epoch: 550, MSE: 126.07252502441406
epoch: 600, MSE: 126.07251739501953
epoch: 650, MSE: 126.07252502441406
epoch: 700, MSE: 126.07251739501953
epoch: 750, MSE: 126.07251739501953
epoch: 800, MSE: 126.07251739501953
epoch: 850, MSE: 126.07251739501953
epoch: 900, MSE: 126.07251739501953
epoch: 950, MSE: 126.07251739501953
epoch: 1000, MSE: 126.07251739501953
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{combined_files/combined_143_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{combined_files/combined_143_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    In the visualizations below, we see the earlier layers not updating at
all from vanishing gradients.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{transformations\PYZus{}plot\PYZus{}filename}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<IPython.core.display.Image object>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{loss\PYZus{}landscape\PYZus{}plot\PYZus{}filename}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<IPython.core.display.Image object>
\end{Verbatim}
\end{tcolorbox}
        
    \section{Neural Network}\label{neural-network}

Neural Networks are a machine learning model that learns the optimal
parameters to approximate complex functions.

GitHub Repo: https://github.com/gavinkhung/neural-network

    Import the libraries

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{o}{\PYZpc{}}\PY{k}{matplotlib} ipympl
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{mpl\PYZus{}toolkits}\PY{n+nn}{.}\PY{n+nn}{mplot3d} \PY{k+kn}{import} \PY{n}{Axes3D}

\PY{k+kn}{from} \PY{n+nn}{celluloid} \PY{k+kn}{import} \PY{n}{Camera}
\PY{k+kn}{import} \PY{n+nn}{scienceplots}
\PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k+kn}{import} \PY{n}{Image}

\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{style}\PY{o}{.}\PY{n}{use}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{science}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{no\PYZhy{}latex}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Training Dataset}\label{training-dataset}

The neural network will learn the parameters to fit a Hyperbolic
Paraboloid.

\begin{align*}
z &= \frac{y^2}{b^2} - \frac{x^2}{a^2}
\end{align*}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{generate\PYZus{}function}\PY{p}{(}\PY{n}{dims}\PY{p}{)}\PY{p}{:}
    \PY{n}{a} \PY{o}{=} \PY{l+m+mi}{1}
    \PY{n}{b} \PY{o}{=} \PY{l+m+mi}{1}

    \PY{c+c1}{\PYZsh{} Hyperbolic Paraboloid}
    \PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{dims}\PY{p}{)}
    \PY{n}{y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{dims}\PY{p}{)}
    \PY{n}{X}\PY{p}{,} \PY{n}{Y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{meshgrid}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
    \PY{n}{Z} \PY{o}{=} \PY{p}{(}\PY{n}{Y}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{o}{/} \PY{n}{b}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{p}{(}\PY{n}{X}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{o}{/} \PY{n}{a}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}

    \PY{n}{X\PYZus{}t} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
    \PY{n}{Y\PYZus{}t} \PY{o}{=} \PY{n}{Y}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
    \PY{n}{Z\PYZus{}t} \PY{o}{=} \PY{n}{Z}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
    \PY{n}{X\PYZus{}t} \PY{o}{=} \PY{n}{X\PYZus{}t}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}t}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
    \PY{n}{Y\PYZus{}t} \PY{o}{=} \PY{n}{Y\PYZus{}t}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{Y\PYZus{}t}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
    \PY{n}{Z\PYZus{}t} \PY{o}{=} \PY{n}{Z\PYZus{}t}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{Z\PYZus{}t}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
    \PY{n}{features} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{stack}\PY{p}{(}\PY{p}{(}\PY{n}{X\PYZus{}t}\PY{p}{,} \PY{n}{Y\PYZus{}t}\PY{p}{)}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{labels} \PY{o}{=} \PY{n}{Z\PYZus{}t}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{Z\PYZus{}t}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}

    \PY{k}{return} \PY{n}{X}\PY{p}{,} \PY{n}{Y}\PY{p}{,} \PY{n}{Z}\PY{p}{,} \PY{n}{features}\PY{p}{,} \PY{n}{labels}


\PY{n}{dims} \PY{o}{=} \PY{l+m+mi}{12}
\PY{n}{X}\PY{p}{,} \PY{n}{Y}\PY{p}{,} \PY{n}{Z}\PY{p}{,} \PY{n}{features}\PY{p}{,} \PY{n}{labels} \PY{o}{=} \PY{n}{generate\PYZus{}function}\PY{p}{(}\PY{n}{dims}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Visualize the Hyperbolic Paraboloid}
\PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
\PY{n}{ax} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{111}\PY{p}{,} \PY{n}{projection}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{3d}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{ax}\PY{o}{.}\PY{n}{plot\PYZus{}surface}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{Y}\PY{p}{,} \PY{n}{Z}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{red}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<mpl\_toolkits.mplot3d.art3d.Poly3DCollection at 0xffff6c1f8530>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{combined_files/combined_151_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Loss Function}\label{loss-function}

The loss function is needed to evaluate the performance of the model and
to update the weights accordingly. The optimizaton process of the neural
network training will find the weights and biases to minimie the loss.

    \subsubsection{Mean Squared Error}\label{mean-squared-error}

Quadratic loss functions, like mean squared error, are used for
regression tasks, like this example.

\[
J = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{mse}\PY{p}{(}\PY{n}{y\PYZus{}true}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{y\PYZus{}true} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}pred}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsubsection{Mean Squared Error
Gradient}\label{mean-squared-error-gradient}

In order to perform backpropagation to update the weights, we need to
calculate the gradient of the loss function.

\begin{align*}
J^{\prime} &= \frac{\partial}{\partial \hat{y}} [ \frac{1}{n} \sum_{i=1}^{n}(y_{i}-\hat{y})^2 ] \\
&= \frac{1}{n} \sum_{i=1}^{n}\frac{\partial}{\partial \hat{y}} [ (y_{i}-\hat{y})^2 ] \\
&= \frac{2}{n} \sum_{i=1}^{n} (y_{i}-\hat{y}) \frac{\partial}{\partial \hat{y}}[y_{i}-\hat{y}] \\
&= \frac{2}{n} \sum_{i=1}^{n} (y_{i}-\hat{y}) (-1) \\
&= \frac{2}{n} \sum_{i=1}^{n}(\hat{y}-y_{i})
\end{align*}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{mse\PYZus{}prime}\PY{p}{(}\PY{n}{y\PYZus{}true}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{l+m+mi}{2} \PY{o}{*} \PY{p}{(}\PY{n}{y\PYZus{}pred} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}true}\PY{p}{)} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{n}{y\PYZus{}true}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Neural Network Layer}\label{neural-network-layer}

We will implement a feedforward neural network, which contains many
layers. Each layer at a very high applies a linear transformation to the
input data to create the output data, which is often in a different
dimension than the input data. Then, all of the values in the output are
run through a function, known as an activation function.

Recall from Linear Algebra that an input column vector \(x\) in
\(\mathbb{R}^n\) can be transformed into another dimension
\(\mathbb{R}^m\) by multipying it with a matrix of size \(m \times n\).
Finally we can add a bias term to this linear transformation to shift
the transformed data up or down.

As a result, each layer stores a weight matrix and a bias vector to
apply the linear transformation.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{abc} \PY{k+kn}{import} \PY{n}{ABC}\PY{p}{,} \PY{n}{abstractmethod}


\PY{k}{class} \PY{n+nc}{Layer}\PY{p}{(}\PY{n}{ABC}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{input} \PY{o}{=} \PY{k+kc}{None}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output} \PY{o}{=} \PY{k+kc}{None}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{weights} \PY{o}{=} \PY{k+kc}{None}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias} \PY{o}{=} \PY{k+kc}{None}

    \PY{n+nd}{@abstractmethod}
    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n+nb}{input}\PY{p}{)}\PY{p}{:}
        \PY{k}{pass}

    \PY{n+nd}{@abstractmethod}
    \PY{k}{def} \PY{n+nf}{backward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{output\PYZus{}gradient}\PY{p}{,} \PY{n}{optimizer}\PY{p}{)}\PY{p}{:}
        \PY{k}{pass}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Forward Propagation}\label{forward-propagation}

This the process of transforming our input data into the predictions
from our feedforward neural network. The input data is passed through
every layer of the network. Below is a visualization of the 3 layer
network we will implement.

    Each neuron in a layer is a weighted sum of its inputs plus a bias term.
Finally, an activation function is applied on each neuron in the layer.
We will have a class to represent a fully-connected layer and another
class to represent an activation function.

The computation for each neuron in a layer being a weighted sum of the
products between the inputs and the layer's weights plus a bias term is
the same as a matrix multiplication between the weight matrix and the
input data added with a bias vector.

Thus, the foward propagation of a fully connected layer can be written
as:

\begin{align*}
Z &= W X + B \\
A &= g(Z)
\end{align*}

Where: - \(W\) is the weight matrix for the layer - \(X\) is the input
data - \(B\) is a bias vector - \(g\) is the activation function

    \subsection{Our Network's Forward
Propagation}\label{our-networks-forward-propagation}

    We will implement a 3-layer fully-connected neural network with a Tanh
activation function after each layer. These transformations can be
written as these matrix multiplications:

\begin{align*}
Z_1 &= W_1 X + B_1 \\
A_1 &= tanh(Z_1) \\
\\
Z_2 &= W_2 A_1 + B_2 \\
A_2 &= tanh(Z_2) \\
\\
Z_3 &= W_3 A_2 + B_3 \\
\hat{y} &= A_3 = tanh(Z_3)
\end{align*}

    \subsection{Backpropagation}\label{backpropagation}

Backpropagation is the process of updating all of the weights and biases
in a neural network using the chain rule and gradient descent.

The following equations use \(J\) as our loss function. In this
Notebook, we use the mean squared error loss function. Click
\hyperref[mean-squared-error]{here for the mean squared error loss function}.

Our goal is to apply gradient descent on the weights and bias using the
following equations:

\begin{align*}
W_1 &= W_1 - lr * \frac{\partial J}{\partial W_1} \\
B_1 &= B_1 - lr * \frac{\partial J}{\partial B_1} \\
W_2 &= W_2 - lr * \frac{\partial J}{\partial W_2} \\
B_2 &= B_2 - lr * \frac{\partial J}{\partial B_2} \\
W_3 &= W_3 - lr * \frac{\partial J}{\partial W_3} \\
B_3 &= B_3 - lr * \frac{\partial J}{\partial B_3} \\
\end{align*}

We need to use the chain rule to get the values for
\(\frac{\partial J}{\partial W_1}\),
\(\frac{\partial J}{\partial B_1}\),
\(\frac{\partial J}{\partial W_2}\),
\(\frac{\partial J}{\partial B_2}\),
\(\frac{\partial J}{\partial W_3}\), and
\(\frac{\partial J}{\partial B_3}\).

    \subsubsection{Chain Rule for
Backpropagation}\label{chain-rule-for-backpropagation}

Let's derive a general formula to update the weight matrix \(W_i\) and
bias vector \(B_i\) of a single fully-connected layer.

To apply gradient descent on the 3rd layer, for example, we need to find
the loss with respect to \(W_3\) and \(B_3\), which are
\(\frac{\partial J}{\partial W_3}\), and
\(\frac{\partial J}{\partial B_3}\).

The loss function \(J\) is in terms of \(A_3\), which is the final
output/activation of the last layer. Then, \(A_3\) is in terms of
\(Z_3\). Lastly, \(Z_3\) is in terms of \(W_3\), \(B_3\), and \(A_2\),
which is the activation from the second to last layer. Let's get the
loss with respect to \(W_3\) and \(B_3\).

We can represent these matrix operations as the following composite
functions: \(J(A_3)\), \(A_3(Z_3)\), and \(Z_3(W_3, B_3, A_2)\)

Click
\hyperref[our-network-s-forward-propagation]{here for the operations of each layer}.

    \subsubsection{Chain Rule For Weight Matrix
W}\label{chain-rule-for-weight-matrix-w}

Use the chain rule to derive \(\frac{\partial J}{\partial W_3}\)

\begin{align*}
\frac{\partial J}{\partial W_3} &= \frac{\partial}{\partial W_3}[J(A_3(Z_3(W_3, B_3, A_2)))] \\
&= \frac{\partial J}{\partial A_3} \frac{\partial}{\partial W_3}[A_3(Z_3(W_3, B_3, A_2))] \\
&= \frac{\partial J}{\partial A_3} \frac{\partial A_3}{\partial Z_3} \frac{\partial}{\partial W_3}[Z_3(W_3, B_3, A_2))] \\
&= \frac{\partial J}{\partial A_3} \frac{\partial A_3}{\partial Z_3} \frac{\partial Z_3}{\partial W_3} \\
\end{align*}

    \subsubsection{Chain Rule For Bias Vector
B}\label{chain-rule-for-bias-vector-b}

Use the chain rule to derive \(\frac{\partial J}{\partial B_3}\).

\begin{align*}
\frac{\partial J}{\partial B_3} &= \frac{\partial}{\partial B_3}[J(A_3(Z_3(W_3, B_3, A_2)))] \\
&= \frac{\partial J}{\partial A_3} \frac{\partial}{\partial B_3}[A_3(Z_3(W_3, B_3, A_2))] \\
&= \frac{\partial J}{\partial A_3} \frac{\partial A_3}{\partial B_3} \frac{\partial}{\partial B_3}[Z_3(W_3, B_3, A_2))] \\
&= \frac{\partial J}{\partial A_3} \frac{\partial A_3}{\partial Z_3} \frac{\partial Z_3}{\partial B_3} \\
\end{align*}

    \subsection{Backpropagation for Weight Matrix
W}\label{backpropagation-for-weight-matrix-w}

\begin{align*}
\frac{\partial J}{\partial W_3} = \frac{\partial J}{\partial A_3} \frac{\partial A_3}{\partial Z_3} \frac{\partial Z_3}{\partial W_3}
\end{align*}

Let's break each component down:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(\frac{\partial J}{\partial A_3}\) is the gradient of the loss
  function, which is the gradient of the mean squared error function.
  Click
  \hyperref[mean-squared-error-gradient]{here for the derivation of the loss function gradient}.
  In the general case for any layer, this is the gradient from the next
  layer (idx+1).
\item
  \[\frac{\partial A_3}{\partial Z_3} =
  \frac{\partial}{\partial Z_3}{[}tanh(Z\_3){]} \] is the gradient of
  the activation function. Click
  \hyperref[tanh-activation-function-gradient]{here for the derivation of the activation function gradient}.
\item
  \(\frac{\partial Z_3}{\partial W_3} = \frac{\partial}{\partial W_3}[W_3 A_2 + B_3] = A_2\)
  is the original input to the layer, which is the output of the
  previous layer (idx-1).
\end{enumerate}

As a result, to the gradient of the weight matrix of a fully-connected
layer is the matrix multiplication products of the following: 1. The
gradient from the next layer (idx+1) 2. The gradient of the activation
function 3. The input from the previous layer (idx-1)

    \subsection{Backpropagation for Bias Vector
B}\label{backpropagation-for-bias-vector-b}

\begin{align*}
\frac{\partial J}{\partial B_3} = \frac{\partial J}{\partial A_3} \frac{\partial A_3}{\partial Z_3} \frac{\partial Z_3}{\partial B_3}
\end{align*}

Let's break each component down:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(\frac{\partial J}{\partial A_3}\) is the gradient of the loss
  function, which is the gradient of the mean squared error function.
  Click
  \hyperref[mean-squared-error-gradient]{here for the derivation of the loss function gradient}.
  In the general case for any layer, this is the gradient from the next
  layer (idx+1).
\item
  \(\frac{\partial A_3}{\partial Z_3}\) is the gradient of the
  activation function. Click
  \hyperref[tanh-activation-function-gradient]{here for the derivation of the activation function gradient}.
\item
  \(\frac{\partial Z_3}{\partial B_3} = \frac{\partial}{\partial B_3}[W_3 A_2 + B_3] = 1\)
  is 1, we can ignore this in the gradient computation.
\end{enumerate}

As a result, to the gradient of the bias vector of a fully-connected
layer is the matrix multiplication products of the following: 1. The
gradient from the next layer (idx+1) 2. The gradient of the activation
function

    For more information, I recommend the follow resources: -
\href{https://www.youtube.com/watch?v=pauPCy_s0Ok}{Neural Network from
Scratch}. I also watched this video to help write this Notebook. -
\href{https://www.youtube.com/watch?v=SmZmBKc7Lrs}{The Most Important
Algorithm in Machine Learning}

    \subsection{Dense Layers}\label{dense-layers}

A dense layer is a fully connected layer. Let's use the equations
derived in the forward and backwards propagation sections above to
implement the \texttt{forward()} and \texttt{backward()} methods of our
dense layer class.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{Dense}\PY{p}{(}\PY{n}{Layer}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{input\PYZus{}neurons}\PY{p}{,} \PY{n}{output\PYZus{}neurons}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} Random Values from Normal Distribution}
        \PY{c+c1}{\PYZsh{} self.weights = np.random.randn(output\PYZus{}neurons, input\PYZus{}neurons)}
        \PY{c+c1}{\PYZsh{} self.bias = np.random.randn(output\PYZus{}neurons, 1)}

        \PY{c+c1}{\PYZsh{} All Zeros}
        \PY{c+c1}{\PYZsh{} self.weights = np.zeros((output\PYZus{}neurons, input\PYZus{}neurons))}
        \PY{c+c1}{\PYZsh{} self.bias = np.zeros((output\PYZus{}neurons, 1))}

        \PY{c+c1}{\PYZsh{} Xavier Initialization Uniform Distribution}
        \PY{n}{limit} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{l+m+mi}{6} \PY{o}{/} \PY{p}{(}\PY{n}{input\PYZus{}neurons} \PY{o}{+} \PY{n}{output\PYZus{}neurons}\PY{p}{)}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{weights} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{uniform}\PY{p}{(}
            \PY{o}{\PYZhy{}}\PY{n}{limit}\PY{p}{,} \PY{n}{limit}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{p}{(}\PY{n}{output\PYZus{}neurons}\PY{p}{,} \PY{n}{input\PYZus{}neurons}\PY{p}{)}
        \PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n}{output\PYZus{}neurons}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n+nb}{input}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{input} \PY{o}{=} \PY{n+nb}{input}
        \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{weights}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{input}\PY{p}{)} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias}

    \PY{k}{def} \PY{n+nf}{backward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{output\PYZus{}gradient}\PY{p}{,} \PY{n}{optimizer}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} Calculate gradients}
        \PY{n}{weights\PYZus{}gradient} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{output\PYZus{}gradient}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{input}\PY{o}{.}\PY{n}{T}\PY{p}{)}
        \PY{n}{input\PYZus{}gradient} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{weights}\PY{o}{.}\PY{n}{T}\PY{p}{,} \PY{n}{output\PYZus{}gradient}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Update weights and biases}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{weights}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias} \PY{o}{=} \PY{n}{optimizer}\PY{o}{.}\PY{n}{backward}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{weights}\PY{p}{,} \PY{n}{weights\PYZus{}gradient}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias}\PY{p}{,} \PY{n}{output\PYZus{}gradient}
        \PY{p}{)}
        \PY{k}{return} \PY{n}{input\PYZus{}gradient}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Activation Function}\label{activation-function}

Activation functions are applied after the matrix multiplication to
introduce nonlinearity into our neural networks. Choosing the correct
activation function is essential for the neural network to learn general
patterns of the training data. Most notably, the ReLU function is very
useful when training very deep neural networks with many layers, as seen
from the 2012 AlexNet paper, in order to prevent vanishing gradients,
where the network fails to update its weights from super small gradients

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{Activation}\PY{p}{(}\PY{n}{Layer}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{activation}\PY{p}{,} \PY{n}{activation\PYZus{}prime}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{activation} \PY{o}{=} \PY{n}{activation}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{activation\PYZus{}prime} \PY{o}{=} \PY{n}{activation\PYZus{}prime}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{input\PYZus{}val}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{input} \PY{o}{=} \PY{n}{input\PYZus{}val}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{activation}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{input}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{backward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{output\PYZus{}gradient}\PY{p}{,} \PY{n}{optimizer}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{multiply}\PY{p}{(}\PY{n}{output\PYZus{}gradient}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{activation\PYZus{}prime}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{input}\PY{p}{)}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{plot}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x\PYZus{}min}\PY{p}{,} \PY{n}{x\PYZus{}max}\PY{p}{,} \PY{n}{points}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{)}\PY{p}{:}
        \PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{x\PYZus{}min}\PY{p}{,} \PY{n}{x\PYZus{}max}\PY{p}{,} \PY{n}{points}\PY{p}{)}
        \PY{n}{y} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{activation}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{n}{y\PYZus{}prime} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{activation\PYZus{}prime}\PY{p}{(}\PY{n}{y}\PY{p}{)}

        \PY{n}{fig}\PY{p}{,} \PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{X}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Y}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{F(X)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y\PYZus{}prime}\PY{p}{)}
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{X}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Y}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{F}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{(X)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsubsection{Tanh Activation Function}\label{tanh-activation-function}

We will use the Tanh activation function for our network:

\begin{align*}
\sigma(z) &= \frac{e^z-e^{-z}}{e^z+e^{-z}}
\end{align*}

    \subsubsection{Tanh Activation Function
Gradient}\label{tanh-activation-function-gradient}

Since gradient descent relies on knowing the gradient of our activation
function, let's derivate the gradient of the tanh function.

\begin{align*}
\sigma^\prime(z) &= \frac{\partial}{\partial z} [ \frac{e^z-e^{-z}}{e^z+e^{-z}} ] \\
&= \frac{(e^z+e^{-z})\frac{\partial}{\partial z}[e^z-e^{-z}] - (e^z-e^{-z})\frac{\partial}{\partial z}[e^z+e^{-z}]}{({e^z+e^{-z}})^2} \\
&= \frac{(e^z+e^{-z})(e^z+e^{-z}) - (e^z-e^{-z})(e^z-e^{-z})}{({e^z+e^{-z}})^2} \\
&= \frac{(e^z+e^{-z})^2 - (e^z-e^{-z})^2}{({e^z+e^{-z}})^2} \\
&= \frac{(e^z+e^{-z})^2}{({e^z+e^{-z}})^2} - \frac{(e^z-e^{-z})^2}{({e^z+e^{-z}})^2} \\
&= 1 - [\sigma(z)]^2
\end{align*}

    Now that we have derived the gradient of the Tanh function, let's create
the class for the Tanh activation function.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{Tanh}\PY{p}{(}\PY{n}{Activation}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{n}{tanh} \PY{o}{=} \PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{tanh}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{n}{tanh\PYZus{}prime} \PY{o}{=} \PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{tanh}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{o}{*}\PY{o}{*} \PY{l+m+mi}{2}
        \PY{n+nb}{super}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n}{tanh}\PY{p}{,} \PY{n}{tanh\PYZus{}prime}\PY{p}{)}


\PY{n}{Tanh}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{combined_files/combined_177_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Optimizer}\label{optimizer}

Our optimization algorithm will be Gradient Descent, allowing us to
determine how to update the parameters in the next iteration.
$ X\_\{n+1\} = X\_n - lr * \frac{\partial}{\partial X} f(X\_n)$.

Let's create a class that updates the weight matrix and the bias vector
using the Gradient Descent equation.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{GradientDescentOptimizier}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{n}{learning\PYZus{}rate}

    \PY{k}{def} \PY{n+nf}{backward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{weights}\PY{p}{,} \PY{n}{weights\PYZus{}gradient}\PY{p}{,} \PY{n}{bias}\PY{p}{,} \PY{n}{output\PYZus{}gradient}\PY{p}{)}\PY{p}{:}
        \PY{n}{weights} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{learning\PYZus{}rate} \PY{o}{*} \PY{n}{weights\PYZus{}gradient}
        \PY{n}{bias} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{learning\PYZus{}rate} \PY{o}{*} \PY{n}{output\PYZus{}gradient}

        \PY{k}{return} \PY{n}{weights}\PY{p}{,} \PY{n}{bias}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Graphing Functions}\label{graphing-functions}

This Notebook will create many visualizations of the neural network
during its training phase.

\texttt{create\_scatter\_and\_3d\_plot()} creates a 2 column graph for
the Mean Squared Error graph on the left and a 3D graph on the right.

\texttt{create\_3d\_and\_3d\_plot()} creates a 2 column graph with two
3D graphs.

\texttt{plot\_3d\_predictions()} plots the neural network's current
preditions and the expected output of the neural network.

\texttt{plot\_layer\_loss\_landscape()} plots how close one layer's
current weights are from the optimal weights by seeing how the total
mean squared error changes if the weights were shifted a little.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{copy}


\PY{k}{def} \PY{n+nf}{create\PYZus{}scatter\PYZus{}and\PYZus{}3d\PYZus{}plot}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{16} \PY{o}{/} \PY{l+m+mf}{9.0} \PY{o}{*} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{4} \PY{o}{*} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
    \PY{n}{fig}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Neural Network Predictions}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Epoch}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontweight}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{normal}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontweight}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{normal}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Mean Squared Error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{off}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{off}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{projection}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{3d}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{X}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Y}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}zlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Z}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Function Approximation}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{view\PYZus{}init}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{35}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}zlim}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{equal}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{camera} \PY{o}{=} \PY{n}{Camera}\PY{p}{(}\PY{n}{fig}\PY{p}{)}
    \PY{k}{return} \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{camera}


\PY{k}{def} \PY{n+nf}{create\PYZus{}3d\PYZus{}and\PYZus{}3d\PYZus{}plot}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}
        \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{16} \PY{o}{/} \PY{l+m+mf}{9.0} \PY{o}{*} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{4} \PY{o}{*} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{subplot\PYZus{}kw}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{projection}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{3d}\PY{l+s+s2}{\PYZdq{}}\PY{p}{\PYZcb{}}
    \PY{p}{)}
    \PY{n}{fig}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Neural Network Loss Landscape}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W3\PYZus{}1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{W3\PYZus{}2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}zlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Mean Squared Error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{view\PYZus{}init}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{35}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}zlim}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{equal}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{X}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Y}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}zlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Z}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Function Approximation}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{view\PYZus{}init}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{35}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}zlim}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{equal}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{camera} \PY{o}{=} \PY{n}{Camera}\PY{p}{(}\PY{n}{fig}\PY{p}{)}
    \PY{k}{return} \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{camera}


\PY{k}{def} \PY{n+nf}{plot\PYZus{}3d\PYZus{}predictions}\PY{p}{(}\PY{n}{ax}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{Y}\PY{p}{,} \PY{n}{Z}\PY{p}{,} \PY{n}{predictions}\PY{p}{,} \PY{n}{dims}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Plot Neural Network Function Approximation}
    \PY{c+c1}{\PYZsh{} Ground truth}
    \PY{n}{ground\PYZus{}truth\PYZus{}legend} \PY{o}{=} \PY{n}{ax}\PY{o}{.}\PY{n}{plot\PYZus{}surface}\PY{p}{(}
        \PY{n}{X}\PY{p}{,} \PY{n}{Y}\PY{p}{,} \PY{n}{Z}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{red}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ground Truth}\PY{l+s+s2}{\PYZdq{}}
    \PY{p}{)}

    \PY{c+c1}{\PYZsh{} Neural Network Predictions}
    \PY{n}{predictions\PYZus{}legend} \PY{o}{=} \PY{n}{ax}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}
        \PY{n}{X}\PY{p}{,}
        \PY{n}{Y}\PY{p}{,}
        \PY{n}{predictions}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{(}\PY{n}{dims}\PY{p}{,} \PY{n}{dims}\PY{p}{)}\PY{p}{)}\PY{p}{,}
        \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{blue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
        \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,}
        \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Prediction}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{p}{)}
    \PY{n}{ax}\PY{o}{.}\PY{n}{plot\PYZus{}surface}\PY{p}{(}
        \PY{n}{X}\PY{p}{,}
        \PY{n}{Y}\PY{p}{,}
        \PY{n}{predictions}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{(}\PY{n}{dims}\PY{p}{,} \PY{n}{dims}\PY{p}{)}\PY{p}{)}\PY{p}{,}
        \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{blue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
        \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.3}\PY{p}{,}
    \PY{p}{)}
    \PY{n}{ax}\PY{o}{.}\PY{n}{legend}\PY{p}{(}
        \PY{p}{(}\PY{n}{ground\PYZus{}truth\PYZus{}legend}\PY{p}{,} \PY{n}{predictions\PYZus{}legend}\PY{p}{)}\PY{p}{,}
        \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ground Truth}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Predictions}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,}
        \PY{n}{loc}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{upper left}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{p}{)}


\PY{k}{def} \PY{n+nf}{plot\PYZus{}layer\PYZus{}loss\PYZus{}landscape}\PY{p}{(}
    \PY{n}{ax0}\PY{p}{,}
    \PY{n}{network}\PY{p}{,}
    \PY{n}{target\PYZus{}layer\PYZus{}idx}\PY{p}{,}
    \PY{n}{features}\PY{p}{,}
    \PY{n}{labels}\PY{p}{,}
    \PY{n}{w1\PYZus{}min}\PY{p}{,}
    \PY{n}{w1\PYZus{}max}\PY{p}{,}
    \PY{n}{w2\PYZus{}min}\PY{p}{,}
    \PY{n}{w2\PYZus{}max}\PY{p}{,}
    \PY{n}{loss\PYZus{}dims}\PY{p}{,}
\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} current target layer weights}
    \PY{n}{target\PYZus{}layer\PYZus{}idx} \PY{o}{=} \PY{n}{target\PYZus{}layer\PYZus{}idx} \PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{network}\PY{p}{)}

    \PY{n}{w1} \PY{o}{=} \PY{n}{network}\PY{p}{[}\PY{n}{target\PYZus{}layer\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{weights}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
    \PY{n}{w2} \PY{o}{=} \PY{n}{network}\PY{p}{[}\PY{n}{target\PYZus{}layer\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{weights}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
    \PY{n}{curr\PYZus{}error} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{k}{for} \PY{n}{x}\PY{p}{,} \PY{n}{y} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{features}\PY{p}{,} \PY{n}{labels}\PY{p}{)}\PY{p}{:}
        \PY{n}{output} \PY{o}{=} \PY{n}{x}
        \PY{k}{for} \PY{n}{layer} \PY{o+ow}{in} \PY{n}{network}\PY{p}{:}
            \PY{n}{output} \PY{o}{=} \PY{n}{layer}\PY{o}{.}\PY{n}{forward}\PY{p}{(}\PY{n}{output}\PY{p}{)}

        \PY{n}{curr\PYZus{}error} \PY{o}{+}\PY{o}{=} \PY{n}{mse}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{output}\PY{p}{)}
    \PY{n}{curr\PYZus{}error} \PY{o}{/}\PY{o}{=} \PY{n}{labels}\PY{o}{.}\PY{n}{size}
    \PY{n}{ax0}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{p}{[}\PY{n}{w1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{w2}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{curr\PYZus{}error}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{red}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.4}\PY{p}{)}

    \PY{n}{target\PYZus{}layer} \PY{o}{=} \PY{n}{copy}\PY{o}{.}\PY{n}{deepcopy}\PY{p}{(}\PY{n}{network}\PY{p}{[}\PY{n}{target\PYZus{}layer\PYZus{}idx}\PY{p}{]}\PY{p}{)}
    \PY{n}{w1\PYZus{}range} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{w1\PYZus{}min}\PY{p}{,} \PY{n}{w1\PYZus{}max}\PY{p}{,} \PY{n}{loss\PYZus{}dims}\PY{p}{)}
    \PY{n}{w2\PYZus{}range} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{w2\PYZus{}min}\PY{p}{,} \PY{n}{w2\PYZus{}max}\PY{p}{,} \PY{n}{loss\PYZus{}dims}\PY{p}{)}
    \PY{n}{w1\PYZus{}range}\PY{p}{,} \PY{n}{w2\PYZus{}range} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{meshgrid}\PY{p}{(}\PY{n}{w1\PYZus{}range}\PY{p}{,} \PY{n}{w2\PYZus{}range}\PY{p}{)}
    \PY{n}{w\PYZus{}range} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{stack}\PY{p}{(}\PY{p}{(}\PY{n}{w1\PYZus{}range}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{w2\PYZus{}range}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}

    \PY{n}{error\PYZus{}range} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{]}\PY{p}{)}

    \PY{k}{for} \PY{n}{target\PYZus{}layer\PYZus{}weight} \PY{o+ow}{in} \PY{n}{w\PYZus{}range}\PY{p}{:}
        \PY{n}{target\PYZus{}layer\PYZus{}weight} \PY{o}{=} \PY{n}{target\PYZus{}layer\PYZus{}weight}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
        \PY{n}{target\PYZus{}layer}\PY{o}{.}\PY{n}{weights}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{=} \PY{n}{target\PYZus{}layer\PYZus{}weight}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}

        \PY{n}{error} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{k}{for} \PY{n}{x}\PY{p}{,} \PY{n}{y} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{features}\PY{p}{,} \PY{n}{labels}\PY{p}{)}\PY{p}{:}
            \PY{n}{output} \PY{o}{=} \PY{n}{x}
            \PY{k}{for} \PY{n}{layer\PYZus{}idx}\PY{p}{,} \PY{n}{layer} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{network}\PY{p}{)}\PY{p}{:}
                \PY{k}{if} \PY{n}{layer\PYZus{}idx} \PY{o}{==} \PY{n}{target\PYZus{}layer\PYZus{}idx}\PY{p}{:}
                    \PY{n}{output} \PY{o}{=} \PY{n}{target\PYZus{}layer}\PY{o}{.}\PY{n}{forward}\PY{p}{(}\PY{n}{output}\PY{p}{)}
                \PY{k}{else}\PY{p}{:}
                    \PY{n}{output} \PY{o}{=} \PY{n}{layer}\PY{o}{.}\PY{n}{forward}\PY{p}{(}\PY{n}{output}\PY{p}{)}

            \PY{n}{error} \PY{o}{+}\PY{o}{=} \PY{n}{mse}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{output}\PY{p}{)}
        \PY{n}{error} \PY{o}{/}\PY{o}{=} \PY{n}{labels}\PY{o}{.}\PY{n}{size}
        \PY{n}{error\PYZus{}range} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{error\PYZus{}range}\PY{p}{,} \PY{n}{error}\PY{p}{)}

    \PY{n}{ax0}\PY{o}{.}\PY{n}{plot\PYZus{}surface}\PY{p}{(}
        \PY{n}{w1\PYZus{}range}\PY{p}{,}
        \PY{n}{w2\PYZus{}range}\PY{p}{,}
        \PY{n}{error\PYZus{}range}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{loss\PYZus{}dims}\PY{p}{,} \PY{n}{loss\PYZus{}dims}\PY{p}{)}\PY{p}{,}
        \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{blue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
        \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,}
    \PY{p}{)}


\PY{k}{def} \PY{n+nf}{plot\PYZus{}mse\PYZus{}and\PYZus{}predictions}\PY{p}{(}
    \PY{n}{ax0}\PY{p}{,} \PY{n}{ax1}\PY{p}{,} \PY{n}{idx}\PY{p}{,} \PY{n}{visible\PYZus{}mse}\PY{p}{,} \PY{n}{mse\PYZus{}idx}\PY{p}{,} \PY{n}{errors}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{Y}\PY{p}{,} \PY{n}{Z}\PY{p}{,} \PY{n}{predictions}\PY{p}{,} \PY{n}{dims}
\PY{p}{)}\PY{p}{:}
    \PY{n}{ax0}\PY{o}{.}\PY{n}{plot}\PY{p}{(}
        \PY{n}{mse\PYZus{}idx}\PY{p}{[}\PY{n}{visible\PYZus{}mse}\PY{p}{]}\PY{p}{[}\PY{p}{:} \PY{n}{idx} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
        \PY{n}{errors}\PY{p}{[}\PY{n}{visible\PYZus{}mse}\PY{p}{]}\PY{p}{[}\PY{p}{:} \PY{n}{idx} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
        \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{red}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
        \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{,}
    \PY{p}{)}

    \PY{n}{plot\PYZus{}3d\PYZus{}predictions}\PY{p}{(}\PY{n}{ax1}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{Y}\PY{p}{,} \PY{n}{Z}\PY{p}{,} \PY{n}{predictions}\PY{p}{,} \PY{n}{dims}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{plot\PYZus{}loss\PYZus{}landscape\PYZus{}and\PYZus{}predictions}\PY{p}{(}
    \PY{n}{ax0}\PY{p}{,}
    \PY{n}{ax1}\PY{p}{,}
    \PY{n}{network}\PY{p}{,}
    \PY{n}{target\PYZus{}layer\PYZus{}idx}\PY{p}{,}
    \PY{n}{features}\PY{p}{,}
    \PY{n}{labels}\PY{p}{,}
    \PY{n}{X}\PY{p}{,}
    \PY{n}{Y}\PY{p}{,}
    \PY{n}{Z}\PY{p}{,}
    \PY{n}{predictions}\PY{p}{,}
    \PY{n}{preds\PYZus{}dims}\PY{p}{,}
    \PY{n}{w1\PYZus{}min}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{,}
    \PY{n}{w1\PYZus{}max}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,}
    \PY{n}{w2\PYZus{}min}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{,}
    \PY{n}{w2\PYZus{}max}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,}
    \PY{n}{loss\PYZus{}dims}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,}
\PY{p}{)}\PY{p}{:}
    \PY{n}{plot\PYZus{}3d\PYZus{}predictions}\PY{p}{(}\PY{n}{ax1}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{Y}\PY{p}{,} \PY{n}{Z}\PY{p}{,} \PY{n}{predictions}\PY{p}{,} \PY{n}{preds\PYZus{}dims}\PY{p}{)}
    \PY{n}{plot\PYZus{}layer\PYZus{}loss\PYZus{}landscape}\PY{p}{(}
        \PY{n}{ax0}\PY{p}{,}
        \PY{n}{network}\PY{p}{,}
        \PY{n}{target\PYZus{}layer\PYZus{}idx}\PY{p}{,}
        \PY{n}{features}\PY{p}{,}
        \PY{n}{labels}\PY{p}{,}
        \PY{n}{w1\PYZus{}min}\PY{p}{,}
        \PY{n}{w1\PYZus{}max}\PY{p}{,}
        \PY{n}{w2\PYZus{}min}\PY{p}{,}
        \PY{n}{w2\PYZus{}max}\PY{p}{,}
        \PY{n}{loss\PYZus{}dims}\PY{p}{,}
    \PY{p}{)}


\PY{k}{def} \PY{n+nf}{show\PYZus{}epoch}\PY{p}{(}\PY{n}{epoch}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{p}{(}
        \PY{n}{epoch} \PY{o}{\PYZlt{}} \PY{l+m+mi}{25}
        \PY{o+ow}{or} \PY{p}{(}\PY{n}{epoch} \PY{o}{\PYZlt{}} \PY{l+m+mi}{25} \PY{o+ow}{and} \PY{n}{epoch} \PY{o}{\PYZpc{}} \PY{l+m+mi}{2} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}
        \PY{o+ow}{or} \PY{p}{(}\PY{n}{epoch} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{100} \PY{o+ow}{and} \PY{n}{epoch} \PY{o}{\PYZpc{}} \PY{l+m+mi}{10} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}
        \PY{o+ow}{or} \PY{p}{(}\PY{n}{epoch} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{500} \PY{o+ow}{and} \PY{n}{epoch} \PY{o}{\PYZpc{}} \PY{l+m+mi}{25} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}
        \PY{o+ow}{or} \PY{p}{(}\PY{n}{epoch} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{1000} \PY{o+ow}{and} \PY{n}{epoch} \PY{o}{\PYZpc{}} \PY{l+m+mi}{50} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}
        \PY{o+ow}{or} \PY{n}{epoch} \PY{o}{\PYZpc{}} \PY{l+m+mi}{250} \PY{o}{==} \PY{l+m+mi}{0}
    \PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsection{Training the model}\label{training-the-model}

Let's tie everything together to train the neural network. In each
epoch, we will do the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Pass the training data into our model to get the model's predictions
\item
  Calculate the loss from the model's predictions and the expected value
\item
  Use the loss to run the optimizer to update the weights and biases
\item
  Call the visualization functions to visualize the training process
\end{enumerate}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{fit}\PY{p}{(}
    \PY{n}{network}\PY{p}{,}
    \PY{n}{features}\PY{p}{,}
    \PY{n}{labels}\PY{p}{,}
    \PY{n}{X}\PY{p}{,}
    \PY{n}{Y}\PY{p}{,}
    \PY{n}{Z}\PY{p}{,}
    \PY{n}{preds\PYZus{}dims}\PY{p}{,}
    \PY{n}{epochs}\PY{p}{,}
    \PY{n}{optimizer}\PY{p}{,}
    \PY{n}{mse\PYZus{}plot\PYZus{}filename}\PY{p}{,}
    \PY{n}{loss\PYZus{}landscape\PYZus{}plot\PYZus{}filename}\PY{p}{,}
\PY{p}{)}\PY{p}{:}
    \PY{n}{mse\PYZus{}idx} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{epochs} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{errors} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{full}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{mse\PYZus{}ax}\PY{p}{,} \PY{n}{predictions\PYZus{}ax1}\PY{p}{,} \PY{n}{camera1} \PY{o}{=} \PY{n}{create\PYZus{}scatter\PYZus{}and\PYZus{}3d\PYZus{}plot}\PY{p}{(}\PY{p}{)}
    \PY{n}{loss\PYZus{}landscape\PYZus{}ax}\PY{p}{,} \PY{n}{predictions\PYZus{}ax2}\PY{p}{,} \PY{n}{camera2} \PY{o}{=} \PY{n}{create\PYZus{}3d\PYZus{}and\PYZus{}3d\PYZus{}plot}\PY{p}{(}\PY{p}{)}

    \PY{n}{network\PYZus{}len} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{network}\PY{p}{)}

    \PY{k}{for} \PY{n}{idx} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{epochs}\PY{p}{)}\PY{p}{:}
        \PY{n}{error} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{n}{predictions} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{]}\PY{p}{)}

        \PY{k}{for} \PY{n}{x}\PY{p}{,} \PY{n}{y} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{features}\PY{p}{,} \PY{n}{labels}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} Forward Propagation}
            \PY{n}{output} \PY{o}{=} \PY{n}{x}
            \PY{k}{for} \PY{n}{layer} \PY{o+ow}{in} \PY{n}{network}\PY{p}{:}
                \PY{n}{output} \PY{o}{=} \PY{n}{layer}\PY{o}{.}\PY{n}{forward}\PY{p}{(}\PY{n}{output}\PY{p}{)}

            \PY{n}{predictions} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{predictions}\PY{p}{,} \PY{n}{output}\PY{p}{)}

            \PY{c+c1}{\PYZsh{} Store Error}
            \PY{c+c1}{\PYZsh{} no need to convert to numpy cpu, since both are tensors on device}
            \PY{n}{error} \PY{o}{+}\PY{o}{=} \PY{n}{mse}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{output}\PY{p}{)}

            \PY{c+c1}{\PYZsh{} Backpropagation}
            \PY{n}{grad} \PY{o}{=} \PY{n}{mse\PYZus{}prime}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{output}\PY{p}{)}
            \PY{k}{for} \PY{n}{layer} \PY{o+ow}{in} \PY{n+nb}{reversed}\PY{p}{(}\PY{n}{network}\PY{p}{)}\PY{p}{:}
                \PY{n}{grad} \PY{o}{=} \PY{n}{layer}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{n}{grad}\PY{p}{,} \PY{n}{optimizer}\PY{p}{)}

        \PY{n}{error} \PY{o}{/}\PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}

        \PY{k}{if} \PY{n}{show\PYZus{}epoch}\PY{p}{(}\PY{n}{idx}\PY{p}{)}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{epoch: }\PY{l+s+si}{\PYZob{}}\PY{n}{idx}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, MSE: }\PY{l+s+si}{\PYZob{}}\PY{n}{error}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

            \PY{c+c1}{\PYZsh{} Plot MSE}
            \PY{n}{errors}\PY{p}{[}\PY{n}{idx}\PY{p}{]} \PY{o}{=} \PY{n}{error}
            \PY{n}{visible\PYZus{}mse} \PY{o}{=} \PY{n}{errors} \PY{o}{!=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
            \PY{n}{plot\PYZus{}mse\PYZus{}and\PYZus{}predictions}\PY{p}{(}
                \PY{n}{mse\PYZus{}ax}\PY{p}{,}
                \PY{n}{predictions\PYZus{}ax1}\PY{p}{,}
                \PY{n}{idx}\PY{p}{,}
                \PY{n}{visible\PYZus{}mse}\PY{p}{,}
                \PY{n}{mse\PYZus{}idx}\PY{p}{,}
                \PY{n}{errors}\PY{p}{,}
                \PY{n}{X}\PY{p}{,}
                \PY{n}{Y}\PY{p}{,}
                \PY{n}{Z}\PY{p}{,}
                \PY{n}{predictions}\PY{p}{,}
                \PY{n}{preds\PYZus{}dims}\PY{p}{,}
            \PY{p}{)}

            \PY{c+c1}{\PYZsh{} plot the loss landscape of the second to last layer}
            \PY{c+c1}{\PYZsh{} a 3D plot can be made because it\PYZsq{}s only 2 neurons}
            \PY{n}{plot\PYZus{}loss\PYZus{}landscape\PYZus{}and\PYZus{}predictions}\PY{p}{(}
                \PY{n}{loss\PYZus{}landscape\PYZus{}ax}\PY{p}{,}
                \PY{n}{predictions\PYZus{}ax2}\PY{p}{,}
                \PY{n}{network}\PY{p}{,}
                \PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{,}
                \PY{n}{features}\PY{p}{,}
                \PY{n}{labels}\PY{p}{,}
                \PY{n}{X}\PY{p}{,}
                \PY{n}{Y}\PY{p}{,}
                \PY{n}{Z}\PY{p}{,}
                \PY{n}{predictions}\PY{p}{,}
                \PY{n}{preds\PYZus{}dims}\PY{p}{,}
            \PY{p}{)}

            \PY{n}{camera1}\PY{o}{.}\PY{n}{snap}\PY{p}{(}\PY{p}{)}
            \PY{n}{camera2}\PY{o}{.}\PY{n}{snap}\PY{p}{(}\PY{p}{)}

    \PY{n}{animation1} \PY{o}{=} \PY{n}{camera1}\PY{o}{.}\PY{n}{animate}\PY{p}{(}\PY{p}{)}
    \PY{n}{animation1}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{n}{mse\PYZus{}plot\PYZus{}filename}\PY{p}{,} \PY{n}{writer}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pillow}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{animation2} \PY{o}{=} \PY{n}{camera2}\PY{o}{.}\PY{n}{animate}\PY{p}{(}\PY{p}{)}
    \PY{n}{animation2}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{n}{loss\PYZus{}landscape\PYZus{}plot\PYZus{}filename}\PY{p}{,} \PY{n}{writer}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pillow}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    The model can be visualized with the following:

    Our model consists of 3 layers with the Tanh() activation function after
each layer.

Layer 1: - Input Dimensions: 2 - Output Dimensions: 12

Layer 2: - Input Dimensions: 12 - Output Dimensions: 2

Layer 3: - Input Dimensions: 2 - Output Dimensions: 1

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model} \PY{o}{=} \PY{p}{[}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{12}\PY{p}{)}\PY{p}{,} \PY{n}{Tanh}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{Tanh}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{Tanh}\PY{p}{(}\PY{p}{)}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    Let's train our model by passing our model and optimizer to our training
method

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{301}
\PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{l+m+mf}{0.005}
\PY{n}{optimizer} \PY{o}{=} \PY{n}{GradientDescentOptimizier}\PY{p}{(}\PY{n}{learning\PYZus{}rate}\PY{p}{)}

\PY{n}{mse\PYZus{}plot\PYZus{}filename} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{neural\PYZus{}network.gif}\PY{l+s+s2}{\PYZdq{}}
\PY{n}{loss\PYZus{}landscape\PYZus{}plot\PYZus{}filename} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{neural\PYZus{}network\PYZus{}loss\PYZus{}landscape.gif}\PY{l+s+s2}{\PYZdq{}}
\PY{n}{fit}\PY{p}{(}
    \PY{n}{model}\PY{p}{,}
    \PY{n}{features}\PY{p}{,}
    \PY{n}{labels}\PY{p}{,}
    \PY{n}{X}\PY{p}{,}
    \PY{n}{Y}\PY{p}{,}
    \PY{n}{Z}\PY{p}{,}
    \PY{n}{dims}\PY{p}{,}
    \PY{n}{epochs}\PY{p}{,}
    \PY{n}{optimizer}\PY{p}{,}
    \PY{n}{mse\PYZus{}plot\PYZus{}filename}\PY{p}{,}
    \PY{n}{loss\PYZus{}landscape\PYZus{}plot\PYZus{}filename}\PY{p}{,}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
epoch: 0, MSE: 3.0033681391919607
epoch: 1, MSE: 2.861607692334731
epoch: 2, MSE: 2.825853287737221
epoch: 3, MSE: 2.817140865761648
epoch: 4, MSE: 2.814117393067351
epoch: 5, MSE: 2.8099737108937255
epoch: 6, MSE: 2.8023452137616185
epoch: 7, MSE: 2.7902417103220167
epoch: 8, MSE: 2.773063690822775
epoch: 9, MSE: 2.7502645093729323
epoch: 10, MSE: 2.721249705555072
epoch: 11, MSE: 2.685389899197139
epoch: 12, MSE: 2.642101512218608
epoch: 13, MSE: 2.590967280467696
epoch: 14, MSE: 2.531869391821693
epoch: 15, MSE: 2.4651083530357476
epoch: 16, MSE: 2.3914851384273637
epoch: 17, MSE: 2.3123291935128214
epoch: 18, MSE: 2.229456375261003
epoch: 19, MSE: 2.1450438173269872
epoch: 20, MSE: 2.061423945351905
epoch: 21, MSE: 1.9808288922004156
epoch: 22, MSE: 1.9051432156154482
epoch: 23, MSE: 1.8357262341875262
epoch: 24, MSE: 1.773340422003911
epoch: 25, MSE: 1.7181844787747815
epoch: 30, MSE: 1.5333665989875438
epoch: 40, MSE: 1.3570304555393908
epoch: 50, MSE: 1.2049292538302114
epoch: 60, MSE: 1.0420867215852363
epoch: 70, MSE: 0.8858079903889653
epoch: 75, MSE: 0.8162489390669957
epoch: 80, MSE: 0.7542965170101418
epoch: 90, MSE: 0.6530217909549149
epoch: 100, MSE: 0.5740174523365321
epoch: 125, MSE: 0.37933561711053515
epoch: 150, MSE: 0.24891189943713543
epoch: 175, MSE: 0.22012182602057515
epoch: 200, MSE: 0.20367177160551178
epoch: 225, MSE: 0.1908399435300111
epoch: 250, MSE: 0.1800010056452711
epoch: 275, MSE: 0.17054412799607388
epoch: 300, MSE: 0.16213374967702018
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{combined_files/combined_188_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{combined_files/combined_188_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Output GIF}\label{output-gif}

In this visualization, we see the predictions fit the ground truth. The
neural network was able to find the optimal parameters to fit this
function. Now think about the applications of this. Given input data
about people's shopping habits, we can predict things to recommend to
them. We can recommend a social media post to show them or a show to
watch next. We can pass data to neural networks and it will uncover
patterns from input data and find patterns.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{mse\PYZus{}plot\PYZus{}filename}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<IPython.core.display.Image object>
\end{Verbatim}
\end{tcolorbox}
        
    The visualization below shows that backpropagation finds the optimal
weights for the neural network. On the left graph, the red dot shows the
current values of the weight matrix in the last layer of the neural
network. The z axis is the mean squared error loss. If the weights
weren't at the current value, the loss wouldn't be at a minima, meaning
that backpropagation in fact does update the weights to the most optimal
(local extrema) value and allows the network to converge.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{loss\PYZus{}landscape\PYZus{}plot\PYZus{}filename}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<IPython.core.display.Image object>
\end{Verbatim}
\end{tcolorbox}
        
    \subsection{Pytorch Implementation}\label{pytorch-implementation}

Machine Learning libraries, such as PyTorch, provide utilities to easily
train and test neural networks on all types of optimized hardware. Now
we will implement the same network in PyTorch.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{torch}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn} \PY{k}{as} \PY{n+nn}{nn}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn}\PY{n+nn}{.}\PY{n+nn}{functional} \PY{k}{as} \PY{n+nn}{F}

\PY{n}{torch}\PY{o}{.}\PY{n}{manual\PYZus{}seed}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<torch.\_C.Generator at 0xffff42d26ed0>
\end{Verbatim}
\end{tcolorbox}
        
    \subsection{PyTorch nn.Module}\label{pytorch-nn.module}

We represent our neural network by creating a subclass of the
\texttt{nn.Module} PyTorch class that defines all of the layers in the
network and how data flows through the network. The \texttt{forward()}
method is the forward propagation of our neural forward. Notice that we
don't need to specify anything for the backpropagation process. PyTorch
takes care of this for us using their auto-differentiation support. We
just need to import an optimizer class, like the PyTorch provided
gradient descent class, and pass in our model's parameters. This is the
beauty of using machine learning libraries.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{TorchNet}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{TorchNet}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} define the layers}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc1} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{14}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc2} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{l+m+mi}{14}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc3} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} pass the result of the previous layer to the next layer}
        \PY{n}{x} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{tanh}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc1}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
        \PY{n}{x} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{tanh}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc2}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
        \PY{k}{return} \PY{n}{F}\PY{o}{.}\PY{n}{tanh}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc3}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsection{PyTorch Training}\label{pytorch-training}

Similar to our own implementation, let's use our model to define our
training process. In each epoch, we will do the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Pass the training data into our model to get the model's predictions
\item
  Calculate the loss from the model's predictions and the expected value
\item
  Use the loss to run the optimizer to update the weights and biases
\item
  Call the visualization functions to visualize the training process
\end{enumerate}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{torch\PYZus{}fit}\PY{p}{(}
    \PY{n}{model}\PY{p}{,} \PY{n}{features}\PY{p}{,} \PY{n}{labels}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{Y}\PY{p}{,} \PY{n}{Z}\PY{p}{,} \PY{n}{dims}\PY{p}{,} \PY{n}{epochs}\PY{p}{,} \PY{n}{optimizer}\PY{p}{,} \PY{n}{output\PYZus{}filename}
\PY{p}{)}\PY{p}{:}
    \PY{n}{mse\PYZus{}idx} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{epochs} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{errors} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{full}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{mse\PYZus{}ax}\PY{p}{,} \PY{n}{predictions\PYZus{}ax1}\PY{p}{,} \PY{n}{camera1} \PY{o}{=} \PY{n}{create\PYZus{}scatter\PYZus{}and\PYZus{}3d\PYZus{}plot}\PY{p}{(}\PY{p}{)}

    \PY{n}{loss\PYZus{}fn} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{MSELoss}\PY{p}{(}\PY{p}{)}

    \PY{k}{for} \PY{n}{idx} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{epochs}\PY{p}{)}\PY{p}{:}
        \PY{n}{error} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{n}{predictions} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{]}\PY{p}{)}

        \PY{k}{for} \PY{n}{x}\PY{p}{,} \PY{n}{y} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{features}\PY{p}{,} \PY{n}{labels}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} Forward Propagation}
            \PY{n}{output} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{n}{x}\PY{p}{)}

            \PY{n}{output\PYZus{}np} \PY{o}{=} \PY{n}{output}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}
            \PY{n}{predictions} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{predictions}\PY{p}{,} \PY{n}{output\PYZus{}np}\PY{p}{)}

            \PY{c+c1}{\PYZsh{} Store Error}
            \PY{n}{loss} \PY{o}{=} \PY{n}{loss\PYZus{}fn}\PY{p}{(}\PY{n}{output}\PY{p}{,} \PY{n}{y}\PY{p}{)}

            \PY{n}{error} \PY{o}{+}\PY{o}{=} \PY{n}{loss}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}

            \PY{c+c1}{\PYZsh{} Backpropagation}
            \PY{n}{optimizer}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
            \PY{n}{loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
            \PY{n}{optimizer}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}

        \PY{n}{error} \PY{o}{/}\PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}

        \PY{k}{if} \PY{n}{show\PYZus{}epoch}\PY{p}{(}\PY{n}{idx}\PY{p}{)}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{epoch: }\PY{l+s+si}{\PYZob{}}\PY{n}{idx}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, MSE: }\PY{l+s+si}{\PYZob{}}\PY{n}{error}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

            \PY{c+c1}{\PYZsh{} Plot MSE}
            \PY{n}{errors}\PY{p}{[}\PY{n}{idx}\PY{p}{]} \PY{o}{=} \PY{n}{error}
            \PY{n}{visible\PYZus{}mse} \PY{o}{=} \PY{n}{errors} \PY{o}{!=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}

            \PY{n}{plot\PYZus{}mse\PYZus{}and\PYZus{}predictions}\PY{p}{(}
                \PY{n}{mse\PYZus{}ax}\PY{p}{,}
                \PY{n}{predictions\PYZus{}ax1}\PY{p}{,}
                \PY{n}{idx}\PY{p}{,}
                \PY{n}{visible\PYZus{}mse}\PY{p}{,}
                \PY{n}{mse\PYZus{}idx}\PY{p}{,}
                \PY{n}{errors}\PY{p}{,}
                \PY{n}{X}\PY{p}{,}
                \PY{n}{Y}\PY{p}{,}
                \PY{n}{Z}\PY{p}{,}
                \PY{n}{predictions}\PY{p}{,}
                \PY{n}{dims}\PY{p}{,}
            \PY{p}{)}

            \PY{n}{camera1}\PY{o}{.}\PY{n}{snap}\PY{p}{(}\PY{p}{)}

    \PY{n}{animation1} \PY{o}{=} \PY{n}{camera1}\PY{o}{.}\PY{n}{animate}\PY{p}{(}\PY{p}{)}
    \PY{n}{animation1}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{n}{output\PYZus{}filename}\PY{p}{,} \PY{n}{writer}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pillow}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Let's call the training method and pass in the model, training data, and
optimizer.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{device} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{device}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cuda:0}\PY{l+s+s2}{\PYZdq{}} \PY{k}{if} \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{is\PYZus{}available}\PY{p}{(}\PY{p}{)} \PY{k}{else} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cpu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{torch\PYZus{}model} \PY{o}{=} \PY{n}{TorchNet}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}

\PY{c+c1}{\PYZsh{} the inputs and outputs for PyTorch must be tensors}
\PY{n}{features\PYZus{}tensor} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{features}\PY{p}{,} \PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{labels\PYZus{}tensor} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{labels}\PY{p}{,} \PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}

\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{101}
\PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{l+m+mf}{0.005}
\PY{n}{optimizer} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{optim}\PY{o}{.}\PY{n}{SGD}\PY{p}{(}\PY{n}{torch\PYZus{}model}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{n}{learning\PYZus{}rate}\PY{p}{,} \PY{n}{momentum}\PY{o}{=}\PY{l+m+mf}{0.0}\PY{p}{)}

\PY{n}{output\PYZus{}filename\PYZus{}pytorch} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{neural\PYZus{}network\PYZus{}pytorch.gif}\PY{l+s+s2}{\PYZdq{}}
\PY{n}{torch\PYZus{}fit}\PY{p}{(}
    \PY{n}{torch\PYZus{}model}\PY{p}{,}
    \PY{n}{features\PYZus{}tensor}\PY{p}{,}
    \PY{n}{labels\PYZus{}tensor}\PY{p}{,}
    \PY{n}{X}\PY{p}{,}
    \PY{n}{Y}\PY{p}{,}
    \PY{n}{Z}\PY{p}{,}
    \PY{n}{dims}\PY{p}{,}
    \PY{n}{epochs}\PY{p}{,}
    \PY{n}{optimizer}\PY{p}{,}
    \PY{n}{output\PYZus{}filename\PYZus{}pytorch}\PY{p}{,}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
epoch: 0, MSE: 2.984565019607544
epoch: 1, MSE: 2.7075679302215576
epoch: 2, MSE: 2.6843478679656982
epoch: 3, MSE: 2.6776466369628906
epoch: 4, MSE: 2.681764841079712
epoch: 5, MSE: 2.690561294555664
epoch: 6, MSE: 2.700050115585327
epoch: 7, MSE: 2.7078511714935303
epoch: 8, MSE: 2.7127301692962646
epoch: 9, MSE: 2.7142038345336914
epoch: 10, MSE: 2.712219476699829
epoch: 11, MSE: 2.7069084644317627
epoch: 12, MSE: 2.698435068130493
epoch: 13, MSE: 2.686903715133667
epoch: 14, MSE: 2.672306776046753
epoch: 15, MSE: 2.654496908187866
epoch: 16, MSE: 2.6331663131713867
epoch: 17, MSE: 2.607834577560425
epoch: 18, MSE: 2.5778284072875977
epoch: 19, MSE: 2.5422799587249756
epoch: 20, MSE: 2.500131607055664
epoch: 21, MSE: 2.4501821994781494
epoch: 22, MSE: 2.3912034034729004
epoch: 23, MSE: 2.322108507156372
epoch: 24, MSE: 2.242227554321289
epoch: 25, MSE: 2.151536703109741
epoch: 30, MSE: 1.5780051946640015
epoch: 40, MSE: 0.4502718448638916
epoch: 50, MSE: 0.15558312833309174
epoch: 60, MSE: 0.10384205728769302
epoch: 70, MSE: 0.08925554901361465
epoch: 75, MSE: 0.08637455850839615
epoch: 80, MSE: 0.08450805395841599
epoch: 90, MSE: 0.0817754715681076
epoch: 100, MSE: 0.0793890580534935
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{combined_files/combined_200_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{output\PYZus{}filename\PYZus{}pytorch}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<IPython.core.display.Image object>
\end{Verbatim}
\end{tcolorbox}
        

    % Add a bibliography block to the postdoc
    
    
    
\end{document}
